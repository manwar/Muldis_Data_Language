                                 Muldis::D
                                TODO DRAFT
---------------------------------------------------------------------------

This TODO_DRAFT file is an overflow destination of sorts for the TODO file,
and it contains a large amount of brainstorming, some verbose, in the guise
of TODO items, but much of this is highly unlikely to be done without major
changes, and in any event it is considered inappropriate for release
distributions due at least to its great rate of growth and low signal to
noise.  This TODO_DRAFT file only exists in the Muldis D version control
repository, not in the packaged Muldis D distributions on CPAN, so it is
still available for interested parties to read but doesn't waste the
resources of everyone else.

----------

* http://rajeevrastogi.blogspot.ca/2015/06/presented-paper-in-pgcon-2015-on-native.html

* http://www.frozennorth.org/C509291565/E1939404619/ - The Cult of the NDA

* Free up $ to just be for generic operator use.
- Make \$foo mean what $foo used to mean, and \\$foo for \$foo, or we could
do other alternatives, but the main point is $foo is not special syntax.

* Stop using triple-dot "..." as special symbolic Plain_Text syntax to mean
"definition not given here" or such, which could either be a visual TODO
marker that compiles but dies if run, or which is for use by extensions to
mean the implementation must supply a meaning as it is eg low-level.
- Instead have some \^*[] or other syntax to mean implementation defines.
- Thus ... is freed up for use as an ordinary symbolic operator, such as
for "x...y" sequence specs etc like Perl 6 has.
- One is still free to user-define a ... operator that means die, though
some alpha name is probably more appropriate.

* In the Perl 5 parser, at every stage of the pipeline, the input and
output are both node trees consisting usually of nested Perl 5 arrays.
The input of stage 1 is something like ['source_code','...'] and so on.

* Figure out some extended syntax to support very long numeric literals
such that they can be broken with whitespace.
- Perhaps ,, is suitable for this?

* Muldis D structs, particularly source code, should be represented in
foo-hosted language variants by their native concepts of structs or generic
objects if possible, or hash maps otherwise.  For example, with Perl 5
hosts, only Muldis D arrays should be represented by Perl 5 arrays, and
Perl 5 hashes should represent structs in general, with 'ordered' keys just
being strings like "\x0" or "\x1" etc; if hashes displayed in sorted key
order those would sort first presumably.  Or Perl/etc arrays could still be
used sometimes as an alternate terser syntax.

* Figure out what escape sequences are best for tersely representing
single-quotes and double-quotes context-free; \qs and \qd are candidates,
or \a and \q etc.

* Even though it takes more space, make the Muldis D code have possibly
several levels of meta that keeps copies of the raw string source, so that
it actually is possible to regenerate the original string source to the
smallest detail, including individual parts of runs of string literal
tokens, and the original escape sequences in strings, etc.
- That is, each stage of the parsing that would otherwise do some kind of
lossy normalization would keep a copy of the pre-normalized tokens.
- To save space/complexity, only save the extra copy if it adds information
such as if the string actually either did have escape sequences or was
split into segments etc.  Otherwise, the non-meta version is identical to
the source string token.
- Besides perfect round-tripping, this will also make it easier to give
meaningful error messages or debugging messages showing the context in
terms of the original source code.
- But its still safe to start this only after the level of Unicode NFD/NFKD
folding; if users REALLY cared about that level of raw, they have the
original Blob/Text as applicable.

* Now ALL string-like contexts in Muldis D are quoted using ['"`] pairs,
including ALL comments.
- Now / and * are treated as ordinary symbolic chars in all outside-string
contexts, and /* or */ are valid as ordinary entity names.
- Now an ordinary backtick-quoted string is a treated-as-whitespace comment
and this is just a simple take contents literally there is no escaping.
- So only '' and "" support escape sequences and run-catenating, `` neither.

* Backticks are no longer available (comments took them) to use with generic
prefix/infix/postfix operator call syntax.  Any operator name that is not
syntactically valid as a bareword eg "op arg" or "arg op arg" or "arg->op"
may no longer be called that way and must instead use either the foo() or
()-->foo form.
- All the other disambiguation options eg `op op "arg"` are still available.
- As a side benefit, quoting an op no longer possibly changes its precedence
level, so its much cleaner now, just alpha precedence and symbolic precedence.
- Now there is the new disambiguator '<-' which is the mirror of '->' and
lets you force prefix interpretation rather than infix, eg "op<-op arg" or
"op op<-arg" etc where none means "arg op arg".
- Perhaps for completeness we can support -><- to be explicit / have parity
for that third option, eg `arg->op<-arg`.

* Now :: is the namespace separator for names within a package, eg
Muldis_D::Integer::NN etc.
- The : is special anyway and the :: mnemonic already assoc with naming things.
- Leading :: means we have an absolute path rather than a floating one.
- Having whitespace around the :: is allowed and doesn't change the meaning.
- Now . is treated as ordinary symbolic char in all outside-string contexts
and . is valid as ordinary entity names, presumably used for infix operators.
- Now :: is no longer a namespace separator within a package name itself,
that is for organizing packages themselves relative to other packages,
and we have to come up with some other scheme to use there.
- Perhaps ;; is a good candidate for separating parts of package names, and
it has the additional benefit of being portable on Mac HFS+ filesystems.
- Languages using :: for namespace separators include C++, Perl.

* Consider namespace-block syntax to provide a syntactic shortcut for declaring
entities, which would prefix that to any entities declared within, for example:
  namespace Muldis_D::Integer
  {
    add ::= function ...
    NN ::= type ...
  }
... which has the same effect as:
  Muldis_D::Integer::add ::= function ...
  Muldis_D::Integer::NN ::= type ...
- See both C++ and C# for prior art that use that exact syntax.

* It turns out C# also like Perl supports trailing commas in lists.
So does Python.

* This is useful http://www.hl7.org/ example http://hl7.org/fhir/2015May/datatypes.html .

* See http://design.perl6.org/S22.html about packaging stuff, which is also
relevant to compilation unit long names etc.  Also not to be confused with
distribution names.
- A compilation unit is just something that can be compiled on its own,
typically a file or a string.
- Saying 'use foo' may get foo from the file system or from an installation
repository (not the same thing) or from the network etc; the compunitrepo
in use determines where to look.

* Eliminate 'floating' identifiers as they were previously known such that
they traversed namespaces and tried to match on leaf nodes, which had a
variety of issues and led to other complexities.
- Instead, inspired by C#, the replacement 'floating' identifiers just look
at the 'searching' list and directly catenate the identifiers to all of the
prefixes given and those are the full candidate names to try and dispatch
on where corresponding materials exist.  Corresponds to C# 'using'.
- Note, C++ also has a 'using' keyword which appears similar maybe.

* Fundamental reorganize in-package namespaces to be more like
MD."+".Integer rather than MD.Integer."+" for several reasons.
- One is this works much better with the redef of 'floating' operators.
- Another is this makes the organization a lot more like typical languages
that support multiple instances of the same routine base name in a
namespace and generate a unique name etc on the parameter types; the
overloaded variants now are more clearly bundled visually.
- How this typically works is we would cluster overloaded ops like this:
    MD."+"."" ::= function virtual ...;
    MD."+".Integer ::= function ... implements "" ...;
    MD."+".Ratio ::= function ... implements "" ...;
    MD.NumericOrSomeSuch ::= type abstract;
    MD.Integer ::= type ...;
    MD.Ratio ::= type ...;
- We can still have namespaces to group things under, eg MD.Source_Code or
MD.Integer etc but then those are more for less common or optional things.

* Make Reference disjoint from Identifier where the latter is what users
write and the former the system generates/resolves at runtime that is
sensitive to context like what packages are loaded/etc, a Reference probably
is just an integer payload like with a filehandle etc, both are Structure.
- As before, identifiers have the absolute/relative/floating variants.

* Fully separate the designs of the fully-qualified language names of
Muldis D syntax/low-level itself and of packages / compilation units.
- Keep the syntax name declarations simpler in principle as in theory this
is less likely to have incompatible versions develop or code wanting to
declare multiple authority support rather than just one; they can declare
just one and the onus is more on forks to figure out how to be compatible.
In theory more variation would happen in the core library than the syntax,
or anyone forking the syntax would be good and use a different Syntax Name.
- Package / compilation unit names can become more complicated and in
particular become data structures definable by routines or possibly able to
evaluate their environments, such as is the case with Perl 6 comp units.
They can then for example declare different vnum ranges per declared auth.
- Make local package short names and their long names logic separate; only
the former is used for referencing in a package and the latter is mainly
just for instructing the package loader / comp unit finder etc.
- While Perl 6 has the more complexity in the language name too, that one
declaration covers both the syntax and core library while mine doesn't.
- Review how segmenting of packages is declared/done.

* http://peter.eisentraut.org/blog/2015/03/03/the-history-of-replication-in-postgresql/

* DBIx::PreQL looks interesting ... and SQL::Interpol maybe.

* See https://www.snip2code.com/Snippet/454117/Relational-data-languages
or https://github.com/seinokatsuhiro/koshucode-design/tree/master/note/relational-language
or http://c2.com/cgi/wiki?QueryLanguageComparison .

* Apparently here-docs Perl inherited from shells; eg:
psql <<EOF
\x
SELECT * FROM foo;
EOF

* https://www.simple-talk.com/sql/learn-sql-server/sql-server-sequence-basics/
- Also says: A sequence is a list of numbers. A series is a sum of numbers.

* SQLite list comment:
Artifact IDs used by fossil are the SHA-1 hash of the file contents, and 
the checkin IDs are the SHA-1 hash of the check-in manifest contents.  They 
are *NOT* random but rather, are 100% deterministic -- that is if you run 
the sha-1 hash over the same input data you will ALWAYS get the same result.

* Add new abstract types Boolable and Numable which contract that composing
types each have a single canonical way to cast their values as a Boolish or
a Numeric respectively, where said cast operators are the unary ? and #.
- For a numeric type, ? always means is-nonzero, and for a collection type,
? always means has at least one element, for a boolish type, ? just results
in its operand.  For a collection type, # is a count of its elements (a Bag
counts each instance of a quantified value as 1 towards the total), for a
boolish type, # is 1 for true and 0 for false, for a numeric type, # just
results in the # operand.
- Both ? and # are only defined for homogeneous collection types and not
heterogeneous collection types.  For a Relation, they count the tuples not
the attributes.  For Array/Bag/Set/String they count elements.  They are
not defined for a Tuple to eg count attributes, nor for a Structure.
- Whether or not to define ? or # for a Blob or Text is questionable since
there isn't such a clear 1:1, especially for #.
- Note that Perl 6 has removed special cases so while empty string is
False, any nonempty string including '0' is True.  Many other languages are
similar I think.

* It looks like C# has a feature analagous to Perl 6's "trusts:
internal members are visible to all code in the assembly they are declared in.
(And to other assemblies referenced using the [InternalsVisibleTo] attribute)

* In C#, "extension methods" act like methods defined within a class but
aren't actually defined in or alter that class.  Muldis D analogy is all
in-scope routines for a type whose first parameter is the class object analogy.
Or it seems to me that you can have divide a class's methods into bundles
so some bundles are loaded optionally by the user if they want to use them.

* Derived from / assumed based on http://www.pipelinedb.com/beta (based on
Pg 9.4) how a real-time or streaming DBMS works is that SQL/etc queries are
always active, and continue to send new result rows as new data arrives
that satisfies the query; queries are not constantly re-issued.

* Date: 	Wed, 8 Apr 2015 09:58:24 +1000
From: 	David Bennett <david@yorkage.com>
The point of this is to avoid having to copy the relation. If I have a relation 
of 1 million rows and an update affecting 10 of them, then the update can be 
expressed as the removal of (at most) 10 tuples and the insertion of (at most) 
10 tuples. Then if the relation fails a constraint, reversing the operation can 
be accomplished by the removal of (at most) 10 new tuples and the insertion of 
(at most) 10 old tuples.
Full scale DBMS do something similar by creating all the required tuples as 
blocks in temporary storage and then just switching around the links. If every 
block has a serial identifier then it becomes possible for every user to see a 
consistent view at all times, with no locks.
As part of this approach to updates it is straightforward for the implementation 
to add a unique identifier (to A and B) so that the deleted tuple and the 
inserted tuple can be paired up. This makes them accessible to a transition 
constraint using RA operations.

* Demote SC_Identifier and Capsule from low level.
- We then have: Boolean,Integer,Pair,Array,String,Bag,Structure,External.
- SC_Identifier and Capsule are now subtypes of Structure instead.
- A SC_Identifier is just a Structure of the 4 elements where each element
is typically a Structure of String, or an Integer.
- A Capsule is just a Structure whose first positional element is a SC_Reference.
- All Source_Code types plus core-defined Source_Code_Meta are now typically
just Structure whose first positional element is a string saying what main
type that value is of.
- Generally speaking, anything related to identity-qualified identifiers has
been booted from the low level.
- Generally speaking, the Plain_Text parser and compiler need to be able to
work entirely in the absense of the Muldis_D package and all of the data types
composing the native form of Muldis D code need to be simplified so each is
identified with a simple single-level string rather than a SC_Reference.
- Now typically most types defined by the Muldis_D package are unlikely to
be defined as Capsule but rather just as Structure with a leading positional
element giving its main type.
- So coin a new set of simple strings that are the identifiers for
the non-low-level core types, such as Ratio etc.  The existing core types
that are Capsule are now instead these new kinds, and their definitions are
still essentially unchanged, eg: \(\Ratio,numerator:5,denominator:8).  That
value we can use as-is in the absense of the Muldis_D package; the package
would still declare though:
    Muldis_D::Ratio ::= type of Structure where topic ^?= \(
        0 : type of SC_Attr_Name where topic = \Ratio,
        numerator : $Integer,
        denominator : $Integer,
    );
- We need some generalized shorthand for declaring a singleton type whose
value is drawn from another type, such as the above example.
- The $Foo will still resolve to an SC_Reference like before, because
actually testing the $Integer thing formally is still at runtime, though
while bootstrapping and we don't have the Muldis_D package, that check
doesn't actually have a SC_Reference to contend with, um stuff.
- Generally any system-defined type that a typical compiler would have
special knowledge of, such as Ratio or Relation etc, would have the new
simpler form of specification.
- The standard hosted-data variants of Muldis D and this new form will then
also very closely resemble each other as much as possible.
- It will generally be all pure Muldis D code that both resolves non-identity
Identifiers to identity ones, and likewise it is pure Muldis D code that tells
the low level register/load-package routines what their identity namespace
actually is, never mind what the package actually declares itself.

* Add abstract type Handle, to be composed by types whose values are each
associated with some possibly mutable state kept 'somewhere' such that the
package defining the composing type may typically know where that is but
the type user might not.  This may be the closest we get in practice to
mutable OO where value identity is per object itself rather than its
payload.  Routines would mainly be procedures rather than functions.
Example API procedures may be analagous to those for explicit memory
management in say C++ or where one explicitly has allocate() and free() etc
calls, plus fetch() and store() etc values there.  The value of the handle
type would have some kind of lookup key into an external data structure and
the value of that key would typically be generated by allocate() etc like
with C's malloc.  Explicit free() would likely be necessary but that for
performance reasons we could probably attach directly-write-only meta-data
to the handle value that lets it auto-release when no one refers to it,
similar to the write-only meta-data that helps say with indexing
Bags/derived etc.  So Handle is a root class in some senses like Object is
in some OO languages with respect to mutable objects.
- Related to this, we might want some global database with an attribute per
loaded fully qualified package and abstracted access to that, where some
packages can keep their squirreled away data?

* Simplify parser by an order of magnitude.  All delimited strings do not
contain a literal occurrance of their delimiter, or to be specific of their
trailing delimiter if different, and all characters between the delimiter
pairs simply represent themselves, including backslashes, iff the string
does not contain a backslash as its very first literal character.  If the
string does contain said leading backslash, then it is stripped from the
input and the subsequent characters are interpreted as including escape
sequences of various kinds.  A trailing single backslash either represents
itself or is an error but won't prevent string from being parsed out of
surroundings.  Optionally/ideally we can forbid some other characters from
appearing literally, particularly all control characters, or at least say
that the canonical Plain_Text form will escape them, eg so they survive
unscathed through various transports, but having them appear literally is
not strictly forbidden.
- Any consecutive delimited strings of the same kind are treated as 1 token
that is 'foo''bar' is equivalent to 'foobar' and "foo""bar" to "foobar";
this feature is not allowed for `foo``bar` which still counts as 2 tokens.
Among said consecutives, the internal leading backslash to indicate
escaping behavior only applies to one segment, so for example you can have
'foo''\bar''baz' and only the middle part is escaped while the end parts
are treated as is, especially useful if the literal contains something like
a regex or the middle part has a single-quote character in the data.
- In an escaped string, \q stands for a character of whatever the delimiter
is, so '\\q' is a single-quote and "\\q" is a double-quote while 'x\q' or
"x\q" is exactly what they contain.

* See http://en.wikipedia.org/wiki/Dependency_injection and
http://en.wikipedia.org/wiki/Inversion_of_control which describe some
things we already are doing as well as that we could do significantly more
of.  In particular, if for every regular system-defined type intended to be
used for user data, there was an associated abstract type which defines its
entire API, and users always defined their code in terms of the abstract
type, or other abstract types that defined portions of its API that they
cared about, then it would maximize the amount of library code that would
be reusable with different user code and user-defined data types.  That
said certain type definitions may be a notable exception where users may
prefer to name the composing types directly, eg for database relations.

* Remember when translating to/from SQL, "from foo" means "from foo foo"
and that with "from foo bar" that means "for each foo as bar".

* It turns out that RefEng implements the "flyweight pattern" re its design
to share object state where possible, though I'd never heard of "flyweight"
at the time I designed that feature.

* Make a new Muldis D codeless language module for BBEdit replacing the old
one that is completely obsolete.  Use things like "::= function" etc as
search pattern for the generated routines/etc menu.

* Add one or more new node types that make available the compilation/etc
context of code to that code, or other contextual information known at
compile or link time.  The source code of any registered or loaded package
could be returned as a Source_Code etc value and then studied like any data
structure at runtime.  Particularly important also is context-sensitive
info such as, what is the fully-qualified name of the routine asking the
question.  With these bits, we could basically implement all the
complicated stuff such as resolving partially-qualified names to absolute
or identity ones, or looking up or testing dispatch candidates, or reading
traits or other meta-data of types or routines, can be done in Muldis D
itself.  What actually needs to be low level is simpler stuff like "call
this (fully-qualified-named) routine".  This is valid to have in purely
functional code because, during the context of any single higher-level
statement execution, the values returned by such are constant, so its as if
these questions were asked and answered at compile time, as if the answers
were baked literally into the source code of the function asking the
question.  The "current" source code can not change but possibly between
statement boundaries, and it is infrequent.  Logically it is like every
function has been given and is passing an extra read-only argument etc.  An
example use case of this general feature, besides implementing $foo or foo
isa bar or default_of foo etc or "what is the type or MST of this value" is
letting one have multi-dispatch for "constants", eg, one could declare a
virtual constant, or more likely actually a function taking a type name or
example value, and then ask, eg call the function named "one()" for that
type, so eg the Numeric role could call a particular type's selector for
the number 1 in a generic way.  This is largely where one gets a lot of the
introspection power of Muldis D, eg meta-model etc access, eg, given a
value, you can then ask details about its type or what routines are
implemented for it etc.  Also makes the language more self-hosted.  This
stuff may possibly lead the way to being able to invoke functions on their
definitions rather than names, eg more truly higher order functions? 
Runtime information such as "tell me about my call stack" is not part of
this so there should be no worries about this breaking pure-functionality
or memoization etc.

* Make abstract types try and implement all operations where logically
possible, shades of Low_Level.mdpt, eg multiply in terms of add etc, and so
the logical semantics are recorded, but then implementing types would then
'implement' these non-virtual abstract type operators in order to be
faster, eg dispatching to \*^[integer-multiply] etc.  Of course we still
want to minimize the number of \*^[] and emphasize each implementation
choosing to override for itself for efficiency, but what I'm talking about
here is for the use cases where it would just be insane to not always
override, like integer-multiply, and so by having the \*^[] we declare we
expect all implementations must provide it, or something.  This policy can
be relaxed.

* We want to figure out rules for where particular syntaxes are overloaded
so to help consistency and predictability.
- The "." generally says "accessor" but its a question of how far one can
go with that.  In general it can't have both the Attributive and Collective
applications because Relation satisfies both, and then there's also the
question as to which of those two Tuple should satisfy.  Or alternately
there is some other role that "." is used for that is distinct from those.
- Where we have accessors that access single elements, we also need variants
for questioning the presence of that element or some other aspect.
- Where we have accessors for single elements, we need something for dealing
with multiple elements, though those would generally return another value of
the collection type and may more likely be type specific.
- Would be nice to have some symbolic syntax for has_attrs, has_just_attrs,
and for each versions that both do and don't check the types of those attrs,
these would be used a lot in definitions of attributive types.
- Is \foo just shorthand for \~{foo} or \$foo or something distinct?
Especially given the accessor context we probably want it to be distinct,
eg, maybe that's the new SC_Name or SC_Attr_Name.
- .? for has_attrs, .?= for has_just_attrs; variants take Heading or Tuple
to either not or so check the data type; SC_Name for singular of Heading
to check one attr name; you still need Tuple for the singluar due to needing
name plus type.
- Or ^? plus ^?= for the multiple versions of the above which looks like
projection, though don't forget that ^ has a host for complement or to-set
or to-bag etc so we need to play nice with that.
- We should reserve the dot for things that return single elements and not
slices of the collection, and something else for the slices, it really cuts
on confusion, and then one can use a single item in place of a list when
doing a slice and still get a slice, that's more the dwimmy we want.
So single elem accessors (always by key/index) or does-exists or exists-or-voids (vert+horiz or horiz):
  tup . \atnm
  tup .? \atnm
  tup .! \atnm
  cpsl .> \atnm
  cpsl .>? \atnm
  cpsl .>! \atnm
  ary . index
  ary .? index
  ary .! index  - returns elem val if exists or void
  dict . key
  dict .? key
  dict .! key
  bagsetrel .# member  - returns quantity for bag, else 1 if exists, 0 if no exist
  bagsetrel .? member aka 'has'  - returns bool based on existence
  bagsetrel .! member  - returns member if exists or void
So attributive (always by key/index) slices or tests (vert slice):
  tuprel ^ \atnm or tuprel ^ \~{atnm} aka 'on'
  tuprel ^? \atnm or tuprel ^? \~{atnm} aka has_attr_names
  tuprel ^?= \atnm or tuprel ^?= \~{atnm} aka has_just_attr_names
  tuprel ^? \%{atnm:$Integer} aka has_attrs
  tuprel ^?= \%{atnm:$Integer} aka has_just_attrs
  cpsl ^> \atnm or cpsl ^> \~{atnm}  -- returns as tuple
  cpsl ^>? \atnm or cpsl ^>? \~{atnm} aka has_attr_names
  cpsl ^>?= \atnm or cpsl ^>?= \~{atnm} aka has_just_attr_names
  cpsl ^>? \%{atnm:$Integer} aka has_attrs
  cpsl ^>?= \%{atnm:$Integer} aka has_just_attrs
  tuprel -^ \atnm or tuprel -^ \~{atnm} aka 'but'
  cpsl -^> \atnm or cpsl -^> \~{atnm}  -- returns as tuple
  rel ^+ \atnm  - project to Bag - if someone wants multiple attrs they can wrap() first
  rel ^+? \atnm  - project to Set
For collective (always by key/index) slices or tests (horiz slice):
  ary .+ 4 or ary .+ [4] or ary .+ [4,5,6] or ary .+ {4,5,6} or ary .+ (4..6)
  ary .+? 4 or ary .+? [4] or ary .+? [4,5,6] or ary .+? {4,5,6} or ary .+? (4..6)
  ary .+? $Integer  - do we want this?

* TODO: Think about matters of allowing strictness.
MD.Universal.Comparable ::= type abstract;
MD.Universal.Comparable.may_be_compared ::= function virtual
    --> Boolean <-- (Comparable, Comparable)
    is {commutative};
MD.Universal.same_Universal ::= function
    --> Boolean <-- (Universal, Universal)
    is {commutative}
    : \*^[same_Universal,topic.0,topic.1];
MD.Universal."=" ::= function --> Boolean <-- (Universal, Universal)
    is {commutative}
    : same_Universal(*topic) ?? true !! may_be_compared(*topic) ?? false !! fail;
Alternately forget about doing this here and just make checks for such to
be a domain of add-on code analyzers.

* It seems C# has a lot in common with what I designed so far of Muldis D
independently of that knowledge.
- Has both 'using' keyword and also local aliases for used 'namespaces':
    using Net = System.Net;
    using DirInfo = System.IO.DirectoryInfo;
- Also namespace disambiguation / like fully qualified names:
    global::System.Console.WriteLine(number);
    # Above works if someone declared a local thing named 'System'
- But C# 'using' seems to do both Muldis D 'using' and Muldis D 'searching'
since Muldis D 'using' doesn't possibly point to sub-namespace.
Or C# 'using' may be more like Muldis D 'searching'
and C# References declarations in project are more like Muldis D 'using',
or the 2 'using' may in fact directly correspond as all,
since 'using' not required to call identity/global things in either case.
- In C#, 'using' can appear either inside or outside a 'namespace' declaration.
- Regarding implementation of Muldis D over C#, see C# "dynamic" type.
    dynamic x = new Foo();
    x.DoSomething();  // Will compile and resolved at runtime. An exception will be thrown if invalid.
- In C# the 'set' accessor routine has the implicit parameter named 'value'
which would seem analagous to Muldis D's 'topic' at least there.

* In MS SQL Server:
- The money and smallmoney data types are accurate to a ten-thousandth of
the monetary units that they represent, so 4 decimal places rather than 2.
- The sql_variant type is a union type of sorts that can be used in db
columns among other places, though it is limited in its options.
- Given "select top 5 with ties ... order by foo", what the 'with ties'
does is, when multiple rows might sort equally into the 5th place, all such
rows are returned, even if more than 5 total rows may then be returned.

* I should consider a re-look or merger of Muldis D using/searching concepts
considering that to some extent they overlap functionality.

* Define some shorter-hand for common formats of types restricted from
generic collection types, analagous to eg Array<Text> or Set<Integer> etc
that many other modern languages have.  The shorterhand would take the
place of the "where" in common cases, or supplement it.  We'd want versions
both for restricting simple collections like
Array,Set,Bag,Interval,Quantity but also attributive ones like
Tuple,Relation.  That being said, we can see how short the likes of
has_attrs() or forall() can get, may be good enough.  But we still probably
want a few extra keywords for 'type' for the ones that can be really short,
eg "type of Array member Text" or "type of Tuple attrs (x:Integer,y:Ratio)"
where in the latter case we take the same input as a function's "from" but
disallow positionals for Tuple.  Both member/attrs are sugar for where with
function calls and those functions are defined by abstract types, so a wide
variety of Collective or Attributive types could take advantage of this ...
or maybe this would be a problem if the package doesn't use Muldis_D pkg?

* Define as many routines as possible just against abstract types, so only
a minimal number of routines need to be virtual.  For example, many of the
Numeric ops or Collective ops etc don't need to be defined per composing
type.  Likewise, define most Boolean/Integer/String/etc ops at
Boolish/Integral/Stringy level etc.
- Keep in mind that a routine can still declare that it implements another
routine even when the other routine isn't virtual, and in that case the
former is actively overloading the latter, and calls directly to the latter
will be redirected to the former if the signature is compatible.  That
is, when X implements Y, X will always take precedence over Y for callers
for whom X is directly visible, but when Y is not virtual then X won't be
called, so to avoid action at a distance.
- In that case maybe we want keyword to be 'overrides' instead so it is
more explicit that the named routine already has an implementation.
- Likewise define as many synonyms as possible against just the abstract
versions of things, to minimize duplication, even when there is an
implementing routine, unless synonym only applies to specific composers.
- See also requires_implements for type declarations which helps with
defining a contract and so we have more of a full interface definition.

* Look up Perl 6 Iterable role and other roles to see what we can mimic in
our abstracts/virtuals eg where routines are useful across types.

* Have procedure statement grammar nodes or \*^[...] for loading or
unloading or reloading etc packages at runtime.  Or call it mounting/etc.
- The loading version take a SC_Package value as input, which other than
any sanity checking it has to do, will cause that SC_Package to appear
verbatim in the system catalog, under the identity (base+auth+vnum) it
declares with 'package', will make it visible to identifier resolution and
routine dispatch code, and will compile it as applicable.  The loading node
will explicitly not load dependencies, and will fail if the package cites
any 'using' dependencies that are not loaded first.  It is invalid to have
circular 'using' dependencies between packages.  The act of actually going
to repositories or parsing to SC_Package values or introspecting the latter
to then go get dependencies, that is the job of higher-level code, and once
bootstrapped is performed in Muldis D code.  The loading code will fail if
a package with the same identity is already loaded.
- The unloading version either takes a SC_Package for consistency, though
it only needs package identity name, or it takes just a Reference of said.
- The reloading version is like an unloading followed by a loading, this is
used say when there is runtime manipulation / data definition of a package.
- Perhaps the nodes may be split into separate compile/register and
link/make-available nodes.
- Maybe use 'activate' for the term of when it is linked / made vislble to
run, thinking like the term with chemicals etc or military; similarly or
etc 'reserve' could be the namespace locking or compile/register.
- When registering a package, it can also be marked as immutable, and as
such later attempts to unregister or reregister will be rejected.  This can
be done for practical or security reasons for "built-in" packages or others
that should be treated as read-only rather than updateable.  To change
those the DBMS/application itself would have to be restarted.
- The separate register/mount stage is like a name reservation but the code
doesn't explicitly use memory when it isn't needed (or it still can behind
the scenes but users don't see it so its like its not there).
- For that matter, perhaps also have it defined to an extent in Muldis D
code canonically how to resolve Identifier in general to Reference, since
if we have to activate dependencies first, we can see all the candidates we
have to choose from already for resolution, what the invoker can "see".
- Perhaps the data dictionary is implemented at a higher level, as normal
variables, and updates to them would cause a reload/etc behind the scenes.
- So matters of persistence are orthogonal to execution.
- We would do persistence or repositories somewhat in terms of tied
variables or various kinds of I/O facilities whether files or otherwise.
- There may be multiple versions, eg original Source_Code and other that
has been optimized or folded, what was written vs what actually runs, may
be relevant for debugging or other meta-model related querying in functions.
- http://en.wikipedia.org/wiki/Java_Classloader may also be relevant.

* Update Plain_Text.pod concerning how and where the Script Name applies,
update the other parts on the post-20150226 design of packages vs modules
vs repositories etc.
- Split Script Name into 2 declarations, on encoding and on normalization.
- Example declarations at the top of a Module:
    Muldis_D:Plain_Text:"http://muldis.com":"0.200";
    script_encoding:ASCII;
    script_normalize:ASCII;
- Or:
    Muldis_D:Plain_Text:"http://muldis.com":"0.200";
    script_encoding:Unicode:"6.2.0":"UTF-8";
    script_normalize:Unicode:"6.2.0":canon;
- Only the script_encoding says how to map a Blob to a Text, and this is
possibly round-trippable losslessly depending on whether an encoding allows
multiple octet sequences for the same character codepoint.
- Note that valid options for script_normalize are {none,canon,compat}
which correspond to {none,NFD,NFKD} respectively.  There are possibly extra
terms for script_normalize if eg the compat rules changed independently of
Unicode main versions, eg with independent changes of annex versions.  It
is recommended to use canon by default as that most closely corresponds to
what users expect, probably, or compat possibly for security.
- See http://en.wikipedia.org/wiki/UTF-16 for explanations of surrogates etc.

* Formalize layout of package logical contents somewhat.
- A "package" is the primary logical unit for Muldis D code and consists of
a set of type/routine/constant declarations organized into a multi-level
hierarchical namespace.  Every package is versioned, its fully-qualified
name typically including a base name plus an authority and version number,
logical references to a package's contents are fully-qualified with such.
- A "material" is a {type,constant,function,procedure,synonym,etc}.
- A "folder" is a logical namespace that in/directly organizes materials.
- A "package" is a folder that has no parent folder.
- A folder may not directly contain both folders and materials; the
children of an folder must either be all folders or all materials.
- A "binder" is an folder that has at least 1 child material, and exactly 1
of those child materials is the binder's "cover" and must have the
unqualified name "" (empty string).
- A binder is conceptually a proxy for the material that is its cover from
the point of view of external users, and the other materials in that binder
are conceptually components of the cover.  A binder is a pseudo-private
namespace for its parts to reference each other but is automatically
excluded from "search" identifier resolution; "search" identifier
resolution will only ever resolve to a binder's cover.
- All explicitly named public "material" declarations actually are naming a
binder for which that material is its cover.
- All material declarations embedded in other materials
live in the same binder as what they are embedded in, the binder providing
a flat namespace for them.  Any non-cover materials in a binder may have
automatically generated names but could have explicit names.
- The term "schema" is not used to describe any kind of entity in the above
sense and is left available for use as generic database terminology.
- A "constant" with a true "folded" trait is the canonical way to represent
the current value of "the database".  In practice "the database" may be a
"binder" whose cover is a folded constant representing the database as a
whole, and other folded constants are in there too representing its parts
for easier human readability when serialized; in any event, on
serialization the folded constant's format/structure would be generated.

* Distinguish logical compilation units from their repositories.
- A "repository" is somewhere on or accessible to the system where packages
live, such as a "local" or "installed" OS file system, or some "object
store" service, or some version control system say.  When a user or
packages wishes to "use" a package, the available repositories are
consulted when looking for it.
- A "module" is a more physical manifestation of a package which includes
metadata of what version of the Muldis D language/grammar it is declaring
conformance to, as a Muldis D language name.  Usually this is something
like Muldis_D:Plain_Text:"http://muldis.com":"0.200" but depending on
context it might optionally declare the character encoding etc in use,
depending on whether the module is logically already in the form of a
character string versusan octet string for example.
- Muldis D code is often compiled per "module", and one can be completely
parsed and to some extent be compiled in complete isolation from others.
- Perl 6 concepts of CompUnit and CompUnitRepo loosely correspond to the
above package/module and repository respectively.
- A "module" may either be read-only or writeable during the course of
program execution depending on the context.
- A "module" is the level of concept that knows about segments, so for
example the module for Muldis_D is composed of several segments, one per
disk file, and the multiple segments are reflected in the homoiconic/native
form.  But the module as a whole then maps to a package, and at the package
abstraction level the package is in a single piece and is not split up.
- TODO: Expand on what read-only vs writeable means.
- TODO: Use the term "library" somewhere.

* Alternatives we have re Muldis D source code:
- A Blob value whose octets are of some potentially unknown raw format,
this is what is natively read from a generic file system or object store,
if it is composed of character data then repertoire and encoding unknown.
- A Text value where we know we have a string of codepoints; the repertoire
(eg ASCII or Unicode) is known and the encoding (eg UTF-8) is not relevant
or ceased to be relevant at the language level during the process of
conversion from Blob.  Mapping from the Blob to this Text value involved
the scanning for the declared script name or otherwise determing what the
character encoding was in the Blob, and mapping the string of octets to the
string of character codepoints.
- A Text value that, in the case of Unicode, has been effectively
normalized to NFD per declaration of canon vs compat, this decision also
driven by the script name declaration.  The script declaration is retained
for maintaining intent should we round-trip to Blob, but Unicode normalize
transforms are lossy, we don't remember the original codepoints as they
should not be considered logically significant.
- A Source_Code value derived from parsing the latter Text value according
to the language name declaration, where this Source_Code defines a module
or segment thereof etc; at this point the language name declaration is no
longer needed to understand the Source_Code, but it would be retained as
meta-data for the purpose of inserting the same declaration in code
round-tripped to the Text form (subject to changes while in SC form not
invalidating the language name declaration like compatible version numbers).
- A Source_Code value derived from mapping the Module to a Package or etc.
- So the sequence of parsing, where each is conceptually a different pass,
mapping a value to another value, but can be implemented in a combined pass
is: Blob -> Text anormal -> Text normal -> SC_Module, SC_Package.
- A Repository can validly store a Package in any of the above forms
directly, or others.

* Potentially packages could be used to represent symbolic math etc, eg the
numeric values ARE source code specifying how to calculate it etc as applicable.
- A multi-versioned database could be represented on disk as Muldis D
source code where each version is a constant-defining package that refers
to its parent version(s) as 'using' and its value is a constant derived
from those.
- A graph could be similarly represented.
- A non-small database could be represented in multiple files similarly where
different parts are constants which compose by reference each other.

* Allow foo(:.bar) or \%{:.bar} which mean foo(bar:.bar) or \%{bar:.bar}.

* Add or rename-to the keyword/concept semantic_type which exists mainly to
be composed by other types for the purpose giving semantic meaning to a
type, eg this represents money or this represents a duration, and that in
particular these have nothing to do with defining a common interface or
there is no implication that types sharing a semantic_type would have any
operators or interchangeability in common.
- As such, the abstract_type concept which does imply / is intended for
having a common interface, eg Orderable, this is a different thing.
- Both of the above are open union types cited by the types they are
comprised of, and not the other way around.
- In any event, the fact that semantic/abstract types are cited as parents
of other types is important as something that can be introspected, and its
not okay for them to just disappear or merge into a 'where' definition.
In that respect they are more like type traits associated with a specific
type name, and they are orthogonal in some ways to the concept of a type as
a set of values, as here the name is important.
- Logically then, semantic/abstract types actually can not be named in type
constraints per se and rather they only have a program behavior role in
resolving routine dispatch by extending the candidate list beyond those
normally visible in a scope.  As far as type constraints are concerned,
they are just equivalent to Universal.  Perhaps as far as routine dispatch
is concerned, maybe they can only be used in the signatures of abstract
routines and not concrete ones?
- Perhaps what we want is for this semantic/etc stuff to actually just be
additional traits of a "type ..." definition and not a separate thing.
After all, the "default" keyword of a type has nothing to do with its
definition as a set of values either.
- But types would still have to compose these using separate keywords
since we don't want them to contribute to the set of values, eg don't use
"of" because that would make the composer like Universal.
- ACTUALLY, if these semantic types are considered equivalent to Empty,
that is explicitly consisting of no values themselves, then they can be
named in "type union {...}" with the regular "of" types, so the composing
types then just inherit its "semantic ..." keyword/etc and not any values.
- Maybe lets merge {of,union} into one keyword, which may be spelled either
way for user choice/illustration (as they have the -->|to etc choice),
which takes a set of type names (if missing it defaults to the set of 1
item that is the maximal type) and the curly braces may be omitted if
exactly 1 item is named ... we can make the braces optional for other
commalist-set traits too; with this merger, "where" may be used with union
types, and also the canonical way to specify the empty type is to say "of
{}" (empty set).  As such, "default" is mandatory if "of" lists multiple
non-empty types where empty is defined by their own type defs having an
empty set for "of", recursively plus no "where" ... er, think this through.
On default values, make a set of all the parent types' default values and
then filter it by the current type's constraints; if the resulting set
doesn't have exactly 1 value, the current type must declare a "default".
- Any time a module Y calls X routine, non-abstract X directly visible to Y
are used first if any match the called signature, otherwise abstract X
directly visible to Y are used if any match the called signature, in which
case implementations anywhere on the system may be invoked if declared
somewhere to which the same abstract X is also visible to.  In order for Y
to prevent its call to X being implemented by some unknown remote code, it
must declare its own non-abstract X matching the signature or explicitly
use one such.
- Routines need to have some keyword(s) declaring them as abstract/virtual;
only those may be referred to with 'implements' by some other routine.
- Idea, let routines declare that they 'implement' other non-virtual
routines, as a way of overriding polymorphism, or declaring somewhat how
search-based calls should be resolved when several matching candidates are
found, useful in particular for subtype-specific implementations that are
more efficient, maybe use keyword 'overrides' or such.  However, this is
only respected when the overriding candidate in question is visible
directly to the caller, so such an override definition can't cause action
at a distance.  See also Perl 6 syntax/terms for this.
- TODO: Think about stuff and things.

* Type correspondance between Muldis D and Perl 6:
Abstract:
    Boolish - Boolean
    Integral - Integral
    Rational - Rational
    ? - Real
    Numeric - Numeric
    Stringy - Stringy
    Textual - ?
    Arrayish - Positional
    ? - List
    ? - Associative
    Setty - Setty
    Baggy - Baggy
    ? - Mixy
    Orderable/Ordered - Ordered
Concrete:
    Boolean - Bool (boolean)
    ? - Bit
    Integer - Int (arbitrary precision integer)
    Ratio - FatRat (arbitrary precision ratio)
    String - Buf|Blob (stringish view of an array of integers)
    Blob - Blob|Buf (undifferentiated mass of ints)
    Text - Str (string of characters)
    Array - Array
    Set - Set/SetHash/QuantHash[Bool] (unord collection of values no dups)
    Bag - Bag/BagHash/QuantHash[UInt] (unord collection of values allows dups)
    ? - Mix/MixHash/QuantHash[Real] (unordered collection of values with weights)
    ? - Enum/Pair (a single key-to-value association)
    ? - EnumMap/Hash (mapping of k-v pairs with no dupl keys)
    Tuple - ?/Stash (a symbol table hash for package, class, lexpad, etc)
    Interval - Range (pair of Ordered endpoints)
    Quantity - ?
    SC_Func_Args - Capture (function arguments)
    SC_Func_Params - Signature (function parameters)
    ? - Instant (point on continuous atomic timeline)
    ? - Duration (diff btwn 2 Instant)
Literals:
    1/2 - <1/2> (general case, or 1/2 depending on context as that is constant folding)
        but <1/2> produces a value that is both a Rat and a Str,
        see http://design.perl6.org/S02.html#Allomorphic_value_semantics
    0x'A705E' - :16{A705E}
Note that Muldis D purposefully excludes Complex numbers from core, so Perl
6's <5.2+3i> has no core analogy.  Part of the rationale is the
combinatorial explosion of Complex types, eg cartesian vs polar times ratio
vs float etc.  More broadly speaking, anything that depends on irrational
or symbolic math values like pi are excluded from core.
Typedefs:
    Odd ::= type of Int where ?(topic mod 2);
        subset Odd of Int where { $^n % 2 };
Perl 6 Multi-method dispatch goes only on parameters, return type not
considered, which is the same thing we do / how it should be done.

* Provide alternate versions of expressional if-else/??!! and
given-when-default that either do or don't shortcut respectively; the
former exists to provide a way to gate code that is only valid to execute
in some circumstances, while the latter is a logical foundation for
many/all expression operations but that it is valid to execute both
branches; we should have the latter so that these expressions don't act as
an optimization barrier forcing conditional exec when they don't need to.
Note, see also http://design.perl6.org/S02.html#Lists eg re eager/hyper/race.
- So what we'll do is make the alpha syntax if-then-else and
given-when-then-default the one that does shortcut, which is consistent
between the expression and statement versions of said as well as common
practice / expectations in the world.
- The ??!! syntax is altered to be non-shortcut version of if-then-else and
it has a separate expr node type from if-then-else; users can optionally
wrap it in a function if they wanted to.
- A new ???{}!!! syntax is a non-shortcut version of given-when and in
parallel to ??!! it is infix.  Example:
    .round_rule ??? {
        $=>To_Zero : ...,
        $=>Up : ...,
        ...,
    } !!! fail
- So generally the language is such that for operator-like things which
have more than 1 argument and aren't called like foo(), everything that
doesn't shortcut is used infix-only, eg "foo and bar" or "x ?? y !! z"
while things that do shortcut have a keyword before the first argument in
addition to having infixes, eg "if x then y else z".

* Add several new expression node types / fundamental operations which are
list map and list reduce etc.  Syntactically they would be like a generic
function call syntax but that they call the other function per element of
the provided collection-typed value rather than exactly once.  Have
distinct ones for Array and for Bag; the former is needed for
non-commutative operators.  These are not low-level routines so they can
properly see the execution environment same as routine call syntax does
such as for resolving what to invoke and also for reading the declared
traits of routines such as is-commutative etc.  See also
http://en.wikipedia.org/wiki/MapReduce including the map-shuffle-reduce
threesome and key-1 plus key-2, I would want to support a similar
algorithm.  When calling map on a Bag source, there would be a single
routine call per distinct Bag element, and that call will be given the
key-quantity pair so can act appropriately; for a non-idempotent routine
preserving or acting on the quantity is important while for an idempotent
routine the quantity can be nullified.  Defining list operations in terms
of these should make it much easier for implementations to parallelize them
including over engines based on map-reduce.  That article should also give
me a clue on a parallel Plain_Text parser definition.  All the standard
relational operators etc should be definable in terms of these.
- http://search.cpan.org/~drrho/Parallel-MapReduce-0.09/lib/Parallel/MapReduce.pm
is also instructive.
- http://en.wikipedia.org/wiki/Map_(higher-order_function)#Language_comparison
- http://en.wikipedia.org/wiki/Fold_(higher-order_function)#Folds_in_various_languages

* See http://en.wikipedia.org/wiki/Language_Integrated_Query (LINQ) also for
inspiration of what operators to have etc.

* Leave -> available to be defined at the function level, hence it can be
overloaded for a wide variety of function types; we have use variants of
--> instead specifically defined in the grammar where we need those.

* The 2 main user-defined stages of the generic map-reduce algorithm
correspond more to our 'classify' and 'summarize' and Muldis D will use
terms more like the latter for referring to parts of the map-reduce
algorithm; the Muldis D terms 'map' and 'reduce' will have simpler meanings
corresponding to those of Perl or many other languages.

* The following is partly obsolete;
- The fundamental Collective ops now look like:
    - is_empty(list)->bool  # like count(list)==0 if most count work saved
    - count(list)->nninteger
    - has(list,universal)->boolean
    - where(list,func)->list like Perl 5 grep is
    - map(list,func)->list like Perl 5 has
    - reduce(list,func)->universal
    - group(list-of-pair)->list-of-pair
    - ungroup(list-of-pair)->list-of-pair
- Non-fundamental Collective ops:
    - in(universal,list)->boolean
    - any(list,func)->boolean  # short for map to bool plus [or]
    - all(list,func)->boolean  # short for map to bool plus [and]
    - none(list,func)->boolean  # short for map to bool plus ![or]
    - one(list,func)->boolean
    - does forall or exists go here / are they other names for all/any?
    - etc
        # To optimize we need a way to annotate functions so we know we can
        # stop once a particular condition is met, for example, with 'or'
        # reduction we stop once we see a true, 'and' when we see a false.
    - generate(state[func,args,list],nninteger)->state
    - ...
- The additional fundamental Array ops:
    - has_index(list,nninteger)->boolean
    - elem(list,nninteger)->universal
    - replace_elem(list,nninteger,universal)->list
    - slice(list,range)->list
    - replace_slice(list,range,list)->list
    - sort(list,func)->list
    - zip(list-of-list)->list-of-list
    - index_first_diff_elem(list,list)->nnint
        # For making in_order(list,list) efficient in both cpu, stack use,
        # result index may be just after last element of either input.
        # Alternately we could maybe express by piping simpler ops if we
        # can declare that we keep processing only while elements match.
- Non-fundamental Array ops:
    - subsequence_of(list,list)->boolean
    - supersequence_of(list,list)->boolean
    - ...

* TODO: Add further traits or external declarations that further help with
optimization, such as declaring distributivity, transitivity, negator,
commutator, inverse, monotonicity, truth preserving, false preserving,
symmetry, injective, surjective, bijective, isomorphic, other things.
- More broadly speaking, it is useful for an implementation to be able to
pipe several chained list operations when there is some declaration saying
it is safe to do so, especially say chaining map to reduce so we know the
map can stop when the reduce produces a particular result part way through.
The concept of http://en.wikipedia.org/wiki/Monad_(functional_programming)
may be relevant here.
- Have trait declaring on routine R1 that when we have a repetition of
calls with the same argument/s, we can instead call routine R2 once for the
whole sequence, which will do the same result more directly.  This is
useful when doing a reduction on R1 with a Bag of non-zero quantity
elements, or reduction on an Array with consecutive same elements, etc.
This trait should be useable in principle with any routine, doesn't have to
be commutative or whatever, but it would be invalid to declare for a
routine also declared as idempotent.  Examples are:
    - counting -> add
    - adding -> multiply
    - multiply -> exponent
    - catenation -> replication (Perl 5's "x" op)

* Per http://en.wikipedia.org/wiki/List_comprehension the main thing we're
missing is a generic generator function or three.  For example:
    - MD.Generator.init(func,seed)->state
    - MD.Generator.next(state,nninteger)->state
... where state is some Capsule type whose attributes are:
    - a CS_Reference to the generator function
    - a CS_Func_Args of arguments to give the next exec of that function
    - the last values that the function generated
    - note that besides the generated value, the function returns a new set
    of args to give its next invocation, which lets it maintain its state
    and lets us have effectively a generator of an infinite sequence
... and a simple common example, the function just wraps Integer.succ.
- In some way the Ordinal role is related to this, as anything which
composes Ordinal can generate sequences.  However, the same base type eg
Integer could produce any number of different sequences, for example the
Fibonacci or multiply by -1 etc.
- We need to look up how Perl 6 or other things do lazy lists including the
Iterable role and x...y syntax etc.
- We want an Iterable abstract such that every composer provides functions
like curr_val(state) and next_val(state) etc.

* What Muldis D would have as fundamental operators ...
- read List as either Array or Bag but not both in the same function
- map() takes 1 monadic function plus N inputs and has N outputs.
    - The monadic function is --> Universal <-- Universal
    - map() itself is --> List <-- (Reference,List)
- reduce() takes 1 dyadic function plus N inputs of the kind that map takes
or outputs and has 1 output.
    - the dyadic function is --> Universal <-- (Universal,Universal)
    - reduce itself is --> Universal <-- (Reference,List)
- sort takes 1 ...
... actually that still needs a lot of work.
- classify takes 1 monadic function of the same kind that map takes or
outputs plus N inputs of the same kind that map takes or outputs and has M
outputs where each output is a class-members pair such that there is 1
distinct 'class' per distinct output of the function and 'members' is a
collection of all the pristine inputs that mapped to this particular
function output, so the count of elements of members across the M outputs
is N.
    - the monadic function is --> Universal <-- Universal
    - classify itself is
    --> (List of Tuple{class:Universal,members:List}) <-- (Reference,List)
- summarize takes 1 dyadic function of the same kind that reduce takes plus
M inputs of the kind that classify returns and has M outputs where each
output is a class-summary pair, the summary produced by running reduce on
the group.
    - the dyadic function is --> Universal <-- (Universal,Universal)
    - classify itself is
    --> (List of Tuple{class:Universal,summary:Universal})
    <-- (Reference,(List of Tuple{class:Universal,members:List}))
- At the low level, for each of the above 4 functions, there are 2 variants
where one is Array to Array and the other is Bag to Bag.
- For reduce/summarize, all dyadic functions are permitted to be used with
their Array variants, but only dyadic functions which are both associative
and commutative may be used with the Bag variants.

* About low-level 'map' ...
- Very simple, processing each input element yields exactly 1 output
element, and each element is processed entirely in isolation from the
others.
- Where several input elements are the same value, their corresponding
output elements will all be the same value.  The 'map' is pure, and the
system can choose to memoize to arbitrary degrees, either calculating the
output for each distinct input element once, or once per instance, or
somewhere in between, this is an implementation detail that has no logical
effect, aside possibly from debugging.
- There are 2 variants, one being Array-->Array and one Bag-->Bag; use the
first when you have elements in order and their corresponding output
elements need to preserve that order; use the second when your elements are
not conceptually in order and you don't need to preserve that order, but
just the relative number of corresponding values.
- The processor function for 'map' will never be given context information
like index of the input in the source Array or like quantity for the
distinct element in the source Bag, rather the 'topic' will just be the
distinct element itself, and the output is just the direct transform
without any mandatory-for-all-map output format eg specific to a followup
classification or reduction use case.  One reason is to keep the API simpler
and more consistent no matter the source collection type or intended use
case of the output.  Another is so that the processor function's answer
can't possibly change due to factors that should be agnostic to what
sibling elements exist in the collection.  In particular, giving quantity
information to a 'map' processor function per distinct element would
require deduplicating the whole input data set, either unnecessarily or
prematurely, in order to give guaranteed consistent results for map
processors that pay attention to said quantity, and so would either break
our ability to parallelize the operation effectively or force an eager
classifying when otherwise not logically useful.

* Possible 'map' syntax based on general function call ...
    args --> func  # call func once with pristine args
    array_of_args [-->] func  # call func once per array element
    bag_of_args {-->} func  # call func once per distinct bag element, input is element+quantity pair
Or maybe:
    array_of_args -->~ func
    bag_of_args -->+ func
    set_of_args -->? func
Or better:
    ~-->
    +-->
    ?-->
... so the variant is on the same side as the input it varies on.
Then there's the reduce version, needs versions for: not assoc or commut,
assoc only, commut and assoc; commut only doesn't work in this context or
we treat as not assoc or commut.
Maybe have version like chaining associativity in Perl 6, so its a different
kind of reduce.

* Consider providing 2 implementations of the Muldis D parser in Muldis D,
one functional that parses a Text value to Source_Code, and one procedural
that parses from an input stream instead.  Conceptually the former is what
we really want, but the question is how to work with arbitrarily large
input.  Maybe the answer is for the former to be the canonical parser that
the reference implementation includes and latter can be a third-party-alike
add-on.  Possibly for the best as RefEng keeps the result in memory anyway
so even if the parser works on streams what it feeds to doesn't.

* Adjust or prevent any language grammar features that would make it too
hard to implement a parser functionally as essentially a finite series of
map-reduce-ish operations, eg an early round is tokenize
simply into is-inside-quoted-string from is-outside-quoted-string, so
perhaps things like here-doc quoting might be an issue?

* See "Coding an operator to test a string for numeric" thread on TTM list
for 2015-03-24 for a related discussion.

* NEW PLAN (overrides 'syntax_using' etc) ...
Eliminate Muldis_D::Low_Level package and merge the appropriate parts into
the standard grammar itself.  This brings consistency as not only do the
value literals for Integer etc have their own grammar nodes, now all the
low-level routines do too.
- See also SC_Bootstrap_Singleton further below for a related concept.
- So now there is no question what behavior keywords like "not"/"!", whether
meta-op or not, bind to; they always bind to grammar nodes.
- Now nothing in Plain_Text or its parse tree / native Muldis D binds to
something declared in a package.
- If there are zero 'using' in a module, then there are also zero ordinary
references to either type names or routine names, except for those declared
in the current module.
- The Muldis_D standard package has no dependencies and is defined entirely
in terms of the grammar nodes; eg Integer plus,minus etc are now nodes.
- A lot of Muldis_D routines are just shims over the nodes, consistently.
- Example nodes:
    - SC_Bootstrap_Func_Invo - use like "args --> $name" or "name(*args)" for low-level
        - \*^[same,expr,expr]                  # used by = syntax
        - \*^[Boolean_not,bool_expr]           # used by not/! keyword
        - \*^[Integer_plus,int_expr,int_expr]  # used just like this
        - \*^[select_Pair,expr,expr]
        - \*^[Pair_key,pair_expr]
        - \*^[Pair_asset,pair_expr]
        - \*^[String_elem,str_expr,nnint_expr] # used by x.+[y] syntax
        - \*^[Array_elem,ary_expr,nnint_expr]  # used by x.[y] syntax
        - \*^[Tuple_attr,tup_expr,atnm_lit]    # used by foo.bar syntax
        - \*^[Tuple_has_attr,tup_expr,atnm_lit]  # used by foo.?bar syntax
                # related foo.!bar shorthand "foo.?bar ?? foo.bar !! $=>Void"
        - \*^[select_Capsule,ref_expr,tup_expr]  # wrapped by x=>y function
        - \*^[Capsule_attr,cap_expr,atnm_lit]    # used by foo.>bar syntax
        - \*^[Capsule_has_attr,cap_expr,atnm_lit]  # used by foo.>?bar syntax
    - SC_Bootstrap_Proc_Invo - as you would expect
        - \*^&[Universal_assign,target_expr,source_expr]  # used by x := y
        - \*^&[Array_elem,ary_expr,nnint_expr,expr]  # used by x.[y] := foo
        - \*^&[Tuple_attr,tup_expr,atnm_lit,expr]    # used by x.y := foo
- Note that some of the above operations may instead get their own nodes.
- ACTUALLY, THE ONLY PLACES SYNTAX LIKE THE ABOVE WOULD BE USED IS WHEN
THAT WOULD ALWAYS BE USED TO BOOTSTRAP ORDINARY ROUTINE OPERATORS; WHEN
THE GRAMMAR ITSELF HAS SPECIAL SYNTAX FOR AN OPERATOR, IT DOES NOT GET ONE
OF THE ABOVE NODE TYPES BUT RATHER ITS OWN.
- Note that we don't need to do this for constants because the constants
that the grammar knows about have their own node types as with previously.
- Nodes look unpleasant/verbose on purpose as people aren't supposed to use
them directly, but rather by way of the Muldis_D package shims.
- All normal low level foundation types are now declared by Muldis_D package
including Universal and Integer etc; for example:
    MD.Universal ::= type where true default false;
    MD.Empty ::= type of Universal where false;
    MD.Integer ::= type of Universal where \*^[isa_Integer,topic] default 0;
    MD.Integer.NN ::= type of Integer where topic >= 0;
    MD.Integer.P ::= type of Integer.NN where topic > 0 default 1;
- Generally, \*^[isa_foo,topic] only exist once; other code uses "isa foo".
- The "isa" node looks up any "type" per the same visibility rules as normal
dispatch to "constant" or "function" and invokes the chain of "where"
predicates starting with the parentmost "of" back down to the invoked type,
and results in true iff all the "where" resulted in true.
ACTUALLY "isa" shouldn't do any coercion like that, one should have to say
for example "foo isa $Bar" and not "foo isa Bar".
- Demote isa to a function over \*^[isa,expr,ref_expr].
- So "=" (equality test) is now specially recognized by the grammar, same
as each of "!",":=","::=",":","-->" as it just seems appropriate; as a
side-effect using a package declaring a symbolic "=" must now be invoked
quoted to get that one, where bareword = now means the standard one.
- Likewise ":=" is a special grammar node with no \*^[] for it.  Ah er?
- Note that zero boolean operators are necessary at the grammar level,
other than the constants false and true; they can all be defined in terms
of ??!! and canonically are in the Boolean package.  The only reason
there's a grammar node for "not" is because its a meta-operator, and this
grammar node just short for "<predicate>??false!!true" (no \*^[] for it).

* Need to add something to Plain_Text et al like
"syntax_using <alias> ::= <package name>"
which is like "using" but that it has the additional effect of defining
what Plain_Text itself implicitly uses for its various keywords or special
syntax and what they bind to.  Mainly affects what its meta-operators bind
to such as "not" or "reduce" etc, and they bind with "absolute" paths.
- The alias declared with syntax_using is forbidden from appearing in 'searching';
if you want to search the package, you have to declare it with 'using' also.
- Specifying syntax_using is optional, and if omitted, the alias defaults to
the empty string, eg `::""`, and what the package defaults to is
parser-implementation-defined, typically Muldis_D::Plain_Text.
- As such the empty string package lexical alias is reserved.

* Lookup 'abstract' further down this file regarding how identifier resolution
works.  The rest of this paragraph either amends or supplements that.

* Make parser and compiler as simple as possible such that each one has
practically zero knowledge of runtime issues such as the routine dispatch
system.  Each Muldis D package becomes a Perl 5 package, 1:1, and every
Muldis D routine call is mediated by a runtime routine like
call_function(ident,args) so the compiled Muldis D code is isolated from
all that complexity.  Since most MD code goes through Low_Level to eg do
all the math or array stuff or whatever, there's no Perl equivalent
translation to worry about.  All binding between compiled packages is done
strictly at runtime and call_function(ident,args) is the thing savvy to it.
Actually it is resolve() or something that provides a set of 0..N candidates,
each a Reference, based just on the names, and call_function() then picks
one based on the argument data type.  Only 'search' can produce more than
one result, all others produce exactly one, which fails if it doesn't exist;
for relative and absolute we can know this at compile time, and search only
if the current package doesn't use any others, and identity only if pointing
to the current package.

* Have 2 main classes of comments, where the first class is treated as
documentation and the second class is treated as whitespace.
- The /*...*/ form is treated as whitespace and is preserved in the parse
tree in exactly the same way as whitespace is, and round-trips in the same
way.  It is not useful to introspect these kinds of comments, and they are
mainly for things like visual dividing lines or commenting-out code.
Given especially the latter use, arbitrary bracketing strings may be used
like with heredocs or quoted-printable, so commenting out code with comments
is easy enough.
- Where we desire the comments to be introspectable, they appear as part of
regular the regular code as Text literals along with some keyword or syntax
or routine name to associate them with particular things, eg :::= .
- Where ::= is what-binding, :::= is why-binding, maybe call ::?= ?
- Any parse node can be given a comment.
- :::= binds tighter than ::= which is tight.
- You can do:
    x :::= 'the answer';
    x ::= 42;
  or 
    x ::= 42 :::= 'the answer';
  or
    x :::= 'the answer' ::= 42;
  which all do the same thing.
- Parens may be needed like with ::= to ensure binding is to the right thing.
- Same method works for whole routines etc, todo how to work with param decls etc.

* Make it ideomatic that if a capsule or tuple type has an attribute that
is inapplicable for some values, that attribute is actually missing for
those values, rather than existing and being populated with for example
some special sentinel value.  Related to this, the terse infix ".?" like
"tup.?atnm" is provided to test the existence of an attribute.  Mainly this
idiom is for cases where there isn't any reasonable "default" value for the
attribute to conceptually use when it isn't explicitly provided.  Case in
point for a type defining a type; "of" and "default" should be missing if
not defined; only "where" can default (to "true"), and even then, should
ideally be missing as well when not explicit.

* Let routines/types/etc have traits like public/private whose effect is
making them visible or invisible to partially-qualified identifier
resolution to identity identifier operations.  If not explicitly stated,
a material explicitly named by a user is public by default, whereas one
whose name is generated or that is embedded in another, the latter is
private by default.  Due to this feature, splitting a package into a
private internals and public wrapper should not be necessary.
That being said, support for declaration of multiple packages within a
single module / .mdpt file should be supported, where a common use of said
may be private/public separation.
Note that the public/private trait would attach to the namespace/name of
the material, not to its definition; eg, it belongs on the left hand side
of the eg "name ::= function...".
* OR WE HAVE THE CONCEPT OF LEXICAL ROUTINES INSTEAD OR ADDITIONALLY;
ALL ROUTINES DECLARED INLINE ARE OF THIS KIND.  THEY CAN STILL BE INVOKED
ANYWHERE IF YOU KNOW THEIR IDENTITY BUT SEARCH WOULD NEVER FIND THEM.
See also http://design.perl6.org/S02.html#Scope_declarators eg 'anon'.
* The need for public/private is lessened by more recent developments, eg
the binder/schema split.

* Or we CAN specify a "complete" grammar but that as written you couldn't
actually execute it without help, eg interpreting several barewords in a row,
and so it is still more illustrative; you wouldn't gen a parser from it.
Call the file that defines this illustrative grammer Muldis/D/Plain_Text.pod
and so actually that is a replacement for Plain_Text.pod but it also more
explains the meaning of things rather than referring to Catalog_Types.
Focus example code here probably though Basics etc can have some too.

* In grammar, make it so 2 consecutive quoted string tokens, those enclosed
in single-quotes or double-quotes, will catenate at the parser level and be
treated as a single quoted string token.  This is a way of splitting long
strings over multiple lines without explicit catenation or special
linebreak treatment.  In particular should be useful for splitting
identifiers, as you can't just use ~ for those.  This feature will
emphaticly not be available for backtick-quoted string tokens, both as it
would conflict with things like "x inop preop y" and also no one should be
using such long names with that format; they can use foo() syntax then. 
We still want to remember what happened as meta-data though, probably.
Apparently "infix blank operator" actually quite popular among languages.

* 20150126 comment on TTM list:
The @" convention of C# (and others?) is very useful for multiline string
constants, where you want the final string to look like what is written in
the code (including carriage returns etc). Especially where the string
contains code in another language (eg GLSL code embedded in C#).
- Also, different post same day, perhaps "discriminated union type"
might be a term for describing what an abstract type is?

* Make Low_Level just have Integer division func that rounds in one way,
specifically round to zero, and let any other options be implemented at a
higher level in Muldis_D proper.  Or at least the single function taking an
enum of rounding method is not in Low_Level; the closest is Low_Level might
give more than one distinct divide operator if reasonable.

* In new Basics.pod, start out by introducing the concept of packages and
how all code lives in one or another.
Then say you can only invoke any types or routines, including standard ones,
by default if you explicitly use the packages declaring them.
As an exception, some routines or value literals are implicitly made available by
the Muldis D Plain Text syntax itself, specifically those that the grammer
as special syntax for, such as assign() and not() and reduce() etc but that list is small.
First introduce the things you get just because your code is standard Plain Text
and distinguishing what you have to 'use' a standard package for to get.
All invocations due to implicit grammar will use the 'identity' identifiers
so there's no chance of some 'use' changing their behavior.
Then introduce the most commonly used things that you wuould explicitly use
::Low_Level or Muldis_D proper or ::Unicode etc for, for each one.
An open question is whether using '=>' or '..' etc requires a 'use'.
Declaring :Plain_Text:Unicode just says what literals may exist in the source
eg as identifier or Text values, it doesn't give you the sysdef unicode operators.
Introduce the literal syntax for common data types like numbers etc or
everything that has a special parser-recognized syntax.
Generally speaking Basics.pod should now feature what used to be Plain_Text.pod,
not specifying a complete grammar but by outlining it and giving examples.

* Consider reserving the usage of terms like 'operator' and 'operand' to
being a syntax/grammar feature describing what syntax a routine call takes,
rather than it just being an alias for 'function' etc.  For example, if you
have 3 barewords in a row and either no external context or external
context says that is the whole row, eg delimited by parens, then we say
those barewords are (<operand> <operator> <operand>).  So, we say
backtick-quoting something will force it to be an 'operator' while
doublequote-quoting something will force it to be an 'operand'.  Or maybe
better yet, say 'term' or such rather than 'operand', so eg (<term>
<operator> <term>) and that may be standard practice; so then the operand
isn't a specific thing, it could be a whole multi-part expression itself.
A syntax rule is no 2 terms in a row, but 2 operators may appear in a row
iff the one on the right is a prefix op or the one on the left is a postfix
op, this being disambiguated in general by postfix having a -> marker.

* Note the following is valid Haskell syntax:
    r1 `compose` r2 = (r1 `join` r2) `remove` (r1 `meld` r2)
- the function in backticks is Haskell's standard infix apply

* Make Muldis_D::Low_Level provide multiple selectors for Set values where
some are shorthands/optimized to produce the kind of Set values that are
sets of Tuple eg for implementing Bag or Relation.  The Bag alternate
selectors will for example merge any tuples adding the counts rather than
just eliminating or keeping duplicates; they would also set a property so
that this can be lazy, eg when same() is run and deduplication must occur,
the flag will cause counts to add rather than eliminate.
Or just make Set and Bag disjoint Low_Level types that happen to share
nearly all implementation code, and that distinction is the flag, ok then.

* To keep the first Muldis D implementations much simpler, that is to give
us our 80% solution with 20% of the work, don't try and support any kind of
pseudo-variables.  This means all database updates are direct assignments
to the whole dbvar.  While somewhat awkward for updates, a lot of the power
in Muldis D is demonstratable with what we have left, everything you can
do in SQL SELECTs plus all the typical stuff of general purpose languages,
albeit with some degree of circumlocution, but most non-db variable
assignments are often done directly anyhow and not updating elements,
and that can be done with eg "ary := assign_elem(ary,index,v)" for now.
Or maybe just supporting array elems and tuple elems etc as subject to update
targets will go a long way for usability with little effort.

* NOTE: See latest Perl 6 synopsis 11 and 22 as my concept of Perl 6 long
names and package management is rather out of date, more there I should follow.
- See CompUnitRepo .
- Figure some reasonable code directory pattern to account for storing
multiple versions of the same base name, and also dealing with segments
while also allowing :: in module names to correspond to directory
boundaries.  There could be more than one pattern, see CompUnitRepo.
- Perl 6 has distinct 'need','import','use' where 'use' is simply a shorthand
for the prior 2 in order.  A 'need' is like Perl 5's "use Foo ();" or
alternatively like a compile-time Perl 5 'require'; in Perl 6, 'require'
will also import symbols, at runtime.
- Also, "assuming" keyword is not about "currying" but rather "priming" or
"partial function application"; P6 spec changed in 2011 Oct to more correct
terminology.

* Add new type Quantity which is generic in the same manner that Interval
is generic.  It associates unit names with numbers.
TODO, FLESH THIS OUT PER DETAIL AT: See 2015 Jan 31 - Feb 1 TTM list discussion about it.
But "Subtypes of Capsule" literal syntax list further down this file also says a lot.
- A normalization, eg between cm-cubed and in-cubed, or W = A*V, is simply
defined as a Quantity that is multiplied by one to get the other, or such.

* See http://www.boost.org/doc/libs/1_57_0/doc/html/boost_units/Quick_Start.html .

* Make Muldis_D::Low_Level package API correspond exactly to what would be
hand-implemented in each host language.  API provides and uses just the
disjoint types {Boolean,Integer,Array,String,Set,Tuple,Capsule,Identifier}
plus Universal however Universal is treated as a union type over said and
its internal representation is hidden.  API provides no means to introspect
a Universal in terms of its own possrep, the latter being for now relegated
to a concept rather than something exposed in the language. All selectors
and routines in the API just have parameter or return types that are of
those same 8 types.  The API will include the fundamental operators of each
of the 8 types, at least the minimum needed in terms of which all others
can be defined, eg basic math ops.  The API will include the generic =
operator.  Each host language will have its own hand-written package
providing the same API.  The Muldis_D package will just be written in
Plain_Text and a parser/compiler hand-written in each host language will
translate it into equivalent host language code, same as with user code.
Also, for now, special features that would only be used for defining
Low_Level in Muldis D itself will be left out of the standard
Source_Code/etc types and out of the Plain_Text grammar, including Universal's
own literal.  Thus we avoid unnecessary work done only for theory.
Selecting Universal values not one of the 8 won't be supported.
Any Low_Level package written in Muldis D will just be illustrative.
Leave the name Low_Level rather than say Foundation/etc as its direct use
by users is not recommended; all of its API will have shims in Muldis_D.
The package Muldis_D::Low_Level actually supplied will have ... in the
appropriate places of all type and routine definitions, and nothing will
be synonyms of eg Muldis_D::Low_Level::_ something; the latter won't exist.
As such, the Muldis_D LL will be strictly be interface definitions.
As to the public/private separation of Muldis_D, that will probably remain,
where the public is just synonyms of some of the private.

* To help prove the above point, maybe skip actually trying to define and
compile Muldis_D at the start, and rather write some user-like test
programs that just use Low_Level directly, which users can do anyway.  As
such, Low_Level will need to include procedures like for STDIO as well ...
which, makes sense. Said programs would be more mathlike though maybe as
Text isn't in low level; or minimal Muldis_D could be defined that adds it?

* Maybe skip the parser initially too, and just test natively in Perl too,
as if the host-native Low_Level API were just providing a Perl package for
Perl programs.

* All the concrete Source_Code types should be defined just in terms of
Low_Level types; in particular, none should be defined in terms of
Relation, generally Capsule and Array should do it for collection types
as far as what the canonical Muldis D code is composed of, what Plain_Text
parses into, what the compiler needs to generate Perl/etc.
- Source_Code will not define system catalog relvars, those are elsewhere.

* Add expression node types/etc which let one add hidden meta-data to
values or variables or whatever, particularly for performance hints or
indexing strategies etc.  These are expressly write-only from the point of
view of Muldis D programs, or at least they are within
expressions/functions, so that this can't be used to change the semantics
of functional/pure code, as no logical decisions can be made based on
meta-data read in-situ.  The only places code can read meta-data is in
procedures.  Said hidden meta-data is also often useful for debugging and
the like, not just performance.  Maybe good to also have expr nodes that
can cause side-effects outside the logical flow of the current transaction,
such as message sending to other processes or autonomous transactions.
Still write-only, reading can only be done by procedural code.
The initial point in this bullet is a better alternative to adding traits
to type definitions or whatever about indexing strategies.
Having this may also reduce the need to swap out Set and Dictionary.

* How about make Dictionary a Low_Level type and demote Set to being
Capsule type defined over said.  A Set is a Dict where the keys are the set
elements and the values are always the value False (or alternately always
True); a Bag is a Dict where the values are always positive integers.  This
change brings uniformity to Low_Level types in practice such that all
collection types are essentially a map of unique-key to
possibly-nonunique-value pairs, where the key is either a dense sequential
integer (String,Array) or a non-dense String (Tuple,Capsule) or a non-dense
Universal (Dictionary).

* Actually, Set is going back to the Low_Level and Dict is being demoted,
the latter had problems with semantics, eg what to do if one adds multiple
elements to a Dict at once where there are duplicate keys with distinct
values.  So Set is the Low_Level, Relation is a Capsule over it, and both
Bag plus Dict are Capsule over Relation, or in parallel to it.
That's not to say that Low_Level can't expose useful functions for Set that
are specific to certain Set of Tuple maybe?

* Maybe rename Low_Level to Muldis_D::Foundation/etc to reflect that maybe
it isn't actually a problem after all for people to use it directly, though
those uses should still be limited, and the longer name than plain Muldis_D
suggests this.

* Any non-lexical entity may be referenced or invoked directly by code that
knows what its fully-qualified name is and refers to it explicitly using a
Reference literal, even if the invoking code didn't explicitly "use" the
package declaring said entity.  But declaring package "use" must be done
for any non-Reference Identifier to resolve.

* Everything the standard Muldis D parser knows about must be expressible
just in terms of what Low_Level provides, meaning all the meta-operators
like ! and [] and >><< etc must be defined by Low_Level.  But types such as
Relation or Text etc don't have to be because the parser can take their
special syntax and just parse them into something like Capsule literals.
- As to the matter of, what does the standard parser know what auth+vnum of
Low_Level etc to use if the code being parsed didn't declare directly or
indirectly, well the parser module itself did "use" Low_Level itself, so
that does answer the question.
- Idea, make the package providing the Muldis D parser just "use" Low_Level
and not "Muldis_D" in general; ditto the Unicode-understanding flavor.

* Demote Relation from lower level to the level of {Float,Text,etc},
replace with generic Set at lower level.  Relation is now a Capsule whose
body attribute is a Set of Tuple.  Maybe add other Relation variants that
are also Capsule and are peer to it.
Example variants:
  - heading isn't stored, just emergent from body, just 1 empty relation value
  - stored heading just attr names (the current way)
  - stored heading includes declared types of attributes
  - things meant to emulate SQL now on more equal footing re implement level
ACTUALLY, just keep Muldis D's current paradigm (#2) as the one true
Relation, and simply refer to #1 as a set of tuples, and keep the likes of
#3 out of the core, that considerably more complicated option can be the
domain of an extension library, especially as it would need its own tuple
etc types maybe?
- Maybe generalize the @ sigil to mean homogeneous collection, and apply it
to both arrays and relations and sets etc, having twigils as necessary.
- Canonically a Relation is over a Set of Tuple, and often done similarly
at implementation level as many languages have Set analogies; in particular
users aren't to think about this in terms of relations being arrays or
whatever.
- Indexing is implemented at the Set level, such that the data types of the
elements can declare routines that guide optimal indexing strategies, in
particular can define multiple indexes per type, mainly useful for indexing
sets of tuples, as indexes can be explicitly candidate keys or not.
An index function will take a value of the type as input and return an
ordered array of values that the Set implementation can then use as keys
in the index, analagous to key declarations in SQL.  Somehow, relation-level
operators can also ask the system to create particular indexes on things
maybe, eg the columns about to be joined, or something.  Or not.
Something like want_index() or whatever.  Indexes are somewhat first class
or the system would be savvy enough with the Set type that they could be
properly done both at the memory or disk level.  But generally the lower
implementations don't have to know about relations specifically, this would
be generalized away, so eg SQL table emulations would be same level or such.
- Set ops would include replace-element or such, some generalization of
replacing or updating a tuple, particularly for implementing a bag, eg
updating the count for an existing thing.

* Note that BS12's UNION is Tropashko's (inner) union.
BS12's UNION was actually what became UNION CORRESPONDING in SQL about a
decade later, meaning union of projections of inputs over their common attrs.

* DO FIRST:
- Split Low_Level into namespaces as appropriate and add the minimal sufficient
operators specific to each of the 11 or so low-level types that a maximal
amount the rest of the 'public' types and operators for eg integers or
relations or whatever, can be defined by the Muldis_D core without their
having to explicitly deal with the underlying List or Natural representations,
and so later Low_Level could, particularly in implementations, be swapped
out just by itself for some other implementations, but the language core
proper would continue to work unchanged.
- Probably represent a lot of internals using Array wrappers.
- Most Natural math ops might be ejected / moved into Integer, maybe.
- Some selection ops might be tricky if they have to do canon list sorting
eg for relation values to meet relation constraints; "put_in_order" etc.
- Make Low_Level code higher level where doing so doesn't add dependencies
on types not declared therein; in particular, add versions of not(), and(),
or() etc to Low_Level so the code can be easier to understand without
excessive runaround from being forced to just use ??!! for purity.

* ACTUALLY:
- New rool - any literals for a type may be used in package declaring said
type or in packages using said, but I can still be selective; eg, Array
lits and Integer and Boolean lits can be used in lowest level package.
- Split Low_Level into 2 modules, otherwise more formally nesting things.
- New lowest level has just ordered or lowest common denominator types:
Universal, Boolean, Natural/Integer, Array, String, Set, Tuple, Capsule, Identifier.
- We eliminate "Structure"; see a TODO point below for identifying priors.
- Above the new lowest level we have stuff that doesn't know anything about preceding
types' internals and are generally defined just in terms of the 8 types
{Boolean,Integer,Array,String,Set,Tuple,Capsule,Identifier}.
- Maybe actually give a name to the complementary subset of Universal that
isn't one of the above 8, such as "SC_Abnormal" say.
- Most everything else is a Capsule subtype.
- New higher low level declares: Text, Blob, Rational, Float, Relation,
Heading, Renaming, Stream, External, and basically everything else that has
its own special literal
syntax or would typically be available system-defined on common prog langs.
- Core or each layer does only interface directly with adjacent layer.
- Maybe rename "Low_Level" to something else, eg Sub_Level, Foundation, etc.
- Consider stratifying everything and turning plain "Muldis_D" into nothing
but synonym declarations.  As such, no internal implementation details
including nested functions are exposed in the namespace.
- Maybe have Muldis_D::Internals::* packages for everything underneath.
- Or we could have a convention that for any package Foo, a separate
package Foo::_/::* optionally may exist that has internals for it, like a
separated public and private namespace, analagous to .h and .c, but that
the public is a subset of the private depending on the latter, rather than
the reverse.  So Muldis_D::_::*.  Mneumonic is both "topic" and "private".
- Where routines have multiple aliases, convention saves them all for the
public interface, and in private there is just one name each;
aliases for parameters is a different matter.
- In any event, all generated names will either start with an underscore or
with a digit, the latter being illegal bareword identifier syntax.

* For a Universal topic:
    if topic has 0 elems then
        topic is Boolean.false
    else if topic has 1 elem then
        if topic[0] has 0 elems then
            topic is Boolean.true
        else topic[0] has >0 elems so
            topic is abnormal
    else if topic has 2 elems then
        if topic[0] has 0 elems then
            if topic[1] has 0 elems then
                topic is the Integer zero
            else if topic[1] has >0 elems that all have 0 elems then
                topic is a positive Integer, the count of topic[1] elems
            else topic is abnormal
        else if topic[0] has 1 elem that has 0 elems then
            if topic[1] has 0 elems then
                topic is abnormal (negative zero is not supported)
            else topic[1] has >0 elems that all have 0 elems then
                topic is a negative Integer, the count of topic[1] elems
            else topic is abnormal
        else if topic[0] has 2 elems that all have 0 elems then
            topic is an Array and topic[1] is its elems
        else if topic[0] has 3 elems that all have 0 elems then
            if every element of topic[1] is a valid Integer then
                topic is a String and topic[1] is its elems
            else topic is abnormal
        else if topic[0] has 4 elems that all have 0 elems then
            if the elements of topic[1] are mutually low-level-sorted then
                topic is a Set and topic[1] is its elems
            else topic is abnormal
        else if topic[0] has 5 elems that all have 0 elems then
            if the elements of topic[1] are mutually low-level-sorted then
                if every element of topic[1] has 2 elems then
                    if the first elem of every elem of topic[1] is a valid String then
                        topic is a Tuple and topic[1] is its attrs, as name-value pairs
                    else topic is abnormal
                else topic is abnormal
            else topic is abnormal
        else topic is abnormal
    else if topic has 3 elems then
        if topic[0] is a valid Identifier then
            if topic [1] is a valid Tuple then
                if topic[2] has 0 elems then
                    topic is a Capsule and topic[0..1] define its type and attrs
                else topic is abnormal
            else topic is abnormal
        else topic is abnormal
    else if topic has 4 elems then
        if a certain host of conditions on topic[0..3] are met then
            topic is an Identifier whose elements in order are:
                pkg_name_base - 0..N String elems
                pkg_name_ext - 0..N String elems
                rel_starts_n_lev_up - a positive Integer elem
                path_beneath_pkg - 0..N String elems
        else topic is abnormal
    else topic is abnormal

* A Collection is a set of Pair<key,asset>.
    - Array is homogeneous collection of Element: Ordinal+Universal
    - Bag is homogeneous collection of Member: Universal+Cardinal
    - Structure is heterogeneous collection of Field: Nominal+Universal

* Generally, paren delimiters are for structures/tuples/etc, brackets are
for arrays/strings/etc, braces for bags/sets/etc, relations are hybrid.

* MODIFY OF LOW LEVEL TYPES:
- A Structure field name is now always a String.  A named field has the
regular codepoints/string in all elements as usual.  A positional field has
exactly 2 elements, the first being 0 and the second being any integer.  A
positional is equal to a named of 2 characters where the first character is
the ASCII NUL character, that is positional is a proper subset of named
that has an additional special syntax.
- In practice we shouldn't have to worry about name collisions as no
ordinary character based name would start with an ASCII NUL, and if we were
ever mapping structures from some programming language where the 2 were
truly disjoint and could otherwise collide, we can deal with such things as
some other translation over top.
- The positional/integer names are also sparse like the named.  We have the
sparseness to give consistent handling between the 2 kinds of field names,
in particular with regard to substitution or merging of Structure values.
- Any logic concerning infix/prefix/etc operators or associative/etc traits
still expects fields named \+[0,0] and \+[0,1] corresponding to before,
aka "\\c[0,0]" and "\\c[0,1]".

* This table shows the low level types, plus examples of literal syntax:

    Boolean       | just the keywords false and true
    Integer       | 42 or 0 or -3 or 0xDEADBEEF or 0o644 or 0b11001001
    Pair          | \:[expr,expr]
                  |    When working element-wise with a collection, how each element represented.
                  |    User data should use dyadic Structure instead.
    Array         | [expr,...] or ([expr,...]) to disambiguate from procedure block
    String        | \+[nnint-expr,...]
    Bag           | \+{expr,...} or \+{expr:posint-expr,...}
    Structure     | \(expr,...) or \(lit:expr,...) or \(expr,...,lit:expr,...)
                  |    or \(:lit,...) or etc where lit
                  |    is one of: nnint or alpha or "whatever".
                  |    Is a set of named fields where each field name is
                  |    either a non-negative Integer or a String; a bare
                  |    nnint literal is the Integer, else is the String.
                  |    A field whose name is a String is also called an attribute.
                  |    Where any fields named with Integers, then for 1..N
                  |    such fields, the Integers must be 0..N-1 like Array.
                  |    - Tuple is proper subtype with only attributes;
                  |    it has special alt syntax \%(lit:expr,...).
                  |     ACTUALLY, Tuple is a proper subtype having 1
                  |     positional element of SC_Field_Name value \Tuple,
                  |     plus 1 named element of any Structure value its payload.
                  |    - SC_Field is proper subtype with exactly 1 field.
                  |    - SC_Field_Name is proper subtype of SC_Field
                  |    whose value for that field is true; it has the
                  |    special alternate syntax \lit; it identifies a field
                  |    of a Structure/Capsule.
                  |    - SC_Attr_Name is proper subtype of SC_Field_Name whose
                  |    name is a String; identifies attr of Tuple/Relation.
                  |    - SC_Signature is proper subtype where the value for
                  |    every field is either true or is a SC_Reference;
                  |    this is used when defining a type in terms of a set
                  |    of typed fields, or when defining a function
                  |    signature likewise, or when defining a set of SC_Name;
                  |    a SC_Reference is naming the expected field type,
                  |    or true means either type inapplicable (we just have
                  |    a set of SC_Name) or it means Universal / any value.
                  |    - SC_Field_Names is proper subtype of SC_Signature where
                  |    every field value is just true;
                  |    it has special alt syntax \~(lit,...).
                  |    - SC_Heading is intersect type of SC_Signature,Tuple.
                  |    - SC_Attr_Names is intersect of SC_Heading,SC_Field_Names;
                  |    is T/R heading as userland value, arg for project/etc.
                  |    - SC_Renaming is proper subtype where the value for
                  |    each field is either a non-negative Integer or a
                  |    String, that is, something valid to be a field name;
                  |    it has special alt syntax \<-(lit:lit,...);
                  |    use in routine defs to normalize args, arg for rename.
    OBSOLETE; USE A STRUCTURE SUBTYPE INSTEAD, FREE UP => FOR NON-CORE / USER TYPES:
    Capsule       | expr=>expr - that is Reference=>Structure eg $Date=>\%{year:500,month:4,day:24}
                  |    and Singleton is one with nullary tuple eg $To_Zero=>\()
                  |    and $=>lit is shorthand for Singleton value eg $=>To_Zero.
                  |    Normally a Capsule's Reference is a type name, making
                  |    the Capsule a type, but structurally one is identical
                  |    to a partially applied function if the Reference is to a function name.
    OBSOLETE; USE A STRUCTURE SUBTYPE INSTEAD:
    AND TODO: MAKE \$foo mean what $foo used to mean, \\$foo what was \$foo.
    SC_Identifier | Several kinds, all mutually disjoint:
                  |    - identity : \$pkg-canon-name.litpath   eg \$Muldis_D:"http://muldis.com":"0".Integer.sum
                  |    - absolute : \$::loc-pkg-alias.litpath  eg \$::MD.Integer.sum
                  |    - relative : \$nn-int-expr.litpath      eg \$2.foo.bar or \$0
                  |    - floating : \$litpath                  eg \$Bag
                  |    For non-literals rtn call remove \$
                  |    eg "::pkg.foo.bar()" or "0()" or "foo.bar()".
                  |    Note that leading "0" means "self" / same routine,
                  |    and "0.foo()" is not allowed; leading "1" means
                  |    the schema that directly contains current routine
                  |    so a routine with nested routines would behind
                  |    scenes refer to each with "1.nest_rtn_name()"
                  |    but if invoker routine has no nested, then its
                  |    explicitly decl neighbors ref'd with "1.foo()".
                  |    Also special syntax "resolved expr" will
                  |    produce identity from all kinds of Identifier expr eg "resolved \$foo"
                  |    and $lit is shorthand for "resolved \$lit" eg $Bag short for "resolved \$Bag".
                  |    - SC_Reference is proper subtype has all "identity" values.
                  |    Note that any consecutive double-quoted tokens are treated as 1 token.
    External      | \~external~'...' - the "..." are implementation-dependent
                  |    but typically is source code of some kind for
                  |    whatever the host environment is, eg Perl code
                  |    Note that any consecutive single-quoted tokens are treated as 1 token.

* Subtypes of Capsule:

    Blob        | 0x'27EO4' or 0o'350261' or 0b'101100110'
                |    Note that any consecutive single-quoted tokens are treated as 1 token.
    Text        | 'Hello'
                |    Note that any consecutive single-quoted tokens are treated as 1 token.
    Ratio       | 5/3 or 3.14159
    Float       | 29*10**30 or 4.5209E19 but how to not interp first as Integer expr?
    Quantity    | expr+expr or expr-expr or expr*expr or expr/expr or
                |    expr**posint-expr where at least one expr is a (Singleton)
                |    Reference or a Quantity eg 5.7*$Meter/($Second**2)
                | Optionally consider unit part allowed to be any value not just a Reference? ... or not
    Interval    | expr..expr  or expr^..^expr etc
    Set         | {expr,...} or ({expr,...}) to disambiguate within recipe block
    Relation    | \@{(lit:expr,...),...} or \@[lit,...]:{[expr,...],...}
    Dictionary  | \~{expr:expr,...} or something

    SC_Func_Args   | synonym for Structure 
    SC_Func_Params | \:(typeref-expr,...) or \:(lit:typeref-expr,...) or etc
    SC_Proc_Args   | \&(expr,...) or \&(&var-expr,...) or \&(lit:expr,...) or etc
    SC_Proc_Params | \&:(typeref-expr,...) or \&:(&typeref-expr,...) or etc

* Allow -0 and -0.0 etc in numeric syntax but they just fold to 0 or 0.0.

* Math recognizes different kinds of infinities, eg countable or
non-countable, eg based on repeated add or multiply or exponentiate or
whatever double-up-arrow is etc, and each possibly deserves its own
infinity singletons.  But we probably want a generic infinity singleton
that doesn't care, for generic ranges etc ...
Ooh, ooooohhh ... lets make 'Infinite' an abstract type which is then
composed by the likes of those different infinities or +Inf or -Inf etc.
Or otherwise deal with infinities in a similar sense to units, as these are
units eg for use with Quantity in a matter of speaking.  But where is the
boundary with symbolic math?

* Regarding abstract types, virtual functions, 'composes', 'implements':
- Need to evaluate how useful these actually are.
- Something something about association being tied to specific names rather
than implicitly to all synonyms.

* See http://www.javaworld.com/article/2073649/core-java/why-extends-is-evil.html
as to why in principle we want to do more in terms of interfaces/abstracts.

* The key feature of abstract types is that operators defined over them
have different visibility status.
- Normally within a package, identifiers only ever resolve to things that
are explicitly either in that package or explicitly used directly by that
package, and so the internal behavior of a package can never be changed at
a distance but when the package itself or any of its explicit dependencies
(recursively) are changed.  A polymorphic call will only consider options
within that context to dispatch to, and will ignore / not see any other
overloads of that operator in the program.
- But abstract types provide a lone exception.  If a package uses an
abstract type or an operator defined over said, then an invocation of such
an operator will see / consider overloads over the whole program.  Or to be
specific, if the normal more restricted resolution ends up picking a
virtual operator, then other operators over the program that claim to
'implements' said virtual will be considered; but if a non-virtual and
non-implements version is found the normal way which satisfies the argument
types, it will be chosen instead.
- To be more clear, the entities are subject to two-class rules; in effect,
2 namespace searches will be done; if a candidate visible under normal
rules is found which is not a virtual and satisfies the argument, it will
be chosen regardless of if a virtual also exists that satisfies; a virtual
will only be chosen, on a second effective pass, if no non-virtual matches.

* When a 'search' identifier-chain resolution is performed, the algorithm
is like this (ignoring checks for type or abstract vs not):
0. Set BASE to be the actual invoking entity, which is almost always a
routine but possibly might be something else; we will pretend it is
possible for an invoker to be a schema; go to step 1.
1. If the unqualified name of BASE matches the initial identifier-chain
element and the rest of the chain matches corresponding children of BASE,
we found a match, then stop, else go to step 2.
2. If BASE is a schema then search within its children in a breadth-first
manner, first considering every immediate / first-degree child of BASE per
step 1, then every second-degree child, onwards until either a match is
found (and we stop) or all descendants are checked, and then go to step 3.
Within each level, the candidates should be visited in a truly random /
non-repeatable order (so visiting order within siblings can't be relied on
for semantics between successive invocations, depending on said is a bug;
more generally, having multiple matches on the same type within found
candidates is possibly or often a bug itself).
3. If BASE has no parent then we can't keep searching; no match was found,
so stop, else go to step 4.
4. Set BASE to become the immediate parent of the current BASE, and then go
to step 2, but that we ignore / don't revisit the same child that
previously was BASE.
Amendments:
- The entirety of those 5 steps applies only if the invoker lives within a
search path, in which case the root of its search path is treated as being
the BASE with no parent per step 3.
- For search paths that don't include the invoker, step 0 instead defaults
BASE to being the root of the search path, and then only the descendent
search per step 2 applies.
- As a general rule of thumb, all 'private'/embedded entities should never
be invoked using 'search' syntax, but rather use relative (often this is
automatically generated anyway) so attempts to invoke some embedded thing
don't accidentally call something farther away due to search paths order.

* The Muldis D compiler/runtime/etc written in Muldis D (eg, particularly
to make cross-compiling easier or reduce manual code per language) should
generally be procedural in nature.  It should use one or more nested array
variables to represent the main memory data structures, basically mimicking
what would be done in Perl with its Memory objects and such.
- HERE'S A KEY POINT: To avoid recursive implementation issues, there would
actually be 2 instances of Muldis D running, where the outer one is
implementing a virtual machine in which the inner one is running, and
values do not directly cross between them at all.  That is to say, a "value"
that lives inside the VM is represented by a DIFFERENT "value" (or variable)
outside the VM.  The wrapper on the outside contains extra stuff such as
indexes or whatever that are not semantically part of the inner "value" but
are used in implementing it.  The outside instance of Muldis D is what would
typically get substituted for by Perl or whatever.
- Outer instance would use a minimum of types, in particular no relations.
- Outer analagous to miniperl or NQP or something.
- Outer natively just ASCII source or something.
- Outer possibly slimmed syntax, eg, PT_STD_MINI.
- Things and stuff.

* Create sibling project called "Muldis D Lite" (MDL) which is essentially
to Muldis D (MD) what Not Quite Perl (NQP) is to Perl 6.
- MDL is officially disjoint from MD but also corresponds directly, such
that all syntax in MDL is also valid MD syntax, with the same effective
semantics, but that formally each is declared into separate package and
syntax namespaces.
- For example, using `Muldis_D_Lite:Plain_Text:"http://muldis.com":0` rather
than leading with `Muldis_D`.  The different first element implies
different language and not just different syntax for the same language.
- MDL is intended to be useful by itself but it is also as minimalist as
reasonably possible while still meeting that goal.  The main differences
from MD are those things which are a lot tricker or more verbose to
implement and mainly provide convenient niceties that programmers expect to
have or that may aid making larger programs.
- The intent is to build MDL first, its reference implementation being
bootstrapped in Perl and otherwise self-defined and able to compile itself,
to Perl and NQP and various other things.  Then the reference
implementation of the full MD would be built next and be bootstrapped in
MDL and otherwise be self-defined and able to compile itself, though
initially compiling itself may just compile to MDL.  To be clear, one would
write a Muldis D VM/compiler/etc in MDL as if they were writing it in say
Perl, so for example what the Muldis D inside the machine sees as value X
would not be the same value X to the code outside/implementing the VM, but
rather some value Y which represents X but has extra meta-data such as
indexes or whatever, so one can have multiple host values Y which on the
host side are not the same value but to the language in the VM they are.
- Formally, MDL is treated as a foreign language by MD, so invoking MDL
modules in MD modules would be done like invoking Perl or a host language;
at the same time, MDL can be made native in the same way as SQL or other
languages, albeit trivially.
- Definitions of some corresponding features would differ, eg on traits
for Integer or whatever, and particularly the catalog types would differ.
- Parts:
    - Muldis_D_Lite
    - Muldis_D_Lite::Primitives
        - Logically declares {Boolean,Integer,Array,Structure} over
        Universal, so rest of MDL code can just assume these things exist
        as primitives and would make no attempt to divine internals.
    - Muldis_D_Lite::Memory
    - Muldis_D_Lite::Types
        - Declares likes of String, Tuple, Capsule, etc and a bunch of
        catalog types as needed to represent MDL code.
        - Or maybe Tuple not included, and perhaps we use Structure instead
        of Capsule.
    - Muldis_D_Lite::Storage
    - Muldis_D_Lite::Compiler
    - Muldis_D_Lite::Parser
- Features included:
    - 7-bit ASCII
    - non-abstract types
    - non-virtual functions
    - hierarchical namespace
    - absolute and relative identifiers
    - polymorphism in the sense that operators may take Universal/etc
    - positional collections / types / arguments
    - invoking routines with `args --> foo` or `foo(*args)`
    - non-streaming parser / regular-size code files / small db dumps
    - non-embedded / side-by-side defs of eg routines and types
    - higher-order routines (by name-passing only or that is "-->name")
    - Universal, Boolean, Integer, Array, Structure
    - String, Heading, Renaming, Tuple, Identifier, Capsule, Stream
    - Blob, Text
    - type-specific sorting and ordering, for Integer, minimal other types
    - minimal operators for included types
    - minimal selector syntax for included types
    - generic file I/O as byte streams or memory byte mapping
    - in memory databases
    - single concurrent users of in memory databases
    - trivial expressions / arrays on left hand of assignment
- Features NOT included:
    - non-ASCII
    - abstract types
    - virtual functions
    - 'search' identifiers
    - polymorphism in the sense of operator overloading
    - meta-operators
    - named collections
    - named arguments
    - infix/etc notation eg `arg foo arg` or `foo arg` or `foo(arg)`
    - streaming parser / parsing very large files / typical/large db dumps
    - embedded defs of eg routines and types
    - generic sorting and ordering and intervals
    - most / trans-minimal operators for MDL types
    - most traits such as associative or whatever
    - most alternative selector syntax for included types
    - persisting databases / tied variables
    - multiple concurrent users of in memory databases / memory merging
    - non-trivial assignment left-hand expressions / views in general
- Features could go either way:
    - named collections / types / arguments
    - Relation
    - Set, Bag
    - type-specific intervals for eg Integer, minimal other types
    - some traits, maybe

* ACTUALLY, Muldis D Lite is not necessarily a pure subset of Muldis D; for
core/main language operations it probably is, but in some cases there might
expressly be a break.  For example, every call to foo(*arg) we might decide
to not require the *; in regular Muldis D not having that * means an
implicit Func_Args or whatever wrapper is created around arg, but in MDL
there is no such wrapper and we could for helping brevity to say that in
MDL the leading * is just a noiseword; it is allowed for compatibility with
Muldis D but not required.  A more common example is the hierarchy or
existence of the catalog types in general, eg, the same string syntax may
not parse into the same catalog types.

* Make first Muldis D Lite implementation in Pure Perl.  If I have to write
any of Muldis D Lite in MDL itself, the language is too big.
- Any MDL impl in MDL would be a separate second implementation later,
which would really just be a cross-compiler.
- In contrast, Muldis D proper though implemented in MDL fundamentally
would also have a full self-definition in Muldis D.
- At each abstraction level, a level should really conceptually just be
about translating code from one layer to another.  Conceptually, a MDL
implementation is really just a compiler for MDL to some other language
(eg Perl) and also conceptually the VM it runs in is also provided by said
target language (eg Perl).  Likewise, a Muldis D implementation is really
just a compiler for Muldis D to either MDL or Perl or whatever.  This might
be muddied though by implementing one MD version in another MD version.

* See this: http://www.unicode.org/reports/tr39/#Restriction_Level_Detection .

* If possible add functions corresponding to same_base_type() and
same_low_level_type() that are unary and just return some value that
uniquely identifies the type, a generalization of returning the name of the
type; perhaps it would be the default value of the type;
for example, which_base_type(42) returns 0 and which_base_type('hello')
returns '' etc; which_low_level_type() is the same but that for all
Capsule it just returns the default Capsule value.
- Idea is that for misc Universal types, every one of which is treated as
its own low level singleton type, we just return the same value.
- However, this idea falls apart if there are multiple independently
defined Capsule types that have the same Capsule_type, so which to choose
in order to determine the default value?
- So maybe then what we actually want to do is just hardcode in the new
which function its own idea of a consistent default value that probably
may not even be a valid default value in the normal sense, and so:
  - For each Capsule, just return the value Capsule_type(topic)=>\%{}.
    - Above was for which_base_type(); for which_low_..., default Capsule.
  - For all normal other Universal, return their default value.
  - For all SC_Abnormal just return the argument given to which().
- Maybe also have a which_types() that returns a list of Reference for all
known types that the given value is a member of, if practical;
running this might be slow, though implementations can try maintaining a
type graph that may help this and other things.

* BEFORE DOING ANY OTHER LARGE THING, CREATE A GIT BRANCH THAT I JUST DO A
PILE OF CHANGES IN TOGETHER, AND THEN SEPARATELY I CAN MANUALLY MERGE BITS
OF THAT TO TRUNK WHEN READY TO DO A SERIES OF MORE ORDERLY RELEAESES.
- I SUPPOSE PROBABLY BEST TO DO THE BRANCH ON GITHUB'S END?  RESEARCH!
- I KNOW I WANT TO SEE THE BRANCH AND TRUNK SAME TIME BOTH THERE AND HERE.

* It must be mandatory for each grouping () paren pair in an expression to
have exactly one sub-expression which is not explicitly named with ::= because that
is the root sub-expression of the grouping () ... or alternately we change
the definition of ["" ::=] to simply indicate which sub-expression in a
grouping () is its root; but then we actually should have a keyword,
and maybe "<--" could be that keyword ... the keyword would be optional
and the simple absense of leading ::= from one sub-expr marks it as main;
but if all sub-exprs are named then the keyword is mandatory.

* Eliminate uses of |,&,- to mean union/or,intersection/and,minus with
respect to type defs or {schema object, param/attr} alias declarations.
- NOTE, NEWER VERSIONS OF SOME OF THE FOLLOWING POINTS ARE FURTHER BELOW.
- We should discourage aliasing even if supporting it, and the best way is
to not provide special/terse syntax for doing so that isn't generalizable.
- For schema object aliasing, people can just do this:
    sum ::= add ::= "+" ::= function ...;
... assuming all of said aliases are okay to have identical signatures/etc;
actually, better just make them say
    bar ::= function ...;
    foo ::= synonym of bar;
- In practice, explicit union/intersect/minus types probably aren't that
common except for say enum declarations, so just support those with the
likes of:
    Int_or_Text ::= Universal where (.0 isa $Integer or .0 isa $Text);
- Provide appropriate generic expression syntax / functions such that one
can use them effectively for declaring routine signatures/etc without those
needing special syntax.
- As such, the general form of the declaration of function foo would be
in terms of 5 mutually isolated (having their own expression node
namespaces) generic expression trees, any of which or parts thereof may
optionally be factored out into (the bodies of) other functions;
within each tree, "_" is a keyword / the default name of the (locally)
singleton expression node representing the topical input value of the
expression aka its sole parameter; these are those 5 trees:
    1. A e(Boolean<--Universal) that declares the (usually Tuple) dispatch
    type of foo's input parameter; that is, a call to "foo(x)" will cause
    this particular foo to be dispatched to if True<--e(x) assuming also
    this particular foo is the first so-matching one found during dispatch.
    2. A e(Universal<--Universal) that normalizes foo's input parameter
    from whatever conceptual multiple parameter aliases may exist to one.
    3. A e(Boolean<--Universal) that declares any further constraints on
    the input parameter, eg that denominator may not be zero, when we want
    to explicitly fail certain function calls rather than the dispatcher
    simply falling back to no-such-routine when you call say divide(4,0);
    a result of False means fail and True means accept.
    4. A e(Universal<--Universal) that declares the main body of the
    function, that which does the real work the function exists for.
    5. A e(Boolean<--Universal) that declares foo's result type; for a
    function this is the other part of foo's main signature besides #1.
- A general procedure declaration would follow a similar pattern, except
that there wouldn't be a #5 so to speak and inverse expressions may be
necessary (ideally generated) for #2 etc considering the common practice
of subject-to-update parameters; procedures need more thought.
- Generic system-defined functions such as Tuple_follows_template() can
provide conveniently short syntax for signature declarations, including
whether each attr is mandatory or optional, what aliases an attr may
have, whether the attr list is exhaustive or a subset, whether attrs have
positional-alike/integer names, etc.
- Generic system-defined functions for testing multiple "isa" on the same
value can be given, eg "v is_any_of {$Int,$Text}", which also works for
declaring enumerated types, or "v is_all_of {$Odd,$HundredPlus}" etc.
- Remove ":" from function declaration as the line between function head
and body is more fuzzy, but have some terse way of marking each part of the
function.  Also make bounding parens optional for the 5 expression trees so
eg you don't have to say "(5)" when "5" would do; the bounding parens would
just be needed when it isn't otherwise clear where the expression ends and
the next part of the function declaration or next schema object declaration
begins.  These should all work equally well when a function declaration is
embedded within another value expression, such as for the meat of generic
relational restrictions.  Perhaps something like this:
    foo ::= function --> (_ isa $Integer) <-- (_ follows ...) : (#_);
    foo ::= function to (_ isa $Integer) from (_ follows ...) via (#_);
... noting that the 5 parts may come in any order and are all optional.
Clearly this will take a lot of thought.
- See Perl 6 design for guidance, it already does a lot of that stuff.
- Have function shorthand eg "tup?.attr" that returns a VoidLike value
rather than dying if the tuple doesn't have an attribute of that name.
- Maybe go further and make "?" a parser-recognized meta-operator, similar
to how "!" is; so while "bar !x foo" means "not(bar x foo)",
"bar ?. foo" means "has_attr(bar,$foo) ?? bar.foo !! :$Void" or some such.
- So then defaulting an optional parameter can be like "_?.mynum // 3".
- Have shorthand to choose, among multiple possible attribute names, to
determine under which name, if any, the attribute exists, take its value;
it would also assert no more than one of the attr names exists at once.
- We can also say that technically the only actual required part of a
function definition is #4 the main body expression.

* Add keyword "type" that is like "function" but just takes a body def expr
where that body is a predicate, and implicitly is Universal-->Boolean.
- Actually, it would take a binding def too in the general form, like this:
    type of Foo where ...
    function from Foo to Boolean via ...
- So "of" means "from" and "where" means "via".
- The "type" also takes "default" expr, optional only if inherited.

* Have keyword "constant" that is like function but just takes a body def
expr and explicitly has no input.  As all "function" must have exactly 1
argument, they can not be used to define constants or singletons properly,
so we have "constant" to fill in a critical gap; it looks like this:
    constant expr
    constant 3
    constant foo(6)
    constant ...
- Practially speaking, the concept of embedding constants in an expression
tree would only ever make a named expression node and not also a schema
object, unlike say when embedding a function or type.
- The body expr of a singleton may still be empty/... though if it is
expected the implementation will fill it in as a special case.
- The implementation will likely fold any constant declarations at compile.
- A type 'default' is a constant and a function 'identity' is too.

* Invocation differences:
    foo()  - calls a 'constant' only
    foo(...)  - calls a 'function' only
    foo  - either an expr node name or a pre/post/infix style 'function'

* Change how routines work so that the actual sole input argument may be a
Universal rather than having to be a Tuple.  Also create a new core type
that represents the actual argument which has both an Array and a Tuple
component, which is the canonical way to represent effective argument lists
that have either or both ordered and named arguments.  So we then eliminate
the kludge where ordered parameters are represented by Tuple attributes
named "0" and "1" etc, which is partly bad due to being base-10-centric.
Along with that new type is new special syntax for selecting a value of it,
where that syntax resembles the existing complexities of an argument list.
Then alter the general syntax of a routine call to be more like that of a
Capsule value selector ("=>"), in that it takes the appearance of an infix
operator, eg "<-" (like "." does), like this:
  rtn <- args
eg:
  $div <- :( x, y )
where that particular example also has the shorthand:
  div(x,y)
or:
  x div y
Within a routine, the sole parameter is named "_" and where the new special
type is used, the value composes both ArrayLike and TupleLike, so you can
access the actual parameters like this:
  _.[0]
or
  _.foo
... or as ".[0]" or ".foo" respectively.
There would also be normal operators for merging a Tuple+Array into one of
the new types, or extracting the whole Tuple or Array component.
Obviously the default value of the new type is an empty array+empty tuple.
The "rtn" input to "<-" could also be an anonymous function declaration,
which in its general form is just a body-defining expression; but in that
case, for legibility, we may want to offer "->" as well so one can say:
  args -> rtn
Actually, we may want the "args -> rtn" form to be the "normal" general
form, assuming it is mainly used with anonymous routines, and that use with
named routines is uncommon, and then simply not support "<-" at all.
We will likely replace "v->op(...)" with some other syntax then or drop it.
We will want to use Perl 6 for guidance on how this type/etc works.
Or use "args --> rtn" and leave "v->op(...)"?  But what is more common?
In Perl 6 ":(...)" creates a Signature while "\(...)" makes a Capture.
* SC_Func_Args will compose both the TupleLike and ArrayLike interfaces,
that is, one can say foo.bar or foo.[1] on SDFA which just goes to attr elems.

* Formally make all functions be unary Universal-->Universal and deal with
it re consequences on relationship to other languages or whatever.
- This gives Muldis D more theoretical purity and means that special types
for representing arguments or parameter decls don't have to be low-level.
- But if all functions are unary, how to represent nullary avoiding
recursion eg of the definition of its placeholder argument?
Maybe one way is to say a function may be invoked without an explicit
argument and in that case it is the empty LL_List that the function gets.
- All Low_Level.foo will explicitly not use the new input argument type
that combines Array+Tuple etc, only non-Low_Level foo would.
Instead they would explicitly use LL_List of particular arity for any
functions that conceptually take multiple arguments, and just the argument
directly when just one.  As such, all invocations any LL_foo would by
necessity use the general form of function calling eg args-->func or
func(*args) which would be another way to discourage direct use of LL_foo
in general code.
- All non-Low_Level.foo explicitly will use the special args type.
- The new args type definitely will be a Capsule, a SC_foo to be specific.
- All invocation syntax like "x op y" or "op x" or "op(x,y)" or "op(...)"
will always mean use the new args type, and so they'll only be for non-LL.
- All predicate functions intended to be used in the position of defining a
type must be bare Universal-->Boolean; that is, the type of the "topic" is
NOT a SC_Func_Args in the general case.
- Similarly, all predicate functions used in defining routine headings,
that is dispatch type, input normalizer, input constraint, result type,
for these type type of "topic" is also NOT a SC_Func_Args in the general.
- Consider for general predicates as well eg for relational "where".
- Consider making it "normal" for routines to often take Array or Tuple
rather than SC_Func_Args ... or don't ... but the fact that SC_Func_Args
can "do" both Array and Tuple could mean such practice could be transparent.
- So, Universal."=" will have to be changed to be a wrapper of LL_same
rather than a synonym of it, since the former will use a SC_Func_Args while
the latter will not.  Some other similar splits would happen as approp.

* Make backslash "\" only mean escaping within delimited strings; outside
said it has no mnemonic and can be used as generally as any symbolic chars.
However some special syntax starts with a \, eg relation literals, which
will override in the case of ambiguity.

* Also, make it so function decl type is essentially just a tuple of
traits, where body and params decl etc are just traits same as
is-commutative or default or whatever.  Every trait is a key-value pair
with the key being the trait name and the value either being a boolean or
some other type, typically an expression node set.

* Let "isa" have prefix shorthand so "isa $foo" means "topic isa $foo".

* Ruby has 'assert' and 'refute' keywords, also 'alias'.

* First task - we need to build a list of expression and stmt node types.
- SC_Topic - represents the real function input argument
- SC_Resolved - represents the "resolved" keyword; turns Ident to Ref
- SC_Inlined - represents the Reference name of an entity that was
    declared inline with the expr, typically a function (or type, which is
    a function); this name is generated unless function def preceded with bind
- SC_Routine_Name -
- SC_ISA - represents "isa" keyword
- SC_Default_Of - represents "default_of" keyword
- SC_Meta_Op_Not - represents "!foo" meta-op
- SC_Meta_Op_Reduce - represents "[foo]" meta-op
- SC_Meta_Op_Assign - represents ":=foo" meta-op
- SC_Meta_Op_
- SC_Lit_Universal - represents "\*[...]" literal syntax for Universal
    values; also has "\*:[...]" variant that asserts exactly 2 elements
- SC_Bootstrap_Singleton - represents "\*^1[...]" special syntax for
    bootstrap singletons, each of which may exist exactly once in the
    codebase, and since they're all used once each in Low_Level.mdpt, means
    they are otherwise non-invokable by users; possible singletons are:
    - \*^1[maximal_type] - what Universal/LL_List is defined in terms of
    - \*^1[constant_empty] - what constant LL_empty is defined in terms of
    - \*^1[function_prepend,expr,expr] - function LL_prepend
    - \*^1[function_is_empty,expr]     - function LL_is_empty
    - \*^1[function_first,expr]        - function LL_first
    - \*^1[function_nonfirst,expr]     - function LL_nonfirst
    Note that only the first term in the list defines each singleton.
    Due to these, exactly zero of all system-defined constants or functions
    have "..." bodies, other than to mean TODO.
- ...

* Function call syntax:
    args --> rtn
    rtn(*args)
    \(x,y) --> $"+"
        # some example Haskell looks similar (and Perl 6 uses -> in 'for')
        \x y -> concat ["(",x,"+",y,")"]
    $SC_Func_Args=>\%{ordered=>[x,y],named=>\%{}} --> $"+"
    "+"(x,y)
    "+"(*\(x,y))
    x->"+"(y)  - THIS ONE IS OBSOLETE; x->y NOW A MONADIC POSTFIX MARKER
        # New use is, for example "arg ->op" for example "list ->group".
    x + y
    div(x,y,round_meth:$=>To_Zero)
    \(x,y) --> (function to Integer from (Integer,Integer) via ...)
    \(x,y) --> (function --> Integer <-- (Integer,Integer) : ...)
    42 --> $some_func_taking_Integer_directly_without_SC_Func_Args_wrapper
- Note that "self" can appear anywhere "rtn" does, and means call-myself
as long as it isn't quoted.  OBSOLETE!

* Note that "rtn()" with nothing in the parens now explicitly calls a
"constant" and not a "function"; you only get a "function" call if
something is in the parens.
- If you want the above treated like a function call you instead say like:
    foo(*\())
    \() --> $foo
- How you call a constant in both forms:
    foo()
    (--> $foo)

* Support symbolic constant calls with not "foo"() syntax.  A bare symbolic
is considered a constant (niladic function) call under many of the
conditions when it appears where a bareword token that would be interpreted
as an expression or variable name would appear.  Examples demonstrated here
with ∅ being the symbolic constant.
- Sole item in brackets:
    (∅)
    [∅]
    {∅}
- Whole item in commalist:
    x, y, ∅, z
    foo : ∅, baz: quux
- On tail-end of a routine precedence arrow (arrows point towards all functions):
    foo<-∅
    ∅->foo
    ++ <- ∅
    ∅ -> ++
- Right hand side of another symbolic with no precedence arrows:
    ++ ∅
    foo + ∅
- Right hand of a binding or assignment is no because confuses with
function call / hyper-op probably.

* Inline function declare syntax:
    expr_node_name_containing_Ref_of_func_name ::= function ...
    function ...
So both of these result in expr node whose value is like one said $foo
but the former the user decides what it is and otherwise its generated.
Note that all expressions of inner function have their own namespace and
are completely independent of the expression containing the function decl.

* NOTE THAT INLINE DECLARATION OF A FUNCTION/ETC THE NAME OF SAID WILL DUAL
AS A LOCAL EXPR NODE NAME AND ALSO A SCHEMA OBJECT NAME, IN THE PRIVATE
SCHEMA CREATED FROM THE PARENT FUNCTION BY VIRTUAL OF AN INLINE EXISTING,
TRUE WHETHER GIVEN EXPLICIT NAME WITH ::= OR NOT.

* Function declare syntax general form (same as inline actually?):
    0. keyword "function" followed by 0 or 1 of each of next in any order:
    1. keyword "virtual" is optional, not followed by anything of its own
        - if used, the keywords "via"/":" must be omitted
    2. keyword "to" or "-->" followed by the name of a type
        - the definition of the type may appear in place of the name using
        the syntax "type ..."
        - if omitted, the unnamed system maximal type is implicitly used
    3. keyword "from" or "<--" or "signature" followed by the name of a type
        - the definition of the type may appear in place of the name using
        the syntax "type ..."
        - a simplified form of a SC_Func_Params literal may appear in place
        of the name using the syntax "(...)" where the "..." is a commalist
        of 0..N tokens each of which is either the name/def of a type or a
        lit:type pair; they declare positional/named params respectively;
        these are basically equivalent:
            function <-- (Integer,foo:Integer) : ...;
            function <-- type of SC_Func_Args
                where (topic matching \:($Integer,foo:$Integer)) : ...;
        - if omitted, the unnamed system maximal type is implicitly used
    4. keyword "implements" is optional,
        followed by set of 0..N associated function names
    5. keyword "overrides" is optional,
        followed by set of 0..N associated function names
        - while referant of implements must be virtual, overrides must not
        be virtual; they are essentially the same but we're being explicit
        as to what's going on and users can avoid easy mistakes
    6. keyword "normalize" followed by expr
        - defaults to topic if omitted
    7. keyword "requires" followed by predicate expr
        - defaults to true if omitted
    8. keyword "is" is optional, followed by set of trait name keywords in
        braces: "associative", "commutative", "idempotent"
        and other terms like bijective etc may be added
    9. keyword "identity" is optional (when associative), has associated expr
    10. keyword "via" or ":" followed by expr
        - defaults to topic if omitted
    ...
- Note that "symmetric" is obsolete here; it meant what "commutative" does.
- Every part has a default value that is used if omitted
- Many of the numbered items above correspond to Perl 6 sub traits

* Constant / named value declare syntax general form:
    0. keyword "constant"
    1. optional keyword "folded"
    2. expr
- Note that unlike with functions, "virtual"/"implements" don't make sense
for constants because there's no way for them to be polymorphic dispatching
on a nonexistant argument, so all references to constants need to be
sufficiently name-qualified that the name is unique in the caller context.
- Iff "folded" is absent, the original value expression source code
is guaranteed to be preserved and introspectable in the same manner as
types and routines etc as if it were a niladic function.
- Iff "folded" is present, there is no such guarantee, and the
expression will officially be folded into a single value, so only the value
itself remains and how it was derived is not, aside from it being
maintained as a named constant that can be referenced.  Muldis D source
code produced from this value would be generated by the system, and is not
guaranteed to match the formatting of user-written source code.
- A folded constant is now the canonical way to represent the current value
of "the database" at any given point in time.
- TODO: Consider some other mechanism to declare that a set of routines or
constants for a virtual type should be composed by implementing types.

* Type declare syntax general form:
    0. keyword "type" followed by 0 or 1 of each of next in any order,
        but with some restrictions:
    1. keyword "abstract" is optional, not followed by anything of its own
        - if used, the 2 keywords {of,where} must be omitted
        - until another type composes an abstract type, that abstract type
        is equivalent to the empty type and any attempt to have a variable
        of the abstract type will fail at runtime in the same way
    2. keyword "singleton" is optional, not followed by anything of its own
        - if used, replaces all 3 keywords {of,where,default}
        - is for singletons that have their own Capsule type
        - these 2 declarations are equivalent:
            foo.bar.baz ::= type singleton;
            foo.bar.baz ::= type where !\*^[isa_Capsule,topic] ?? false
                !! \*^[Capsule_type,topic] = $1 and \*^[Capsule_attrs,topic] = \%{}
                default $=>1;
    3. keyword "of" or "union" is optional, followed by set of 0..N
        type names in braces or exactly 1 not in braces
        - if omitted, the unnamed system maximal type is implicitly used,
        and so the new type consists of all values in the type system but
        where otherwise restricted by its explicit "where"
        - if exactly zero type names, the current type has zero values
    4. keyword "where" is optional, followed by predicate expr
        - if omitted, system treats it as if it were unconditionally true
        - while one could declare an empty type with a "where" of false,
        the canonical way to do it is with an empty set for "of"
    5. keyword "default" is optional, followed by expr
        - it is invalid for any used "default" to specify a value that "where" rejects
        - if omitted, system follows "of" chain until it finds a "default"
        and then uses that value; to be specific if the set of effective
        "default" for all immediate "of" types has exactly 1 member that
        is a member of the current type, then that is used, otherwise the
        current type must declare its own "default", except if the current
        type has no values at all, such as with the system minimal/empty type
    6. keyword "composes" is optional, is followed by set of 0..N
        names of abstract types
        - if omitted, semantics are equivalent to if an explicit empty set
        - when type A composes type B, the behavior of the system when
        doing "isa" tests is as if B was defined with "B ::= type of {A,...}"
        but that the definition is done backwards
        - no need to rename "composes" to something else, meaning is approp
        - for each type name the keyword and_provides_its_default may
        appear after it, and if present, the composing type's default value
        is also used as the default value of that abstract type; only one
        composing type per common abstract type may declare this within
        some particular scope to be defined of each place where the
        abstract type is otherwise referenced, or it is an error;
        TODO, figure out that scope, but its probably per package
    7. keyword "requires_implements" is optional iff "abstract" is true,
        followed by set of 0..N names of virtual routines whose signatures
        use the current type, and live in same package as the current type
        - if present, then for every type X which "composes" current one,
        the package declaring X must also declare routines which "implement"
        the named virtual routines and signatures have X in the place of
        the current abstract type
        - this is a means for abstract types to declare a contract

* Schema objects can be easily indexed for fast(er) dispatch.  To be
specific, we can pre-make a map from each abbreviation (particularly the
fully-unqualified) object names to a list of the fully-qualified names that
said short one is short for, so an actual scan isn't needed per dispatch.
We rebuild/alter the map whenever a data definition action occurs.
Similarly, there is also a map for each fully-qualified schema object name
and the data types of its arguments, though that one might be lazily built.

* Perl 5.20+ supports a long-desired analogy to tuple projection.
The new C<%hash{...}> and C<%array[...]> syntax returns a list of key/value
(or index/value) pairs.  See L<perldata/"Key/Value Hash Slices">.
- While older @hash{...} can be used as lvalues, new feature can't, seems.

* Update ordering type per change to Perl 6 (instead of [Inc|Dec]rease):
https://github.com/perl6/specs/commit/5e2c4205
"These operators compare their operands using numeric, string, or C<eqv>
semantics respectively, and if the left operand is smaller, the same,
or larger than the right operator, return respectively C<Order::Less>,
C<Order::Same>, or C<Order::More> (which numerify to -1, 0, or +1,
the customary values in most C-derived languages)."

* From http://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science)
it would seem Muldis D qualifies as "covariant" everywhere, such is the
nature of subtyping by constraint.
Or perhaps more accurately, every Muldis D function is covariant, as this
is more a property of routines than of types.
See https://msdn.microsoft.com/en-us/library/dd799517(v=vs.110).aspx also.
- Covariance
Enables you to use a more specific type than originally specified.
- Contravariance
Enables you to use a more generic (less derived) type than originally specified.
- Invariance
Means that you can use only the type originally specified; so an invariant generic type parameter is neither covariant nor contravariant.

* Have a look at the "active object" design pattern.

* Useful info at http://www.mathsisfun.com/ for examples:
- http://www.mathsisfun.com/sets/function.html
- http://www.mathsisfun.com/sets/injective-surjective-bijective.html
- http://www.mathsisfun.com/sets/function-inverse.html

* http://arstechnica.com/science/2014/05/scientific-computings-future-can-any-coding-language-top-a-1950s-behemoth/

* "ideomatic" - "Peculiar to or characteristic of a given language."

* See http://terrancalendar.com for something interesting.

* Useful:
http://www.itworld.com/slideshow/163234/head-scratchers-10-confounding-programming-language-features-434442
Example: Java will automatically convert primitives types to objects
(autoboxing), such as int to an Integer object. It will also, by default,
cache Integer objects for values from -128 and 127. This can lead to
unexpected behavior when using == to compare autoboxed Integers with the
same value (TRUE from -128 and 127; FALSE otherwise).
Also number zero is TRUE in Ruby.
Also C/C++ have trigraphs like SQL does, say for people who don't have
curly braces on their keyboards.  They are processed before anything else,
as if they were part of the character encoding or such.

* From http://www.reddit.com/r/PostgreSQL/comments/26u0wk/why_is_postgresql_becoming_more_popular_than_mysql/
Some features missing in Oracle [that PostgreSQL has]:
- Range types
- Exclusion constraints
- True serializable
- PL/R
- Indexed KNN
- Transactional DDL
- Transaction level control of synchronous replication
- UNIX domain sockets with user based auth
- psql (the by far best SQL client I have used)

* Here's an article comparing how MVCC works in different DBMSs,
mainly focusing on PostgreSQL, Oracle, and SQL Server:
http://amitkapila16.blogspot.ca/2015/03/different-approaches-for-mvcc-used-in.html

* Actually, it seems transactional DDL support is much more widespread, and
hence MySQL is the main thing lacking it in modern days (InnoDB might have
supported it but the MySQL interface forces implicit commit on DDL),
although various DBMSs still have limitations.  See
https://wiki.postgresql.org/wiki/Transactional_DDL_in_PostgreSQL:_A_Competitive_Analysis
For PostgreSQL itself, you can't alter an ENUM in a transaction, but you
can basically do anything else (other than modify tablespaces).
http://www.postgresql.org/docs/9.4/static/sql-altertype.html
As of Oracle Database 11g Release 2, Oracle supports Edition-Based
Redefinition, which basically gives the feature, and otherwise performing
DDL is an implicit commit to prior operations, like MySQL does.
Also supporting DDL in transactions: SQL Server (in read-committed
isolation, more limmited in higher isolation levels), Sybase Adaptive
Server, DB2 UDB, Informix, Firebird (Interbase).

* http://blog.2ndquadrant.com/progress-online-upgrade/ gives good
explanation on doing zero/etc-downtime DBMS upgrades, how the new logical
replication of Pg 9.4 helps, and how pg_upgrade works (dumps and reloads
just db schema, and copies over disk files for data as-is or something).

* http://blog.2ndquadrant.com/bdr-postgresql-present-future/ is about how
bi-directional asynchronous multi-master replication support for PostgreSQL
has been converted from a branch/extension over time into core piecemeal,
including {Background workers (9.3), Event Triggers (9.3), Replication
slots (9.4), Logical decoding (9.4), REPLICA IDENTITY (9.4)} and the merge
may be done by 9.6 perhaps.  See BDR project.

* http://blog.heapanalytics.com/postgresqls-powerful-new-join-type-lateral/
shows how Pg 9.3's LATERAL join is useful in practice, letting you do in
declarational SQL what you may have needed procedural code for before.

* http://michael.otacoo.com/postgresql-2/postgres-9-5-feature-highlight-pg-dump-snapshots/
seems to give a way for multiple independent connections etc to see the
same database state, where one sets up named snapshot others can refer to.

* http://blog.endpoint.com/2014/11/mysql-to-postgresql-migration-tips.html

* Postgres 9.3+ event triggers probably very useful for DBMS wrappers to
efficiently know when a schema has changed or not, and when their memoized
schema information are still valid or not.
http://www.postgresql.org/docs/9.3/static/event-triggers.html
Also as an extra way to help prevent unwanted schema changes.
http://www.postgresql.org/docs/9.3/static/sql-createeventtrigger.html

* Consider signing up with https://www.gittip.com or other such venues as a
way to collect miscellaneous revenue to support Muldis D / etc development.
Just start this after I make my first deliverable and so supporters have
something already in exchange.  Would treat as business income by way of
Muldis Data Systems.  Not sure if I can identify Canadians from the givers
but if I can I'd treat part of theirs as GST collected.
See also communities such as https://www.gittip.com/for/perl/ .
See http://www.dagolden.com/index.php/2325/why-i-finally-joined-gittip-and-why-you-should-too/
and other blog posts it links to.

* How to make a generic join-like operator that is like SQL's and not like
normal natural join.  SQL's is fundanentally a cartesion product and each
input relation has its own namespace in the output; in Muldis D we will
have to specify said namespace as data, it isn't just gleaned from the
expression or variable names in source code, we do this with a tuple.
  :%{artists:db.artists} xjoin :%{albums:db.albums}
Each tuple of the result has 2 tuple-typed attrs named {artists,albums}
and the values are the tuples of the source relations.
One can then use where() to define a generic join condition like SQL eg:
  artists_tup xjoin albums_tup where :(.0.albums.artist_id = .0.artists.id)
Or another signature of xjoin itself could take more than 2 args where a
third etc provides join conditions etc; but then xjoin(a,b,c) call needed.
To support this in an N-ary fashion, xjoin has 2 signatures (both marked
commutative) which are tup*rel and rel*rel restricted such that a tup must
be unary with a relation-type attr and any rel may be N-ary where every
attribute is tuple-typed, and thus everything should just work.
The output of xjoin is the same type as said rel input type.
In the standard scenario xjoin requires that the primary attr names of all
inputs are mutually distinct since the result is a disjoint union of them.
So this should work:
  [xjoin] {a, b, c, d}
or
  a xjoin b xjoin c xjoin d
This is all basically an alternate way of doing:
  (a wrap all as a) join (b wrap all as b) etc

* It should be worth pointing out explicitly that when I write SQL I
automatically keep wanting to put the WHERE clause earlier in an expression
than SQL allows it to go, eg:
  update x where y set z;
or:
  select ... from x where y join a using (b) ...
... and so on.  And in Muldis D such intuitive actions are actually valid.
So an example of how Muldis D makes common tasks easier than SQL,
especially ad-hoc queries.

* Apparently Perl 6 doesn't have union types after all, eg Int|Str where a
type name could go.  Closest is "subset IntStr of Any where Int|Str".  See
http://www.nntp.perl.org/group/perl.perl6.compiler/2014/03/msg9483.html .
So | is always a junction.

* Note the following change in Perl 5.18.0; I should do the same in the STD
parser, that is treat all 11 of the same characters as white space such
that literal occurrences of said are treated as equivalent to a SPACE etc.
"When a regular expression pattern is compiled with /x, Perl treats 6
characters as white space to ignore, such as SPACE and TAB. However,
Unicode recommends 11 characters be treated thusly. We will conform with
this in a future Perl version. In the meantime, use of any of the missing
characters will raise a deprecation warning, unless turned off.
The five characters are: U+0085 NEXT LINE U+200E LEFT-TO-RIGHT MARK
U+200F RIGHT-TO-LEFT MARK U+2028 LINE SEPARATOR U+2029 PARAGRAPH SEPARATOR"

* Create another Git repository and release series or three that just
contain Muldis D code, these being analagous to Larry Wall's STD.pm / Perl
6 grammar written in Perl 6, and the Perl 6 Prelude and standard libraries
written in Perl 6, and the Perl 6 test suite written in Perl 6.
- Contained therein is all of the "official" muldis.com Muldis D packages,
just as Plain_Text text files, and the Muldis D compilers written in Plain_Text,
and the Muldis D official test suite written in Plain_Text.
- This release series is *not* expected to be released as an executable
product by itself; rather it is more of a data product whose files are then
copied into other projects that actually have the bootstrap or framing or
whatever Perl or whatever code so that the other projects are executables;
they include the as-data files to save themselves work and test themselves.
- Said other Perl/etc projects would synchronize themselves periodically
with the Plain_Text-only project for releases, but the latter has no Perl/etc
in it.
- Think of the Plain_Text-only product like the data files that the Unicode
consortium releases, and the latter being like Perl etc which bundles said
Unicode data files as effectively parts of the Perl/etc executables.
- Maybe the modules, compilers, and test suite will all live in one
repository, but those parts may be released as separate product distros.
- The repository can have lib/, bin/, and t/ dirs to group the above into.
- The above Plain_Text-only alone probably would *not* be distributed on CPAN.
- Even http://www.unicode.org/Public/UNIDATA/ has both spec-data+test fils.
- Maybe name this repository "Muldis-D-Standard"?
- This new repository doesn't have to be held back for any work in existing
repositories such as "Muldis-D" or whatever, and in fact would probably be
the development process leader for a little while.

* Create another Git repository and release series "Muldis-D-Ref-Perl5" or
such that would have the Perl 5 based reference implementation, and it is
bundled into this that "Muldis-D-Standard" would be distributed on CPAN,
and it is this that would actually run.  The "Muldis-D-Ref-Perl5" would
include both a module for embedding in Perl programs ala DBI which succeeds
"Muldis-Rosetta" and also a script for compiling standalone Muldis D
programs.  Any or all of the Perl in this distro may be generated from
Plain_Text code in Muldis-D-Standard, or at least the script and shared parts
would be, but the public parts of the module might not be.
The bundled DBD-alike module may have the name "Muldis::D::RefEng".
- Yet another repository, say "Muldis-D-Perl5", can specify HD_Perl5_STD
as "Muldis-D" soon wouldn't, and also specify a common Perl 5 API for both
"Muldis::D::RefEng" and other DBD analogies sharing the API to follow.

* PRIORITY LIST:
1.  Split off Perl stuff to separate distros/etc, chg language name fmts.
2.  Canonicalize low-level tp-sys so that simple "=" just works everywhere.
3.  Redefine all routines to have exactly 1 parameter, tuple-typed.
    - Have minimal other changes, including minimal syntax changes.
    - Syntax changes mainly re pass/def tup literally or not ala Perl 6.
4.  ...

* Add "is deprecated" flags to materials so that definers can say in
advance that something is going away in future versions before it actually
does, and users can be warned.  I think Perl 6 or Rakudo has it for
subroutines and such.

* Add references to or adoptions of ISO/IEC 11404:2007(E) "Information
technology -- General-Purpose Datatypes (GPD)" which could be very useful.

* Add mention of http://rubydoc.info/github/blambeau/alf/master/Alf .

* See http://scale-out-blog.blogspot.ca/2014/02/why-arent-all-data-immutable.html .

* See http://docs.python.org/2/library/datatypes.html .  Also, Python uses
the type name Counter for what Muldis D calls Bag it seems.
Also a lot of Python and Perl 6 type names are the same, suggesting influence.
Also a Python Dict key can be any immutable type {string,number,tuple,etc}.

* Apparently names like_this are called "snake case".
https://en.wikipedia.org/wiki/Snake_case .

* Apparently this is valid in Perl 5 (trailing ::), and can be disambiguating:
    Package::->method();

* About MySQL: For a multiple-row insert, LAST_INSERT_ID() and
mysql_insert_id() actually return the AUTO_INCREMENT key from the first of
the inserted rows. This enables multiple-row inserts to be reproduced
correctly on other servers in a replication setup.

* Perl 5.19.x/20.x add native sub signatures.

* See https://github.com/wbraswell/rperl/ for something that might be
relevant to translating Perl code to Muldis D code.
Or I can support RPerl as a compilation target; faster subset of Perl.
See also http://perl11.org and stuff.

* See newish DBMS FoundationDB that's about widely-scalable ACID supporting
both relational and non-relational.  A prime DBMS candidate to target, may
possibly be easier due to its design to support than Postgres, or maybe
not; it is also closed source.  Supports Linux (main) + Mac and Win (dev).
See also https://foundationdb.com/blog/call-me-maybe-foundationdb-vs-jepsen
for some stuff on how it works, and also the separate party Jepsen.

* See also Google's Spanner and F1 which are much the same, but internal.

* I probably want to use LMDB http://symas.com/mdb/ for my second
implementation, or alternately support it as an option in the first/ref.
There is lots of precedents for doing this too, eg SQLite4, multi "nosql".

* Quoth Richard Hipp:
Creating an index is essentially the same thing as sorting the whole table
- sorting in index order.  So you are going to have to sort the whole table
three times, once for each index.  The time to do the sort dominates.  The
time needed to scan the original table in order to pull out the elements is
usually negligible compared to the sorting time.

* Use same node type to represent bounding parens as used for synonyms.
Eg `foo ::= bar ::= 3` the foo is a synonym/aliasing node.
But aliasing nodes can have generated node names same as other anon exprs
so eg `1 + ((2+3))` the outer paren is an aliasing/syn node.
Parens themselves are general metadata that can go on any kind of node,
eg, is this surrounded by explicit parens in the text source or not.
Whether explicit node names are quoted or not is also metadata.

* Postgres 9.3 has ddl event triggers so eg you can block accidental
drop table and such if you want with a trigger.

* The Oracle foreign data wrapper for Postgres is declared stable now;
see http://oracle-fdw.projects.pgfoundry.org .

* Postgres 9.4 supports this apparently:
coalesce(min(val) filter (where val > 0), 0)
http://www.depesz.com/2013/07/23/waiting-for-9-4-implement-the-filter-clause-for-aggregate-function-calls/

* Note http://rhaas.blogspot.ca/2014/03/vacuum-full-doesnt-mean-vacuum-but.html
that VACUUM FULL is actually quite different than VACUUM, and in particular,
plain VACUUM is needed to accomplish certain things; generally plain VACUUM
is what you want during regular maintenance, unless eg your relation is
mostly empty and you want to return the space to the operating system;
VACUUM FULL does not deal with the id wraparound as VACUUM does.
Pg 9.4 changes some of this, so FULL deals with id wraparound too, somewhat.
Also, in Pg prior to 9.0, FULL worked differently.

* Postgres 9.5 adds row-level security:
http://michael.otacoo.com/postgresql-2/postgres-9-5-feature-highlight-row-level-security/

* See http://blog.endpoint.com/2014/02/dbdpg-utf-8-perl-postgresql.html and
why we really should use DBD::Pg 3.0+ if we have non-ascii chars, it is much
smarter than 2.x, actually paying attention to client encoding.

* See http://blog.endpoint.com/2014/02/mysql-ascii-null-and-data-migration.html .

* See http://www.sqlite.org/lang_savepoint.html ; apparently SQL savepoints
are already a generalization of "transaction" that can be nested and don't
have to require a parent "transaction" to be active; the first "savepoint"
could take that role itself; this is a lot like my design plans actually.

* Things even SQLite can do (as can Postgres) but MySQL can not do:
- subject data definition to transactions
- savepoints
- WITH clause, including recursive
- EXCEPT clause

* Apparently Windows 8 extends the use of shared pages to allow any pages
that just happen to be identical to be shared. It periodically sweeps
through all the memory in the system, identifies pages that have the same
contents, makes them shared, and frees up some physical memory. If one
process should then try to modify the newly shared page, Windows uses COW.
http://arstechnica.com/information-technology/2012/10/better-on-the-inside-under-the-hood-of-windows-8/3/
So my plan to do similar in Muldis D impls (when = is invoked) does not
seem that unprecedented, though in my case its both memory+performance.

* Apparently Windows applications can have an "application manifest" which
Windows looks at to determine the behavior of some APIs it provides, eg
Vista behavior vs Windows 7.  See also
http://arstechnica.com/information-technology/2014/11/why-windows-10-isnt-version-6-any-more-and-why-it-will-probably-work/ .

* Apparently C# has a "where" construct useable like this (it is lazy):
    var q1 = q0.Where(MoreThanTwenty);
... note that MoreThanTwenty is the name of a predicate function.
Also it has a "select" that works like "map":
    q = xs.Select(x=>1/x);

* See http://queue.acm.org/detail.cfm?id=2611829 - The Curse of the
Excluded Middle - "Mostly functional" programming does not work.

* See also "Haxe" language/platform.

* See also Test::Database the likes of which I can also support.
Its HISTORY.  http://www.nntp.perl.org/group/perl.qa/2008/10/msg11645.html
Quoting Michael Schwern:
"There's plenty of modules which need a database, and they all have to be
configured differently and they're always a PITA when you first install and
each and every time they upgrade."

* See http://www.pgbarman.org about Pg disaster recovery tools and
http://thebuild.com/blog/2014/10/24/be-very-afraid-backup-and-disaster-planning-at-pgconf-eu/ .

* PostgreSQL's model about transaction ids, and that it has special {1..2}
reserved values may be useful in my design.
http://www.depesz.com/2013/12/06/what-does-fix-vacuums-tests-to-see-whether-it-can-update-relfrozenxid-really-mean/

* This is probably useful, concerning timestamps:
http://www.depesz.com/2014/04/04/how-to-deal-with-timestamps/

* http://www.craigkerstiens.com/2014/10/01/a-simple-guide-for-db-migrations/
What happens when you have a not null constraint on a table is it will
re-write the entire table. Under the cover Postgres is really just an
append only log. So when you update or delete data it’s really just writing
new data. This means when you add a column with a new value it has to write
a new record. If you do this requiring columns to not be null then you’re
re-writing your entire table.

* NEXT PRIORITY ...
Rename Muldis::D::Outdated::Dialect::Plain_Text to Muldis::D::Outdated::Plain_Text.
Rename Muldis::D::Outdated::Dialect::HDMD_Perl[6|5]_STD to remove the ::Dialect.
Split off the Perl STDs from Muldis::D so they're no longer core
but they're still official, and distribute the HDMD_Perl5_STD in the same
distro as the Perl 5 Rosetta Engine API documentation.
(SEE ALSO THE TODO FOR MULDIS::ROSETTA FOR CONTEXT.)
Lets call that documentation distro Muldis::D::Perl5.
Create it like how we created Muldis::D::Manual ... empty then transfer.
The meat module lib/Muldis/D/Outdated/HDMD_Perl5_STD.pod is renamed as such so it
can also have its version reset to 0 and then the new distro can
advance versions on its own without needing to match or incorporate the
Muldis D versions which would become onerous.  But the language declaration
name would not change, except to add more vnums, eg to have:
[ 'Muldis_D', 'http://muldis.com', '0.149', ['HDMD_Perl5_STD', '0'] ]
Add new directory mod lib/Muldis/D/Perl5.pod like Manual.pod.
All of this likewise for the Perl 6 versions.
THIS GIVES US MORE FLEXIBILITY TO NOT HAVE TO UPDATE THE PERL-STD SPECS ON
THE SAME SCHEDULE AS THE TEXT-STD/REST.

* SAME TIME AS ABOVE ...
Actually, take this opportunity to overhaul the concept of qualified
language names and such.  Promote "dialects" from being subservient to
language into being full languages of their own that are derived from
Muldis D.  That is, say that what was "Plain_Text" is now simply *the*
concrete syntax of the official "Muldis D" language, and that what was the
Perl dialects are now 2 full languages whose definitions are derived from
the standard text-based one, and have their own versioning.
- So, here are new examples of fully-qualified language declaration names:
    Muldis_D:Plain_Text:Unicode(6.2.0,UTF-8,canon):"http://muldis.com":0.149
    ['Muldis_D','HD_Perl5_STD','http://muldis.com','0']
    ['Muldis_D','HD_Perl6_STD','http://muldis.com','0']
- Actually, we are best to split things up so that Muldis D code typically
declares 2 main versions separately, which are the concrete syntax version
and the system catalog / core libraries version.  Also, the Muldis D spec
itself would start using different version numbers for different parts,
so the concrete syntax and system catalog or core libraries would be
differently versioned and would only increment when those parts change.
- The Muldis D core spec could use at least 3-4 version series:
    - The version of the documentation distribution: 0.149 is next.
    - The version of the Core (name subject to change) library, which
    determines the data types and routines intended for users to use.
    - The version of the system catalog schema, that is the version of the
    "real" Muldis D concrete syntax / data structures as seen by code.
    - The version of the Plain_Text concrete string syntax that users write in,
    which a parser needs to know but the system catalog doesn't.
    - All of the above would start at version 0 except the first item.
    - The second item Core is what is expected to change the most often,
    while the catalog or string syntax are expected to change less often.
    - The second/Core and third/Catalog might be stay merged into one item
    as they are kind of inter-dependent, but we may review minimizing Core.
- Example of Muldis D code declaration of language:
    Muldis_D:Plain_Text:Unicode({1..6.1},UTF-8,canon):"http://muldis.com":{0..42};
    pkg ::= package FooLib:"http://foo.com":0;
    MD ::= using Muldis_D:"http://muldis.com":0;
    Spatial ::= using Muldis_D::Spatial:"http://muldis.com":0;
    Pg ::= using DBMS::Postgres ... emulations of all Pg types/routines/sys-cat/etc
        ... this is defined as a wrapper over Muldis_D/etc, but in a MD
        impl over Postgres, these would likely be implemented natively
        and then Muldis_D::* is implemented as a wrapper over these
    dfoo ::= using DBMS::foo ... emulate other DBMS
    Perl ::= using lang::Perl ... emulate Perl
    lfoo ::= using lang::foo ... emulate foo
    vxyz ::= using VendorXYZ::FooLib:"http://vendorxyz.com":0;
    Lulz ::= using MyAppBar::Lulz:"http://mybusiness.com":0;
    searching [pkg,MD];
    ... then our own module/depot/value/etc content follows ...
- Note, "using" is the C# keyword for this.  Note, "System" is C# core lib.
- Note, "java.lang" is core Java package; it is precedent for "Muldis_D::*".
- Note, a Java "package" is like MD's package/module concept, in that it
can define multiple classes/interfaces etc as per my multiple types/etc.
- Having DBMS::* and lang::* etc makes it easier to build compatibility layers
as eg a SQL parser just has to parse into DBMS::foo stuff and that's it.
    - A DBMS::* package also defines alternatives to Tuple/Relation types
    to represent SQL ROW/TABLE/etc types; these most certainly are all
    Scalar types from Muldis D's perspective and loosely are Tuple/Relation
    wrappers but that extra meta-data is included such as a concept of
    attribute order that is built-in to each value, or of attributes being
    allowed to have non-distinct/anon names, or of having certain 3VL etc.
    - Probably make the "real" names as numbers, like SQL does, and the
    text names are then aliases.
    - Our letter case of package entity names matches the folding rules for
    that DBMS with unquoted identifiers; in Pg they're lower, others upper.
- We use "::" as separators in package/module/extension names because those
are more public namespaces, whereas "." is used for internal namespaces
that subdivide each individual package/etc.  By doing this, if package Foo
exists with internal namespace Bar and routine baz, there won't be any
ambiguity if Foo::Bar comes to exist with a baz() in its root namespace;
they are invoked as Foo.Bar.baz versus Foo::Bar.baz so a clear distinction.
- The package name of the Muldis D language core is simply "Muldis_D"
(keeping it simple/terse); so no reason to explicitly say "Core"/etc.
- Strictly speaking, all other packages don't need to have any particular
hierarchy or be in any particular package namespace, though it would be
good to come up with something early on.
- Note that say a code file that is simply a direct translation of, say,
SQL, may not "using" Muldis_D::* at all, but just the DBMS::* in question.
- In fact, we should emphasize a feature of Muldis D where it is suitable
as a very good alternate syntax for SQL, easier for ORMs to build, etc.
It can exactly repr semantics of each SQL dialect while being cleaner syn.

* Version numbers of anything by authority http://muldis.com are formatted
as strings of 1..N nonnegative integers, where each string element is
separated by a period, and strings sort pairwise one element at a time from
start to end, where each element sorts numerically and shorter strings sort
before longer ones whose leading substring they are equal to.
- By default, version numbers are just a single string element and add more
when they feel they need to; most of the time, just 2 explicit elements are
used, and the initial version of any project tends to be the single zero.

* Make the fully qualified language names declarable by code to be more
flexible than the names declared by the language spec itself, so that the
ones in code can specify multiple language versions that they conform to,
as if the code is declaring that it only uses the parts of the language
spec that are unchanged or that intersect between all the specified
versions.  For example, let one say:
    Muldis_D:Plain_Text:"http://muldis.com":{0.112..0.125,0.127..0.136}
... and then any implementation which takes one of those versions will
parse the code according to any of those same that it supports.  The idea
is to make code more easily compatible with a wider range of interpreters,
such as newer ones designed for version 25 who don't explicitly know how
to emulate version 23 or know what its differences are, and are just
trusting the code to be valid for version 25 even if declaring 23; by the
code declaring a range, it is declaring it is willing to take its chances.
- When an implementation provides or emulates multiple of the language
versions that the code declares itself to be valid for, the implementation
is free to treat the code as if it was just any one of those versions, and
which version is chosen is formally undefined; the implementation can just
pick any version that it chooses and the code compiles/runs or not as such.
If the code fails to compile/run as any random version it claims to be
compatible with, the implementation shall *not* try again as another
version in the list as a fallback, probably.
- A multiplicity of stated authorities is also possible.
- A number of consequences still have to be thought out here.
- Specifying a closed range is the code saying it *knows* it is compatible
with all those versions, specifying an open range says take chances or is
not recommended and maybe won't be supported.
- Or, rather than having open ranges, we could stick to just closed but
have both whitelist and blacklist ranges, where we explicitly specify what
versions we *know* either are or are not compatible, and so then any
versions not explicitly named in one of those 2 lists is considered unknown
as to compatibility.
- Similarly, an implementation could be introspectable for the same 2 lists
white and black that it supports for implementing.
- It would be possibly implementation-defined or externally
user-configurable for what happens if the implementation only supports
language versions outside the whitelist including versions outside the
blacklist.  A stricter setting would say reject, permissive may say accept.
- For permissive, its possible that if vnums are fully orderable, then a
nonmatching implemented vnum directly between or just next to versions in
the whitelist could be considered to work, likewise for blacklist
considered not, and between one in each list considered not.  Eg, if the
program code declares {!1..5,6..10} and the implementation just provides
version 12, then it could be considered to work by default.
- In the interest of being nullary, we could permit code not specifying any
version number at all, or authority, same as Perl 6, meaning that the code
does not explicitly consider that it would work or not work with any
specific language/module/etc version at all.  So a strict implementation
would reject all such code out of hand and a permissive one would accept
all such.
- On the other hand, in the interest of preventing bad habits from
starting, where people just leave out the auth/vnum specifiers and then
people seeing that code not realizing that being specific is even possible,
we could say that it is always mandatory to specify at least a
single authority+vnum that the code is known/declared to work with, so that
any implementation has at least some idea to work with as to the likely age
of the code or what implicit expectations it might have.  The writer of the
code had presumably been testing it with *something* before releasing it.
And even neophytes who copy-paste whole code would be getting either
correct behavior or a rejection by an implementation from version diffs.
And savvy people that go around the internet can more easily spot bad or
outdated code examples in tutorials or script archives because they'll just
have an old dependency version specifier and not mention newer versions.

* Consider use of the term "curator"/ed/ion/etc with respect to authorities.

* Lets say that language name declarations are mandatory in code files but
are optional in ad-hoc code read from a shell or fed from a host language,
because in the non-file cases the shell context or host language could
previously/separately set the context for what language is expected.  In
the case of a shell, lets just say that a language name declaration is
written just like a shell statement or command; we know its that kind of
command if it looks like a language declaration, as no normal code would.

* See also Jesse Vincent's "Perl 5.16 and beyond" talk, where Perl 5 core
policy or Jesse's proposals for such are discussed that when you say "use
v5.xy" then no matter what the actual (newer) executing Perl 5 version is,
you have declared that you expect it to behave with the same semantics as
the same version 5.xy that you named in your code.

* See also
http://tech.valgog.com/2012/01/schema-based-versioning-and-deployment.html .

* Note, http://strangelyconsistent.org/blog/dash-n-and-dash-p-part-three
clarifies what "setting" means in Perl 6, and there can be more than one of
them at a time that are layered; it does not mean "language core"; Perl 6
has the separate CORE concept, so staying with Core seems approp for MD.

* Remove TTM concept/language of "possrep" from Muldis D spec, at least in
the way it is presently used.  Instead, each scalar base type is simply
defined in terms of exactly 1 list of components/attributes, which doesn't
have a name (or corresponds to the empty string).  Also, the idea of
"possible representation" actually applies to the whole language, including
nonscalar types, because the DBMS can still choose any physical
representation it wants; its not like scalars are special there to reserve
the term "possrep".  Additional "possrep" are just syntactic sugar for
wrapper routines or such or pseudovariables.

* Quoth Anthony Clayden: "Specialisation by constraint is the defining
characteristic of the TTM model".

* Don't use the term "scalar" to refer to things that are neither relations
nor tuples as TTM does; conversely don't use a term to refer to things that
are either relations or tuples but not something else; just say "tuples or
relations" when that is meant, which wouldn't be terribly often, and
similarly "values/types that are neither tuples nor relations" when meant.

* Change Array/Set/Bag/Dict/Interval/Maybe/etc so that they are no longer
relation subtypes but rather are disjoint.  Generally eliminate any
system-defined tuple or relation subtypes, leaving all such to users.
This should help eliminate a number of gotchas from Muldis D that could
trip users up, such as users sometimes having to know the names of the
special relation attributes despite those being hidden in practice.
Also we would not confuse semantics; eg, tuples and relations
have predicates but sets/arrays/etc do not.
Emphasize the fact that Arrays are quite different from all the those other
types, in that they have a dense keyspace starting at a fixed point, where
the others are all sparse; eg, never make Array a subtype of Dict or such.
Similarly, consider defining Array directly in terms of List rather than
say in terms of a "scalar" with a Relation component like with Dict/etc,
because List can be used natively; but leave unordered types as over rels.
So Array's like, but disjoint from, String; list of anything versus of int.
The various details need more thought.

* Have the 10 Relation subtypes
"Relation.{K0,K0C0,K0C1,D0,D0C0,D0C1,D1,D1K0,D1C0,D1C1}" where "D" means
"degree", "C" means "cardinality", "K" means "degree of minimal key".
Having D0 implies K0 but K0 does not imply D0; also, K0 bi-implies C0,C1.
And thus we have names for all the relations that can be used unambiguously
without any need to know their attribute names for special purposes.
The D0s are the boolean or identity values for join.
The K0s (and D0s) are the "maybe" types with C0->"nothing", C1->"just";
but there is no single "nothing" value anymore, rather an infinity;
for that matter, redefine the type "Maybe" as an alias for "K0"; despite
the heritage, we may want to ditch the "nothing"/"just" names actually.
The D1s are convenient in that one may reference their attribute
unambiguously without knowing its name.

* All the selector syntax specific to Set/Array/Bag/etc would produce
non-relation values, as if it did, the attr names would be implicit.
But Set-taking functions should also take any D1 relation as an alternate.

* In an attribute list, the first of these is a shorthand for the others,
or specifically for the second, but the third has the same semantics:
    maybe foo : Bar;
    foo : maybe_type { foo : Bar };
    foo : relation_type { foo : Bar; primary_key {} };
... but while the first can only do D1K0, the latter can do K0 in general:
    foo : maybe_type { x : Bar; y : Baz };
    foo : relation_type { x : Bar; y : Baz ; primary_key {} };
So the new maybe is a more workable/flexible way of handling nullability.
Actually, the "maybe foo : Bar" reads interestingly because it might
suggest to some readers that having the "foo" at all is optional;
however the latter thought is not one that we want to encourage per se.
Remember, "maybe" and "set"/etc are now completely disjoint.

* Operators like "//" that deal with maybes need to be closed, where both
arguments must be maybes and the result is a maybe.  This means that the
final/default value in a //-chain must be a just, eg:
  x // y // :@{{3}}
... where each arg can be any zoo relation, and :@{{3}} means :@{{0:3}}.
And so the //-chain is then followed by attr extraction when that's wanted,
eg: "attr(x // y // :@{{3}})" for D1K0s or "(%(x // y // :@{{3}})).foo" or
something for K0s in general where we must name the attr.
Note that a // defined this way can't have an identity value since multiple
values would qualify for the semantics, same as with bitwise-or.
Note that overloading //, eg to work with some "special" value to mean
null, can only accept non-relation args in practice so not to overlap.

* Make a depot/etc have 3 main payload sections (each of them being a
"Database") rather than 2; where we had "catalog" and "data" before, now
add "constants"; names of the 3 subject to change.
- The purpose of "constants" is to be the most effective way to represent
the bodies of non-trivial niladic functions, or the values of non-trivial
expression trees with no free variables.
- The "constants" are semantically part of the code, and changing them is
considered a data-definition rather than a data-manipulation; however they
are stored directly as the values they represent, same as things in "data",
rather than as trees of expression nodes in the catalog.
- Unlike things in "data", you can refer to "constants" directly in the
bodies of function or type definitions.
- Each constant typically lives in its own RVA of the constants database,
or several may be in the same if they have the same structure or type.
- An example usage is that the Unicode character database could be
accessable to Muldis D within a "constants" database of some appropriate
module.  Another example is parser rules defined as relations.  Another
example usage is for the user text of programs, or exception/error
messages, especially multi-lingual ones.
- Where one wishes to comment individual items in a collection constant,
they do so by adding extra RVA attributes for the comment text.
- You do not normally use constants to define enumerated data types that
define new values, such as Boolean etc, which are unions of singleton
types; but you can use constants to define enumerated types that are subset
types of others.  Constants are analagous to values/functions, not types.
- Constants live in the same shared schema-namespaces that are shared by
types/routines/etc and relvars, and all must have distinct names therein.
It is as if the "constants" and "data" databases are superimposed, so no
RVAs in one may have the same names as those in others, but schema/TVAs may
exist in common.
- One always uses function syntax to refer to "constants" RVA/TVA within
routines, as if each one was in fact declared as a niladic function.
- The only way to refer to "constants" as variables in routines, for
dynamic data-definition, is in procedure pseudo-parameters, same as when
referring to the catalog database or "data".
- We would probably have special/shorthand syntax for declaring constants
in code like we have for declaring types or routines, etc; for example:
    FOO ::= constant (...);
    dbname.schemaname.BAR ::= constant (...);
- Similarly, whatever syntax we use for nesting a higher-order function
definition or a type definition in a routine, we use something similar for
nesting a named constant declaration in a routine, in order that it be
treated as a "constants" item rather than an expression tree that it would
otherwise be.
- You can't use "constants" for values that are not deeply-homogeneous,
like something kept in a "Database" must be; for those you have to use
regular expression trees.  But it is expected that in practice any values
that aren't deeply-homogeneous would be transient or generated at least
partly from free variables, so this limitation shouldn't be a problem;
or you should likely be able to refactor the values down into
deeply-homogeneous inputs to a simple regular value expression or niladic
function which then gives the actual desired vaulue.

* CHANGE AFFECTING THE ABOVE ...
Change the "Database" type so its attributes may be "Relation" in general
and don't have to be "DHRelation"; that is, the type of a database relvar
attribute may now be anything up to "Universal".  Or alternately, perhaps
go for some kind of middle ground where we say that every relvar tuple
attribute value must be a Scalar|Tuple|Relation and not some other List.
- Or alternately, the language supports "Universal", but users can request
varying levels of strictness for their databases, such that it should be
very easy for them to declare they only want to allow DHRelations; then
again, the existing facilities already let them ... the declared db type.
- This should simplify Muldis D in some respects, because we would be
removing a language limitation that is more arbitrary than essential.
- Now this change might seem to complicate certain database design or
implementation issues, in that the general case of a relvar TVA/RVA would
no longer be refactorable into a single other relvar per TVA/RVA, if
different relvar tuples might have tup/rel attrs of different headings;
each possible one would then have to become a separate relvar, or otherwise
such refactor just wouldn't be possible.
- However, in practice we already face this problem by our TTM-blessed
support for scalar union types in relvar attributes, generalized to
"Scalar" typed relvar attributes, should we, say, wish to unpack such
scalar values into relvars, or implement as such behind the scenes in some
engine that just likes to have simple homogeneous scalar relvar attributes.
And so, allowing it for TVAs/RVAs just makes things more consistent.
- I also don't see any logical issues with this support, in general.
- This change also means that a "constants" db *doesn't* have to be DH.
- In theory we could also take a next step and have Muldis D support
"databases" that aren't relational at all, as an option, but rather let the
"database" value be of any type at all.
- Or to keep things more manageable, we still require that a "database" is
some kind of "Tuple", but then it is no further restricted than that.
That is, we require "Tuple" so it can be overloaded as a namespace for
types and routines etc, including the restriction on when type/routine
namespaces and attributes may be equal or not, if the current language
grammar even requires them to align, but we will assume that it does.
- So then, the system-defined type "Database" simply goes away, as "the
database" is just a "Tuple", but we can make it very easy for a user to
define a "Relational_Database"/etc subtype with whatever level of
restrictiveness they desire.
- And then, one can also more directly represent a SQL database in Muldis D
(or some other non-relational db/storage du jour), not just by using the
appropriate packages that define DBMS-specific types/routines, but by also
declaring that "the database" is of some DBMS-specific/allowed type, such
as, that all database tuple attrs are "Table" typed rather than "Relation".

* Make Muldis D self-hosted.  Have it support parsing Plain_Text code using a
read-only operator into trees of scalars that are the new native Muldis D
code, which the older system catalog is just views on.  Make it support
writing the full Muldis D compiler in Muldis D, that can eg generate Perl
code, and then the Muldis D compiler can compile itself.  That is, the
reference implementation would be compiled to Perl, and that Perl code is
capable of translating equivalent Muldis D code into that same Perl code.

* Ooh! Oooooh! The trees of scalars form of Muldis D code might just then
give us the option of more comprehensive higher order functions, where you
actually pass the function definition as the argument, rather than its
name, as you do now, though the latter would still be perhaps easier to
implement on more foundations. We'll see if that's necessary or works.
That would certainly be more of the true anonymous function nature.

* Note that the typical action of Muldis D implementations could be called
"transcompiler" or "source-to-source compiler" because they translate
between high-level languages rather than directly to machine code.

* The new system catalog would have several main namespaces where each
comprises a different view of the same package code.
- The primary view, maybe call it "trees", would present each complete
package/module/extension as a single scalar value that is a concrete syntax
tree package node, whose attributes contain the rest of the tree defining
the whole package.  These trees are composed of the nodes that are now the
native Muldis D code format, which Plain_Text very closely approximates, and
the HD_Perl*_STD slightly less closely approximate.
Preserves details such as: whether child expressions were written nested
under parents without explicit node names vs nested with explicit names vs
off-side with explicit names, and
all code comments that are explicitly bound to names or parts of code like
named declared entities, and the visual sequence of code elements in the
string source, and whether operators are invoked prefix or infix, etc.
Does not preserve details like the actual whitespace in the code, or the
actual string escape sequences, or whether identifiers were quoted or not,
or the exact format of the numeric literals, and so on.
Does not explicitly hold gen names for nodes that were anon to programmer.
- Another view, maybe call it "relations", can be loosely what the
current/old system catalog is, having all of the same information as
"trees" but all broken out into a flat namespace, like the current/old but
that all the RVAs/etc are basically ungrouped or split off into separate
relvars.  Or maybe it would be partially flattened and partially not,
whatever makes for the easiest use by people, or there could be multiple
sub-views showing varying degrees of being flattened.
For various reasons it may be wise for "relations" and "trees" to
be information-equivalent, so each is fully updateable with changes
propagating to the other deterministically and losslessly.
- As "relations" has explicit names for all expr/etc nodes while "trees"
only does when the programmer explicitly provided them, names must be
generated when deriving "relations" from "trees" and stripped the other
way; the names must be generated deterministically, and a constraint must
exist on "relations" so explicitly setting them differently is disallowed
while the "generated name" attr is also true on that node; expect that said
gen name is probably something like catenating the names of the ancestor
nodes in the expr/etc trees starting with the nearest non-generated one;
the generated names could take the form of a chain of integers, one integer
per ancestor level, where the integer is the ordinal position of each node
beneath its parent, starting at zero.
- Another view, which is not information-equivalent to the others, would
have further derived information, say more resembling the SQL system
catalog, say showing in one place what all the actual attributes or known
keys constraints of a relvar are, regardless of whether they are defined
using special syntax or as value expressions.
This view would be writeable, but as it is a dependent on the others,
changes to it would propagate in certain ways, such as how changes to a
lowercase attribute would propagate to a mixed-case determinant one.
- The above views, or at least the 2 information-equivalent primary ones,
would be provided by the language core; the last one might be an extension.
- To clarify, views are allowed between modules, so eg one can define
another kind of system catalog, but only between modules that are loaded as
extensions rather than depots, I think.

* An EXTENSION module, eg Muldis_D::Plain_Text, is what provides the routines
to translate Plain_Text code strings into Muldis D's native dialect, the trees
of scalar nodes that the "trees" catalog view uses, and said extension also
provides the reverse translator; working with Plain_Text code strings is *not*
part of the Muldis_D core module, and such would only be needed in a
Muldis D program implementing a REPL or a compiler or one that uses Plain_Text
as its disk storage format.
- This extension would also define another set of syntax node types which
holds a lot more concrete syntax details than the native node trees, such
as the actual whitespace used between things or the actual format of
identifiers (bareword vs quoted) or any extra/explicit nesting parenthesis
and so on, such that the exact same Plain_Text
code string could be reproduced from it; mainly these would just be used
internally to the extension, except say when someone wanted to extend it.
- The Plain_Text->native translator function would probably have 3 main other
functions that it invokes serially, where one is a tokenizer that just
converts the Plain_Text string into a flat array of tokens, where each token
may be represented by some token-type-specific scalar value rather than
being a Text value, so eg each end of a delimiter pair and whitespace hunk
would be a separate token value here, and the second function takes this
token array and converts it to a tree, eg combining delimiter pairs into a
single value with children from 2 without; the third converts this full
concrete tree to the quasi-concrete tree of the native Muldis_D.
- It is expected that some level of Muldis_D::Plain_Text is what deals with
unescaping string literals or compat-normalizing Unicode etc or otherwise
following the wishes of concrete syntax directives.
- Similarly, the extension would be what determines whether a string is
valid Plain_Text, including that it provides a "is this Plain_Text" bool func.
- Similarly, Muldis_D::Plain_Text would provide config options for the
translater to Plain_Text from native, say on what num format opts to prefer
or what length to wrap lines etc.
- To help specify a pure functional parser that is both resistant to having
too-deep levels of recursion (say, a process defined on head+rest), we
should utilize some of Muldis D's more unique features, mainly the
relational operators, and define the parser largely in terms of say
relational join and such, so it is more conceptually parallel than
recursive (or iterative).
- Have a function that derives an Array-of-Codepoint (or similar relation)
from a String, then use it on the Plain_Text source string as the first step,
and then say do tokenizing in terms of joining that relation against a
static relation having say 1 tuple per distinct character in the text
repertoire, mapping for example traits like is-whitespace or is-symbol or
is-alphanum or is-string-delimiter or is-escape-char (\), so say the result
of the join is the indexes into the original string of where tokens are.
Filtering out say escaped chars from unescaped ones is easy; find all
occurrences of the char and then set-minus those with a "\" just before.
It is useful to do edge detection early by joining a folded (to char kind)
string to itself offset by one index position and seeing where the pairwise
characters are not of the same character kind.
The static relation of characters may be best stored in code in a compacted
range-defined form, and if expanding is needed, do it when needed;
then we may only need to store a few dozen tuples rather than thousands.
- This same design would scale transparently to different repertoires,
eg ASCII vs Unicode; just the static tables of characters are larger.
- The very first step in parsing Plain_Text:Unicode(...) is to bring the source
Text to a normal form, either "canon" or "compat" (with semantics of NFD or
NFKD in the spec) and then the Plain_Text parser proper then works with that
Unicode normal form as its input.
- We would have a whitelist of codepoints that may appear outside of a
quoted string in the source, and all others are illegal outside, for
security purposes (lookalikes) if nothing else; it is applied post-normal.
This makes it similar to pigeonhole charcters into symbol vs alpha for
example, where that distinction mainly just matters for barewords.
- We have to figure out how best t work w Unicode combin chars for parsing.
- Note, according to http://en.wikipedia.org/wiki/Unicode_symbols, Unicode
has a main distinction between "scripts" and "symbols".

* Thanks to syntax/package separation, we have a nice semi-orthogonal
situation where Plain_Text syntax can be used to write Muldis D or SQL,
and SQL syntax can likewise be used to write both, in theory.
- That is, the package determines the "semantics", the grammar the syntax.

* It remains to be specified how to support code expecting different
versions of Muldis D or other modules, such as where to say no and where to
emulate and at what level to emulate.  Perl 6's solution may be a guide.
We might consider multiple packages as a solution where different packages
provide different versions of the Muldis D API, and they can be used at
once, similar to having packages provide the APIs of SQL / other languages,
so that the parsers eg for Plain_Text don't have to do it themselves for
modules but rather just for concrete syntax vers.  Still open to debate.
- The best way to do this emulation cleanly, for package/module versions,
is that in the general case we consider every name+auth+vnum permutation
to be a distinct module, and that permutation is the actual name of the
module, so code expecting/using different module versions is using
different names to refer to its entities.
- It is mandatory for a package/etc to, for each of its explicit module
dependencies, to also declare a package-local alias for that dependency,
by which name it must always refer to the package or its entities.
- The effect that this has is like that of the global params of recipe
routines; there is no longer a global namespace of used packages in the
DBMS, but that rather each package's code only ever directly sees within
the same package, and any used packages' contents are aliased to namespaces
within the using package; the language core pkg is treated the same way.
- An implementation may provide packages for different versions of the same
base package at once.  When a package's "using" statement indicates that a
range of versions are suitable, the DBMS implementation would just pick a
single one of those for which it actually has that dependency package
version and associate the user's local package alias to that specific
version longer name.  Since a "using" specifying multiple versions means
any of those should have the same semantics for features used, the DBMS is
also free to remap at runtime which of several satisfying versions it has
to the alias, so that say if multiple dependent modules specify overlapping
but unlike dependency versions, the DBMS can choose to use the same version
it has for both in the intersection of those ranges, without breakage.
A package's code should not generally need to know which specific
dependency version it actually got or has, though there might be rationale.
- The package-specified alias specifier might conceivably be more generic
than just a Name; it might be a path in the using package's namespace into
which to "mount" the other package's namespace.
- Depending on how the aliasing works, conceivably if package Y explicitly
uses X and Z explicitly uses just Y, Z can also access X's namespace within
Y's namespace that it sees.  ACTUALLY, NO, that's a bad idea; Z should not
see any part of Z by way of Y but for Y's explicit synonyms of things of X.
- Regardless, when dealing with higher-order functions and such, those
functions are a closure of sorts wherein all references to package entities
in the function code are resolved relative to the packages where the
functions are declared, not relative to where they are actually invoked.
- We also need to revisit matters of depot mounting and such, and see if
the aliases we declare there and for packages can be generalized together.
- Perhaps from each package's perspective, the entity namespace is this ...
    /local/*  - current package's namespaces/entities
    /used/<alias>/*  - each used package's (or core's) namespaces/entities
... those being the absolute paths.
- Or perhaps better yet, if each package is also required to declare an
internal alias to refer to itself with, the above can be flat, just:
    /<alias>/*  - current or each used or core pkg's namespaces/entities
- Separately, each package needs to declare an explicit ordered list of
search paths that it will use internally by its own code to resolve
unqualified entity references in its own code.  Typically the first 2 items
in the list will be the package's own self and then the core language
package.  The list can be either empty or include all the used packages;
searching will only look in this list, and one can't use search/unqualified
syntax to reference anything outside this list, just relative or absolute.
- Example:
    Muldis_D:Plain_Text:ASCII:"http://muldis.com":0;
    package pkg ::= FooLib:"http://foo.com":0;
    using MD ::= Muldis_D:"http://muldis.com":0;
    using Pg ::= DBMS::Postgres ...
    using Spatial ::= Muldis_D::Spatial:"http://muldis.com":0;
    searching [pkg,MD];
... uses alias "pkg" for current package, "MD" for core, and so on.
- Making the explicit search path list mandatory in order to get
unqualified referencing is important because then neophytes can't just
copy-paste parts of code from random online tutorials or script archives
without being explicit about resolving any references therein, which can
trip up people where the same code may resolve to different operator calls
depending on what modules are used.
- Perhaps having just the language core in the search path list and not
even the current package is best in some circumstances.
- Note http://www.postgresql.org/docs/current/static/ddl-schemas.html#DDL-SCHEMAS-PATH
which has a "search_path" setting that works similar to this.
- But the aforementioned is for schemas; therefore Muldis D may be best to
offer a more finely-grained option, where Muldis D's search path takes a
list of name-chains rather than names.  This has an advantage of letting
one select just parts of a package to search, or the order within a package
to search, etc.  Each name-chain in a search-path list can be either
absolute or relative, the latter being resolved relative to each individual
reference; or better yet, make each search-path item absolute only and that
the language separately has the option to search relative to the
referencing entity first.  This needs more thought.
- For implementation, we are probably best to preindex all the packages,
where all possible unqualified or semiqualified namechains are mapped to a
list of all fully qualified chains they match the endings of, so no actual
"search" is necessary; we just have to evaluate on arguments later.

* We also need some distinct syntax for defining that a package definition
is split into multiple pieces for storage, which for source code typically
means one disk file per piece, but they are logically still one package.

* Consider making the "lib" or "cat" or "data" portions of namechains
optional in contexts where they would always be the same, especially "lib",
so for example one can just say "nlx.myfunc" rather than "nlx.lib.myfunc";
this would be loosely similar to the elimination of "lex".

* For that matter, consider restructuring the namespace tree in Basics.pod
so the lib|cat|data|etc are the namechain first element, with fed|nlx|etc
the second element; and sys doesn't have lib|cat|data already, or in fact
that would all be grouped under "lib" arguably.

* For that matter, lets just eliminate the special namespace prefixes
entirely, even as an option, because typically certain contexts only allow
certain kinds of things anyway, such as just lib or data, and so one should
never have to or be able to write chain elements that mean anything other
than their literal selves.
- Rewrite ENTITY NAMES in Basics.pod to get rid of the formally structured
shared namespace as it currently is, and instead have that pod divided into
context-based sections, saying for example "in this lexical context a
namechain means this" and "in this nonlexical context it means this".
- Where one needs to explicitly use an absolute path vs an explicit
relative path vs a searching relative path, we can provide a new data type
which wraps the basic name chain of old to provide this.
For examples (new arrayish structure <-- old way of stating):
  ['abs',['mydb','myschema','myfoo']] <-- fed.{lib|data}.mydb.myschema.myfoo
  ['rel',2,['myschema','myfoo'] <-- nlx.par.par.{lib|data}.myschema.myfoo
  ['rel',0,['mybar']] <-- nlx.{lib|data}.mybar
  ['shp',['myschema','myfoo'] <-- shp.{lib|data}.myschema.myfoo
The above could possibly be written like this:
  .$mydb.myschema.myfoo or abs$mydb.myschema.myfoo
  2$myschema.myfoo or rel$2$myschema.myfoo
  0$mybar or rel$0$mybar
  myschema.myfoo or shp$myschema.myfoo
I'm still missing an example when one wants to say "sys", maybe, though
both "abs" and "shp" kind of apply there but kind of not.
Note that in the above it is mandatory for there to be no whitespace
around the "$" ... the formatting is like with "#" in numeric literals.
- Now while there are {$:,%:,@:} prefixes for {S,T,R} value literals, and
the colon-less {%,@} mean cast between tuple and relation,
the colon-less $ doesn't make sense for a similar purpose,
so use $ for name literals, that is, "$foo" means "Name:foo",
so one can write projection like "r keep {$foo,$bar}" not "r{foo,bar}",
and then for example "$<>foo" means "NameChain:foo" (and "$<>" the emp nc).
- Also add the ability to use namechains in declarations too, which is
essentially a shorthand; for example, this:
  foo.bar ::= function ...
... is logically equivalent to:
  foo ::= schema {
    bar ::= function ...
  }
Now technically this new way only lets one declare a nonempty schema but
to make an empty one one can still always say:
  foo ::= schema;
- The scalar type name stored inside a ScalarWP is always an 'abs' one,
or some separate canonical ns that takes into account type name aliasing.
- The scalar type name of a scalar value actually stored in a depot
can/should be exactly what the user wrote as per source code; it only is
resolved to some global namespace when compiled from the system catalog;
this is visible to the user of course in the low-level type system when
looking at scalar values, but not in the system catalog / source code.

* To keep things simple, the core Muldis D language does *not* for the most
part have any Unicode knowledge or knowledge of any character repertoires
beyond what common legacy 8-bit encodings handle; extensions are where
Unicode/etc goes because it is sooo complicated and large.
- The Muldis D core still fundamentally considers String/Name/Text to just
be a sequence of "big" integers, and so still supports Unicode etc, but
what the core lacks is any knowledge say about what Unicode characters are
letters vs symbols vs whitespace etc or base vs combining or normal forms
or canon vs compatibility or case folding rules and collations so on.
- The Muldis D core has the "Text" type which just has the integer string
possrep, and it has proper subtypes for ASCII,Latin1,etc (maybe EBCDIC?)
where each adds a possrep.  The Muldis D core knows a small amount of
Unicode, but just the subsets of its repertoire and character codes/names
for characters in Latin1/etc, such that the base number string possrep
still is valid Unicode character numbers for those characters.
- An extension, say Muldis_D::Unicode, adds the complexity of knowledge for
the rest of Unicode, including the NFD and NFKD Text subtypes/possreps etc.
- The Muldis_D::Plain_Text string code parser also comes in 2 or more versions,
where said base version only can work with source with th same simple/8-bit
knowledge as the Muldis D core, and you need another version say
Muldis_D::Plain_Text::Unicode to handle source using the wider repertoire,
at the very least so it can parse code properly where the parsing semantics
depends on the notion of what characters are letters or symbols or ws
and on canonical vs compatibility notions of equivalence.
- The Muldis_D declaration at the top of a code file also indicates what
character repertoire or encoding or etc the file is written in (which may
even include the "auth" part of the name), so it appears almost-first,
say like this:
    Muldis_D:Plain_Text:ASCII:"http://muldis.com":0;
    Muldis_D:Plain_Text:Unicode({1..6.1},UTF-8,canon):"http://muldis.com":{0..42};
    Muldis_D:Plain_Text:Unicode(6.0,UTF-8,compat):"http://muldis.com":0;
- There may be further variants for diff versions of Unicode standard.
- Muldis_D::Unicode also adds the synonyms for core routines/etc that have
Unicode character names, while the core just has the Latin1 char names.
- There is still the question about avoiding combinatorial explosions about
lots of other modules that want Unicode op names but that should also
degrade gracefully for implementations lacking them.  Probably one easy way
out is that these can just declare these with declared delimited entity
names using numeric escape sequences rather than literal Unicode chars.
- We should be using subtype polymorphism to our advantage.  The Muldis_D
core package should declare a mixin type "SourceCode" or several such like
"PackageSourceCode" or "ValueSourceCode" etc and then declare a virtual
parser function whose input type is "SourceCode" and whose result type is a
node of the native Muldis D language.  Then Muldis_D::Plain_Text would declare
a subtype of Text that composes the mixins say "PT_STD_SourceCode_ASCII" as
well as a function implementing said virtual parser which takes that ASCII
type as input, while Muldis_D::Plain_Text::Unicode does a corresponding thing
but for a different subtype of Text as appropriate.  These Text subtypes
would have their defining constraint as, Text value that starts with the
string "Muldis_D:Plain_Text:ASCII:" and such.  And so the logic for what parser
to select in a generic case just comes down to regular multi-dispatch based
on analysis of the Text values for characteristics specific to a language,
and it is easy to auto-extend the system with support for new languages,
simply by somewhere "using" a package that decl an impl of the virt func;
but a main importance is that the Unicode vs not thing is no longer a
complicating factor for users.

* Here is the main program of a possible Muldis D cross-compiler to Perl 5
written in Muldis D; it reads Plain_Text Muldis D code from STDIN and writes
equivalent Perl 5 code to STDOUT.  One of the first goals of implementing
Muldis D should be to empower this being able to compile itself.
Presumably all of its dependencies would start out as shims that are
written directly in Perl, which are gradually converted.  Of course, the
following would initially also have to be written directly in Perl too, to
bootstrap. This is a static compiler, which by default would succeed in
producing valid Perl 5 code just as long as the input Muldis D code is
syntactically correct (and defines a Muldis D package), in which case its
output would compile successfully in Perl 5.  Generally all errors which
would not be caught in that process are of the nature of invoking or
referencing some entity that doesn't exist or is of a mismatched type etc,
and then by default those would produce errors when running the Perl code.

    Muldis_D:Plain_Text:ASCII:"http://muldis.com":0;

    package perl5_from_mdpt ::= perl5_from_mdpt:"http://example.com":0
    {
        using Muldis_D         ::= Muldis_D:"http://muldis.com":0;
        using Muldis_D::Plain_Text ::= Muldis_D::Plain_Text:"http://muldis.com":0;
        using Muldis_D::Perl5  ::= Muldis_D::Perl5:"http://muldis.com":0;

        searching [Muldis_D,Muldis_D::Plain_Text,Muldis_D::Perl5,perl5_from_mdpt];

        bootstrap ::= stimulus_response_rule
            when loaded invoke main;

    /*************************************************************************/

    main ::= procedure () :
    [
        /* Read the Plain_Text source code from STDIN until end-of-file.
           Today we'll just assume that it defines a package. */
        var input_source : Text;
        read_Text_file( &input_source );

        /* Validate the input source code and exit if bad Plain_Text syntax or
           it doesn't define a package. */
        if not (input_source isa $PT_STD_Package_Source_Code_ASCII) then
        [
            write_STDERR_Text_line( 'Sorry, that source code has a syntax error.' );
            leave;
        ];

        /* Parse the input source code into a Package-defining AST value.
           This AST is the "native source code" of Muldis D. */
        var package_AST : Package;
        package_AST := Package_from_Source_Code( input_source );

        /* Compile the Package-defining AST into Perl 5 source code which
           when run has the same semantics. */
        var output_Perl : Text;
        output_Perl := Perl5_from_Package( package_AST );

        /* Write that Perl to STDOUT. */
        write_Text_file( output_Perl );
    ];

    /*************************************************************************/

    };
    /* package perl5_from_mdpt */

* To keep things simpler, don't have a distinct "value" parser node but
rather just use "expr" instead; when we only want a "value" we can then
either do further constraints on the parse tree from using "expr" or do
constant folding or both as applicable.

* A (Unicode-savvy) parser should treat Greek/etc letters in the same way
as Latin letters, as to where they may appear as barewords and how they are
interpreted, so eg "απΣθ" is parsed the same as "abcd", so these are all
var or foo() and not prefix/infix when barewords.
This should work well with how Greek is commonly used in maths.

* I should really be exploiting the variety of mature parser generators out
there to do the hard work for me, certainly in development, but also to
help any Muldis D implementations.
See http://en.wikipedia.org/wiki/Comparison_of_parser_generators for lists.
Dave Voorhis of TTM forum tersely recommends ANTLR (http://www.antlr.org/).
David Barrett-Lennard of TTM forum recommends Coco/R.
Quoth DBL: "Coco/R validates an LL(1) EBNF quite well, but uses a non-ISO
syntax (which seems to resemble Wirth syntax notation). Section 3.6 of the
Coco/R user manual discusses the errors it is able to identity."
The comparison url suggests I should try ANTLR first, as it seems to
generate parsers in more of the languages I'd be interested in, esp Perl.
Including: C, C++, C#, Java, JavaScript, Objective-C, Perl, Python, Ruby.
Or the ANTLR homepage actually says Perl support is at early prototype.
Also ANTLR is BSD, runs on the JVM, takes EBNF as input, generates lexers,
and it has an IDE, and its algorithm is LL(*).
See http://www.antlr.org/grammar/list for various already written language
grammars, including ISO SQL 2003, other dialects;
I should probably use the PHP grammar example to go by for mine.
http://www.antlr.org/wiki/display/ANTLR3/Quick+Starter+on+Parser+Grammars+-+No+Past+Experience+Required\
has the tutorial for the flavor of EBNF that ANTLR uses.

* Note http://blog.endpoint.com/2011/12/sanitizing-supposed-utf-8-data.html
the Perl modules like IsUTF8, Encoding::FixLatin, Search::Tools::UTF8,
Encode::Detect, Unicode::Tussle.

* TODO: Declare explicit subtypes of String in core:
String.U1,String.U7,String.U8,String.U32,String.U64
so String values can be explicitly marked as such and one can know quickly
whether the String is known to be just bits, ASCII chars, octets, etc
and so treat it more efficiently rather than defaulting to "big" semantics.
The U8 and U32 versions correspond to Perl 5's strings with isutf8 off/on.

* Change all the routines that take "Set of Name"/etc as arguments so they
instead take empty/no-tuple relations as arguments instead, such that the
headings of these relations convey the same set of Name.
- They're both relations anyway, but using headings for this is more
efficient, and make more semantic sense for such as projection or ungroup,
such as to say, "make the result look like this".
- Also make the last part of a relation literal optional for empty
relations; eg, so one can just say "@:{foo,bar,baz}" without the ":{}".
- So eg the prior example replaces "{$foo,$bar,$baz}" as project/etc arg.
- Also, the arg to rename can be a tuple rather than a binary relation.
- So eg one can say "%:{x:y,a:b}" rather than "@:[bef,aft]:{[x,y],[a,b]}".
- But we still need an alternative when we want to define attribute lists
dynamically at runtime; so we prob want "Set of Name"<->"emp rel" mappers.

* The generic reduce op (and meta) should be virtual/overloaded, so syntax
is the same whether the inputs are an Array or Set or Bag etc.

* POSSIBLE PARADIGM SHIFT!
Consider that most of the DB programming that is done on the internet is
extremely trivial, and developers are currently doing it using SQL plus
some other language such as PHP/Perl/Ruby/etc.
- So how much would these developers save if they had a simpler way?
- This could be a very interesting and strong business case for Muldis D.
- While I am keeping my focus on doing databases well as the priority, I
can see the use case also that people may actually want to write simple
apps and have them written entirely in one easy to use language, where say
the web app code and the database code are seamlessly integrated, rather
than the SQL-foo impedance mismatch.  So maybe for the many common simple
web/etc apps, one can just use Muldis D for the entire app, so the likes of
Perl/PHP/Ruby/etc are just cut out of the picture.
- While I designed Muldis D to be possible to write a full app in, I
assumed that most people wouldn't do this and would combine with another
language like Perl/Ruby/etc.  But maybe I assumed wrong.
- Maybe for the many very simple apps, using Muldis D for the whole thing
actually *is* the preferential option.
- I think I'm going to have to start pushing this angle more.  Don't just
compete with SQL, but also work to eat PHP's lunch, without doing PHP's
numerous lameness (including baking the kitchen sink into the language
itself).  So push that I make simple apps simpler, but without losing
actual power or flexibility.
- Of course, Muldis D would absolutely not have web stuff in its core, so
an extension would be needed for the web stuff.

* BUT SEE ALSO
http://www.infoworld.com/d/application-development/introducing-opa-web-dev-language-rule-them-all-172060
as it seems that Opa makes many of the same claims as the above paragraph,
such as one language to do the database and server and client side web,
where the DBMS and web server are built-in.  However, Opa's database is
apparently hierarchical, and it has 1 implementation, written in Ocaml.
A commentor says its like ASP.NET WebForms, that being bad.
Fortunately then that Muldis D doesn't actually concern itself with
generating HTML; Muldis D is only meant to replace the SQL+{PHP/Perl/etc}
pair and not also HTML/etc.
Actually, http://opalang.org/ shows examples that look like PHP when that
is mixed into HTML, and Opa calls itself "the cloud language"; different goals really.
See also http://www.infoworld.com/d/application-development/10-programming-languages-could-shake-it-181548
for multiple language mentions.

* This might have similarities to what I'm doing: https://dsl-platform.com .
At first glance seems to have a lot in common actually.  Also a lot not.

* Refactor, and establish as distinct concepts, Muldis D stores that are
code-only, which we will call "modules", versus those that are primarily
for data, which we will call "depots".
- The "modules" are where any code that should be shared by multiple
databases would live, and include all system-defined code plus all
user-defined code that is packaged and useable like system-defined code.
- The "modules" represent all functionality that is either traditionally
built-in to a DBMS, whether considered part of the Muldis D core or
implementation-specific, or is providable by third-party exten (including
Postgres' "extension" concept) or is conceivably low-level enough that it
would likely be implemented at least partially in a manner external to the
Muldis D environment so its internals aren't visible to the user.
- The "modules" *can* be an external declared dependency of a depot (and
of each other), that is, something which must be present in order to
interpret/use the depot, so like the system itself, that is ostensibly
this is an exception to the rule about depots must be self-contained.
- The modules are written physically differently on disk, at least with a
different primary keyword, so they can't accidentally be used as depots.
- The modules are always readonly to the DBMS.
- Modules are introspectable by system catalog but that is readonly.
- Modules are all individually versioned as per Muldis D itself and all
entities using such must declare the dependency using full version names,
so to ensure the dependent, especially a depot depending on it to be
understood, is interpreted correctly.  So when a depot or module is
declaring its language version, it is declaring a list of all determinant
modules with the same level of detail.
- The "depots" are the only stores that are possibly writable by the DBMS,
when mounted as such, and are what are normally considered "the database",
and are where all user data lives.  These can also contain code and often
do, and only code living here can be updated at runtime.
- In theory or practice a depot could be entirely devoid of code and just
contain data, in which case unless its declared database type is "Database"
it *must* depend on a (readonly) module to define its constraints/etc.
A big advantage of that is to support the common paradigm where data and
code *are* separate, which is frequently the case.  Even more so when
implementing Muldis D over simple data stores without the native concept
of stored user code.
- Maybe we should consider separate code and data files to be the
recommended default way of doing things, and that putting code in data
files be the less common option, so more like typical programming and
further distanced from the paradigm that Muldis D has long had.
If nothing else it would probably be simpler to start implementing Muldis D
if we assume that is the normal way of doing things.
- Ostensibly a primary difference between a module and a depot is that a
module is written as a plain text code file in exactly the same way that
programmers normally write code such as Perl or C or whatever, done
externally to the DBMS in a text editor with direct access to the
filesystem, and then this is considered static in running programs, while a
depot is read and edited from within the DBMS at runtime, using
data-definition statements.
- The point of this new module emphasis is to reduce the problem of having
to duplicate or alias type definitions and such to make multiple depots
work together; for example, often the types shared by a database and an
application might be in a module so each of the latter doesn't need their
own copies.
- Muldis D will provide 2 completely distinct namespaces for modules and
depots, where routines/types in modules are referenced in the manners
typical for programming languages, often fully-qualified, though I suppose
modules, depots can optionally choose to create synonyms within themselves
for such things outside themselves (only to modules) for brevity.
- Modules always declare their own namespaces as per typical programming
languages and they do not vary by or can be controlled by DBMS users.  So
such naming will need to be managed by external convention such as that
the modules declare their own versions with distinct base names that
indicate their namespace, ala Perl modules.  Only depots are under the
user's control for what base namespace they mount into.
- Modules would still support "data" sections, but these would be readonly
and intended for static data resources of nontrivial size, which would be
more efficiently maintained that way than in the "catalog", theoretically.
- For various reasons, including security, with searched namechain
invocations, modules are always searched before depots.
- Lets say for good compromise on various things that various shorthand
syntaxes for operators, such as using non-foo() notation, are only
available for invoking those in modules maybe and that otherwise there is a
different qualification for module and depot thing invocation.  Maybe.
- For data sections of both modules and depots, we could consider something
like an "inclusion node", which may appear anywhere a value node may appear
and pulls in that value from an external source, typically another file,
and then the semantics are as if it were defined inline instead.  This is
kind of like C's include or maybe PHP's include or something or other.
An inclusion node would be formatted like a tuple literal and its details
would be implementation-specific, like specifiers for depot mounts etc, but
in the default case would probably just point to a file in the filesystem.

* Add core support for an analogy to SQL TABLESPACES, with that support
somewhat resembling the depots concept in form in that it has
implementation-specific parameters such as filenames/paths/etc.  The idea
is users can help optimize some things say by storing different parts of a
depot in different file layouts as would be more optimal for that section
of the depot, eg code vs data, and particularly insert-only data versus
frequently updated or deleted data.  But it is important to note that the
depot is still generally the scope of self-definedness or ACID, and so only
DBMSs that support this across multiple tablespaces may use the Muldis D
analogy to said, and our "tablespace" would map directly to theirs.
- With this concept supported, we theoretically could relax certain
requirements that "temporary relvars" need to be defined by existing in
"temporary depots".  Or caches.  This matter requires more thinking.
In fact, multiple DBMSs already seem to use tablespaces for temp stuff,
including Postgres and Oracle.
- We might repurpose the term "subdepot" for the tablespace analogy, or
call it "storage space" or "storage pool" or "pool" etc.
- See also the TTM list thread circa 2011 Nov 16, and "Fragmentation" of
http://www.softwaregems.com.au/Documents/Sybase%20GEM%20Documents/ .
- Note that Postgres tablespaces are actually not disposable; that is,
losing one is significant damage to the whole database that takes some
non-clean effort to recover from; so just saying you will put transient
data in a tablespace on unreliable storage won't actually work
(clearly this is a missing feature in Postgres).

* NEXT PRIORITY...
Rework routine headings/definitions, especially functions:
1.  Remove native concept of optional parameters; this lack can be worked
around in various ways such as with wrappers or multiple routine versions
or polymorphism or priming (partial function application) or system design that emphasizes greater
specialization or fewer parameters.
2.  Make public routine parameter aliases a wrapper thing.  Each routine
has exactly 1 native name for each of its parameters, which is the only
name shared by the routine's internals and its users, and both sides have
their own independent alias namespaces.  For any function that explicitly
declares itself to be symmetric or commutative or associative or whatever
in its routine heading, it *must* use the positional names "0","1",etc as
its native parameter names.  Or for that matter, we could say that in
general any routine intended to be used infix must have exactly 2
parameters, with positional names, and any with more than 2 would not be
used infix.  The routine-internal alias namespace is simply ordinary
expression node aliasing, such as [numerator ::= "0"], or that's what it
is fundamentally, anyway, though syntactic sugar may be provided for this
kind of aliasing.  The routine-external aliasing would involve auto-gen
wrappers, in a manner of speaking, that are like the tuple RENAME operator,
or certainly the semantics are the same.  There would also be syntactic
sugar provided for defining sets of external parameter aliases along with
a routine definition.  To be clear, while synonym aliases for whole
routines don't add routine definitions, parameter aliases do add routines.
3.  Change the normal format for tuple-type and relation-type to look just
like a routine definition, sort of, in that the attribute list is declared
with the same appearance as a parameter list.
For example:
    Dict ::= relation-type (word : Text, definition : Text) key (word)
        where (<boolean-valued-expr>);
4.  Make all functions without exception have exactly 1 actually defined
parameter, which is a tuple-typed positional parameter, so that the
functions' conceptually defined named parameters are actually attributes of
this tuple.  And so all functions will take 1 argument and produce 1
result.  The result type does *not* have to be a tuple (this is important)
and is typically the normal result type the function otherwise has.  We now
will always use a tuple type definition to define a function's fundamental
named parameter list, meaning its parameter names and their declared types.
But an added benefit of the change is that we can define what
*combinations* of named arguments are valid for the function in order for
the function to produce a normal result; a predicate or contract for the
combined argument list is now defined quasi-declaratively as a
boolean-resulting expression in the tuple type constraint.
Theoretically, the parameters' tuple type can now just define all the
allowed public-side aliases itself, or any other variations of parameters
including optional or what-have-you.  But in practice for most functions
we will still just want exactly 1 variant defined, so that the body of the
function only has to deal with 1 variant.
In practice there will exist an automatic RENAME-alike operation that maps
the tuple of a caller with 1 set of argument names to the names that the
called function actually wants, and this would probably be auto-generated
per spot of invoking code for efficiency.
Using tuples like this would enhance polymorphism, as now a lot of the same
logic can be shared, and in particular, dispatch would always be determined
using a single actual tuple parameter; the automatic generated code that
dispatches a virtual call can simply invoke "isa" once per candidate to
find an implementer.  So then the logic becomes a lot more like the
multi-sub/method dispatch logic of Perl 6, wherein variants of the same
virtual routine can now have different numbers of or names for parameters.
No change to the empty-string-named expr node; it is the function-result.
Inside the routine body, the tuple argument as a whole is like an inner
expr node, in that one can optionally name it (but using : rather than ::=)
and if they don't then a name will be automatically generated, and then for
all practical purposes it would not be explicitly referenceable; regardless
of that naming, all of the tuple's attributes can be referred to using the
special leading-dot syntax, such as ".numerator"; the leading dot syntax is
no longer a shorthand into the "0" or "topic" argument.
Examples:
    function (ResType <-- ArgsType) : ...;
    function (ResType <-- args : ArgsType) : ...;
    function (ResType <-- tuple-type (x : Text, y : Int) where ...) : ...;
    function (ResType <-- x : Text, y : Int) : ...;
Still todo, how to distinguish 2nd from a single named arg in 4th?
Still todo, sugar for auto-declaring auto-nodes from args, eg so we can
just refer to "x" and "y", though the 3rd/4th syntax may just do that
because we declared the tuple type inline.
Tuple selectors need to have special expr node syntax to avoid chicken-egg.
Tuple attribute accessors don't need special syntax, strictly speaking,
because functions don't have to return tuples.
5.  Making procedures also have exactly 1 tuple-typed positional parameter
should theoretically also work, thanks to pseudo-variables, and because
even subject-to-update parameters are always passed values,
a type constraint or virtual dispatcher can include those too.
However, in the general case, we would expect procedures to have *two*
predicates, because there may be values allowed for output that aren't
allowed for input, and vice-versa, in theory.
Related issues are still marking params/args as subject-to-update or not.
Or we could just conceptualize that the single procedure tuple parameter is
*always* subject to update fundamentally, as a function's is read-only, and
any further markings on its attributes, probably only happening if declared
inline, the "&" or absense of said would be more for optimization and
adding of constraints that variables or non-variable exprs are allowed.
Where the 1st and 2nd formats are used, there can also be a separate
declaration list to indicate ro/rw or alias-exprs etc.
I would expect global-params to remain separate from the parameters-tuple.
6.  We should be able to retain existing signature/etc syntax a large part.
7.  See also what Perl 6 does about routine parameters.

* Do away with TREAT as seems to be useless now.  See TTM chat c2011.06.11.

* Recheck my terminology.  Perhaps "consume" is better than "compose" when
referring to what one does with roles/mixins.  See Perl 5.15.3 perlootut.

* Perl 5.16.0 introduces the __SUB__ token, which is a ref to the current
subroutine, so Perl now has a direct counterpart to rtn(), which is nice.

* Bring back, with enhancements, the "transition constraint" support
removed in version 0.140.0 as I changed my mind again on its utility.
As per my comments to the TTM list on 2011 Oct 16, I consider the primary
benefit of transition constraints to protect the explicitly recorded
history of auditing databases from being changed, eg such as with a
"you may only insert" transition constraint.  The transition constraints
can also be used on their own for simpler business rules.
So this restored feature expressly is pure functional / declarative and is
*not* replaced by stimulus-response rules.

* NEXT PRIORITY...
Make the system catalog into something much closer to a *concrete* syntax
tree-like-thing.  See various following TODO items for details.
Mostly do this as its own spec release with minimal dialect/etc/other chgs,
that is, dialect changes to fill in new slots not needed, but anything the
catalog would break could be altered.
Includes:
1.  Get rid of the "scm_" prefix for cat attrs, since we're more formally
storing concrete syntax anyway, so treat those as normally significant.
Also update or remove/move SCM section in Basics.pod since we aren't really
differentiating between "metadata" and normal code anymore.
2.  Add Bool-typed scm_foo cat attr everywhere we have a "name" attr that
says whether the name is user-specified or compiler-generated.  See below.
3.  Refactor the scm_vis_ord and scm_comment attrs as described below.
4.  Refactor the FooSelExprNodeSet types to merge most of them into one
that just stores an ordered list of Name attrs; eg, [rank|vis_ord]:name
pairs; the existing ArySel would be the closest fit.
5.  Add attr for func-invo etc that specifies whether prefix or infix or
postcircumfix etc form was used, where applicable.
So ultimately distinctions of func-invo-alt-syn are recorded in catalog.
6.  Likewise for := op, to say if "x :=foo y" or "x := x foo y" used,
and so on concerning meta-ops, maybe.
7.  Add attr to other expression kinds saying which forms were used,
eg for if-then-else, whether that or ??!! used.  Or for relation literals,
whether ordered or named form used.

* Maybe add Pair and Dict generator types while we're at it; Pair and Dict
have the same binary heading like "(key : Universal, value : Universal)"
where Pair is the tuple type and Dict is the relation type; and "key" is
also defined to be a unary key.
Consider redefining Array to be a subtype of Dict where the key is
a nonnegative integer and its values are dense; in other words, the new
Array is the same as the old one but renaming "index" to "key".
Then you can use a lot of the same operators on Array and Dict like Perl 6.
Theoretically we could also make Bag a subtype of Dict where its "value"
is a positive integer; so then "key" is its payload rather than "value";
this then would also be like Perl 6 and it might be worth it.
But if we do this then we'll have to relook at the set membership/etc ops.
Maybe while we're at it, rename Set's attribute to "key" for consistency.
- Note: The Postgres hstore type may be relevant for a nested one of these,
where I think the keys and values are all text.
- Note: The "hyper" meta-op would be particularly for Dict then, such that
when we say "foo >>op<< bar", then foo/bar are both Dict and the result is
also a Dict, where "key" match the "key" of both inputs and "value" is the
result of applying "op" pairwise to the "value" of foo/bar.  This would
just work for Array and Bag ostensibly.  What to do when
"foo^{key} != bar^{key}" is an open question though; the simplest answer is
that the result just has the set-intersection of the input keys, like a
join would produce; or we might want to define some "outer" version?.

* NEXT PRIORITY...
Reduce the general options for concrete value literals to have just the
simple ones.  For any given "x:y:z", remove all "y" but for Scalar where it
is necessary; people can wrap a literal in an explicit TREAT assertion/etc
otherwise if they really want to.  Also remove all "x" where possible, so
just the plain "z" is the only option, in general.  So then, to write an
integer/rat/text literal, the only option then is to say "42/3.25/'hello'";
you can't say "Int:42/Rat:3.25/Text:'hello'" any more.  So cascade these
simplifications, and we also free up the ":" mostly for other uses.  This
particularly applies to Plain_Text, but we'll also simply the Perl-STDs where
possible, which is easier in Perl 6.
Unlike "[$|%|@]:..." for generic value literals, only %|@ without the colon
are in use, I believe, as prefix operators, meaning cast-tuple-as-relation
and vice-versa, but there is no colon-less $ prefix in use nor does it make
sense for any similar purpose.
So, use $ for name literals, that is, "$foo" means "Name:foo"
and then say "$$foo" means "NameChain:foo" (doubleup for chain).
And then we're a long way towards being able to ditch the postcircumfix
syntaxes for eg projection since, say "r keep {$foo,$bar}" is terse enough.
Then come up with something for rename, maybe a set of name-pair literals.
After this, keep postcircumfixes rare and common, like for array elements.
In fact, we could just say foo[x] and bar{x} are then array/dict lookups.
Or the dotted forms are elem lookups and no dots are for slices, like Perl.
And then "<expr>.attr" is then its own thing rather than being a shorthand
for "<expr>.{attr}" although it may still be a function shorthand.

* Consider having unary virtual functions for some common kinds of casts,
such as "?" for boolean, "+" for number, "~" for char str.  This terse
syntax would always deal in just base-10 for numbers-with-char.  As for
whether to produce an Integer or Rational or etc from a string, the parsing
rules would be like for Plain_Text, eg, presence or absense of a radix point
or a division/fraction slash, etc.
Casting to the same type produces the same value as the input.
The functions might be transitive, or whatever the term, so eg saying
"~somebool" produces the same result as "~+somebool" and so on.
Likely booleans would translate to integer 0|1 or string '0','1'.
Likewise in reverse, any number equal to zero, or any string that casts as
a number equal to zero, would become bool false, and otherwise bool true,
at least speaking for strings that successfully numify.
So 0 and 0.0 and '0' and '0.0' are false, and other numifyable are true.
Attempts to numify a string that isn't a number will throw an exception.
Attempts to boolify any string will succeed, where the above zeros plus
the empty string are false, and other strings are true; to be specific, any
string that would successfully numify would use numeric rules for truth,
and any string that doesn't successfully numify would be true unless empty.
ON THE OTHER HAND, it would probably be much better and more predictable
and extensible to forget about transitivity, and so, boolifying a string
will only produce false for the empty string and true for all other strings
and to get the '0'=false behavior you have to say ?+str; MUCH BETTER.
Unlike Perl 6, the prefix !foo will *not* be system-overloaded to mean
!?foo as doing so would likely introd accidental logic errors of implicit
casting when one considers ! is widely used when just bool inputs expected.
Similarly, unlike Perl 6, no other ops will implicitly cast like infix ~,+.
If this is adopted, "so" will therefore be virt rather than a "not" mirror,
though for the boolean composer it becomes the same as otherwise.
ON THE OTHER HAND, since there is so little standardization and wildly
different expectations on what strings are considered true or false, it
would seem best for strings to *not* provide system-defined terse bool
casting at all, and instead force people to be explicit, which would
typically mean either going by way of a number, either from a numify cast
of the string or a length-test of the string, or some explicit boolean test
like comparing the string against a list of values.  Similarly, a boolify
of many other things like collection values is best done by way of some
intermediary like a number such as a length or elements count test.
So eg the so/not-empty prefix for a relation/array is then ?#r or !#r for
consistency with "? #r" or "! ? #r", and #?r or #!r is probably invalid.

* Grammar spec related to separating/trailing chars like ',' or ';' could
look like this:
  <foolist> ::=
    <open>
      [<item>? <sep>]* <item>?
    <close>
... which says every item may have a trailing separator, and every item
must have a trailing separator except the last/only one, and any item may
have a leading separator or separators may appear several in a row; the
semantics are that extra separators do not introd any implicit extra items.
The above would be for a 0..n list; here is a 1..n list:
  <foolist> ::=
    <open>
      [[<item>? <sep>]+ | <item>]
    <close>
... but there are probably cleaner versions of both.

* The SQL-92 Standard CREATE ASSERTION is like a generalized db-level CHECK
constraint but few SQL DBMSs support it.  Note this in the spec.  RDB from
DEC had it, but they got bought by Oracle when DEC went out of business.

* FYI, PgOpenCL exists, run intensive db tasks in the GPU.

* See http://vitessedata.com for beta (2014 Oct 28) of Postgres fork that
uses LLVM as backend for better performance of some things.

* See http://people.umass.edu/klement/russell-imp.html for free restored
copy 1920ish "Bertrand Russell: Introduction to Mathematical Philosophy".

* Note that Perl module PQL::Cache released at the end of 2014, which has
some semblance to Set::Relation but has DBIx::Class like query API and it
intentionally has no non-core Perl dependencies.

* ALSO...
Get rid of the Set.new() options in Perl 6 and generally update the
Perl-STDs to use just Array/Seq/arrayref and mostly not set/bag/hash/etc,
partly for code brevity but particularly to preserve the visual order of
elements from source to catalog and back again.  Also add scm_vis_ord to
the catalog or change some catalog types to record order.

* Note that Perl 5.15.4+ fleshes out / completes the support for having
Unicode in identifiers, to treat them properly, such as not ignore the
UTF-8 flag; it also fleshes out support for the null character "\0" so that
eg regexes with them work properly.  See perl5154delta.

* Also add to Perl5-STD explicit support for some more Perl modules whose
objects we would implicitly treat as built-in scalar values.  We already
have BigInt, BigRat etc, but add Ingy's "boolean" (which is expressly
intended for cross-language interchange in its POD) and Juerd's "BLOB".

* Maybe also but probably not yet be concerned w comments in Perl-STD/etc.

* NEXT PRIORITY...
Reformat all declarations, materials/subdepots particularly, to be of the
format "name ::= kind ..." rather than "kind name ...".  Similarly, we may
be able to just nix the "subdepot" keyword so a subdepot is then declared
as just "foo ::= {...}", and a material as "foo ::= kind ...".
So the name on the left of the ::= is no longer part of the material node
itself but rather is part of the larger thing into which the material node
is composed; the material node is now just eg ['function', <payload>].
Material nodes themselves just declare anonymous entities.

* Rename the "subdepot" concept to "schema", even if this term is broad
enough to include what Oracle calls "package" and not just SQL "schema".

* Example of function in Perl with context ...
The FunctionSet tuple:
    ['cube','opt comment of FunctionSet',<Function>]
The Function tuple:
    ['opt comment of Function',<heading>,<body>]
... and so on.  The comments are first so they're like leading comments
as is common with whole-routine definitions and such, and they tend to be
terser besides.

* ALSO...
Change function bodies from {...} to (...).  And make "..." expr/stmt kind.

* Likewise make a code comment a stmt/expr kind or otherwise provide for
specifying where it goes visually in code in a statement position.
Consider reworking scm_vis_ord to be external for some things it describes,
eg mapping an order to a declared name.  In fact, if this is done, then it
becomes much easier to add/remove/reorder code pieces because their
sequence numbers are stored separately and so code diffs on the system
catalog itself may not show much in the way of spurrious diffs, especially
if the mapping is simply an array_of.Name.
Consider pulling out code comments in a similar fashion, just putting them
as their own named (as stmts/etc are named) code bits, which are then
associated by name with other code bits, and are listed in the vis-ord too.
In fact, then where comments are physically visible and what things they
are semantically connected to are then not joined at the hip.
The comment names are optionally user-specifiable too, like with statemnts.
For example:
    cmt_on_x ::= C:'This roxors!';
... or:
    comment cmt_on_x ::= C:'This roxors!';
... and other details still to fill in like saying what it applies to.
Maybe a new infix-op-bind-like syntax will fit the bill for association.
Also make blank code lines or visual dividing lines recordable in syscat.
See also Postgres "COMMENT ON ..." syntax.

* Note: Postgres $$...$$ delimiters are actually general string delimiters
and can be used anywhere '...' are, not just function definitions.  They
also generalize to heredocs, eg, $foo$...$foo$ so maybe Plain_Text should
support this kind of thing too, maybe.  For that matter, I should have a
function that takes a string as input and outputs some delimiter suitable
for quoting it without escapes because it doesn't occur in the string.
This, as well as other escaping functions, would be used by a code gen.

* Don't use any quote-like characters for delimiting code comments after
all, but instead use nonidentical multi-symbolic-character delimiting
tokens, such as a "/* */" pair as in C/etc.  The tokens would be ones not
valid as normal operators, and they would be visually "heavy" rather than
"light" such as quote-like string delimiters are.  To make it easier to
comment-out code blocks that may already contain comments or have literal
strings that contain comment delimiters, the comment delimiters may be
extended to an arbitrary length using repetition, where both ends must
match.  Perl 6 is used as inspiration for this.  We would presumably use a
bracketing character for the extender, for example "/*{ }*/" or "/*{{ }}*/"
etc.  This means there is NO ESCAPING in code comments for the comments.
On the other hand, maybe some certain escaping is still needed such as for
whitespace like linebreaks and indentation, as with text/etc literals, to
control exactly what is captured, or we have a version of comment for each.
Apparently both Tutorial D and SQL support /* */ block comments too.
All Muldis D comments are block comments on purpose; there are no line
comments, because of a desire to make it line-ending-chars-generic.

* Invent some lightweight syntax for the general case of infix operators,
whose use is optional for system-defined infixes, so users can define infix
operators or invoke their routines infix, and the parser will interpret the
infix-using code correctly in a context-free environment where the parser
doesn't have access to other user-defined-stuff definitions, mainly telling
routines apart from variables, since the latter don't have special markings
on purpose.  This syntax will need to be something that users or Muldis D
are unlikely to want to use for some other purpose.  By "lightweight" we
mean something that is visually small but also short to type.
One possibly best choice is the GRAVE ACCENT; for example:
    x `foo` y `bar` z
... or:
    [`foo`] ...
... for user-def reduction, which couldn't be "[foo()]" for arry confusion.
But then we'd need something else for code comments, though code
comments are probably much more amenable to having double-delimiters;
or better yet, lets use single-quotes for comments now, same as for Text or
Blob, and have some kind of prefix like with a Blob to know its a Comment.
We could similarly use that for user-defined prefix operators sans parens,
where context (terms-in-a-row/etc) could differentiate from infix, although
the value of this is lower relative to foo() compared with supp infix.
We would probably never support unary postfix or circumfix operators,
save the few built-in special syntaxes.
Lets just say that there is no part of the catalog for declaring routines
as infix but that rather any user-defined (or system-defined routine can be
used both "foo(x,y) and "x `foo` y" iff it has exactly 2 positional
parameters that are mandatory (names are "0" and "1") and so any other,
optional, parameters may only be caller-specified in the "foo(x,y)" form.
The system catalog would specify if that a routine *invocation* is foo() or
infix etc but the routine *definition* knows no difference; both forms use
exactly the same choices of routine names, whether alpha or symbolic.

* A tangent to the above is that where appropriate we can take any function
of 3+ parameters which conceivably would be made easier to use in a
ternary+ infix format and make a set of dyadic functions that break it
down, kind of like how the creation of a distinct "assuming" function
allowed numerous 3-arg functions to generalize into a pair of 2-arg.

* Note, apparently Haskell has no prefix operators except for unary minus,
and normally all of its infix operators are symbolic where normally all
alpha operators must be used in "foo()" notation.  Also, you can take a
foo() operator and use it infix using backquotes, eg "x `foo` y", meaning
my thought of doing this has a precedent.  Haskell says you can take a
symbolic infix and make it prefix using parens, eg, "x + y" -> "(+) x y".

* Plain_Text Syntax/parsing cluster:
- A general rule is, the parser has zero knowledge of what specific
operators or vars/etc exist, and must be able to derive a concrete syntax
tree using just basic grammatical knowledge, such as knowing the difference
between whitespace and alphanum and symbolic and quoting and bracketing
characters, and so the parsing rules are based on those restrictions.
In particular, the behavior of a parser or how a piece of code is parsed
does not vary depending on what user-defined entities are declared at the
time, which should aid on security and predictability and simplicity.
There are only a small amount of disambiguating rules that say certain
tokens are system-defined operators, so otherwise no knowledge.
The hardest work then, in the face of Unicode, is specifically identifying
what characters count as alpha or symbolic; eg what are Greek letters?
Generally speaking we would whitelist; any characters or groups we don't
whitelist into a particular category are disallowed from appearing outside
of a quoted context.
- Remove dash from bareword ident, leaving just alphanum and underscore;
people can quote names with dashes when they want them.
- There are no postfix operators; eg, ++ is prefix.
- Namespace-qualified/dotty operator calls must be in foo() syntax; plain
prefix or infix operator calls must be unqualified.
- Whitespace affects parsing.  Any consecutive 2 alphanum or 2 symbolic
tokens must have intervening whitespace.  Quoting '"` or bracketing (){}[]
characters count as neither alphanum nor symbolic in general.  Whitespace
may be omitted between any 2 of these distinct things in general: alphanum
tokens, symbolic tokens, quoting or bracketing characters.
- Default to treating any bareword run of 1+ alphanums as a var name or
foo(), treat any bareword run of 1+ symbolics as a non-foo() operator.
- All bareword symbolic tokens are plain infix/prefix op calls.
- All grave-accent-delimited (`) tokens are plain infix/prefix op calls.
- All double-quoted (") tokens are var/etc references or foo() calls.
- All single-quoted (') tokens are string value literals.
- All namespace-qualified/dotty tokens are var/etc refs for foo().
- All tokens followed by an open-paren with no ws between are foo() calls
if the token is bareword alphanum and a plain prefix op if symbolic.
- Any code enclosed by a pair of bracketing chars is treated as a var/etc,
except when they contain only a symbolic token and no whitespace, in which
case they instead are taken as an extension of said symbolic token, eg [+].
- When 2 or more plain op calls appear consecutively, all but the first are
taken as prefix ops; the first op is taken as an infix op if it follows a
var/etc and a prefix op if it doesn't.  Eg "x + -y".
- When a symbolic token appears between 2 alphanum tokens, the symbolic
token is taken as a plain prefix op if there is no whitespace between it
and the following alphanum and there is whitespace between it and the
leading alphanum, while it is treated as plain infix if there either is or
is not whitespace on both sides, or there is whitespace just following it.
- When disambiguation isn't present, sequences of bareword alphanums are
interpreted as alternations of var/etc and plain infix op calls, always
ending with the former, like "var op var op var" or "op var op var".
- Certain bareword tokens are special-cased to always be interpreted as
prefix ops, namely {not,so} (there might be more).
- All operators with more than 2 parameters must be called in foo() syntax.
- All prefix ops have 1 param named "0", all infix just 2, named {"0","1"}.
- All bareword prefix ops are symbolic except for a small number of special
cases such as "not" and "so"; all other bareword alphanum ops are infix.
- The only infix syntax taking more than 2 inputs is special syntax with
its own kinds of parse nodes, not op calls, such as ??!! and if-then-else.

* We have just a small number of precedence levels, here from highest to
lowest, loosely speaking:
  - base literals or selectors or delimited whatevers or foo() etc
    - for foo() there must be zero whitespace (or unspace) before opening paren
    - includes "." of tup.attr etc since "." changes how its RHS is parsed, its not a normal infix
  - why-binding infix ::?= (binds tighter than ::=)
  - what-expr-binding infix ::= (binds to innermost expression, use parens to force looser)
  - general symbolic prefix (only bareword symbolic)
  - general symbolic infix (only bareword symbolic)
  - general alpha/other prefix (bareword alpha or tilde-quoted)
  - general alpha/other infix (bareword alpha or tilde-quoted)
  - short-circuiting / ??!! / if-then-else / given-when-def
  - assignment := (both base statement and any derived meta-op eg :=+)
  - statement-binding infix ::&= (because plain ::= binds to lvalue expr if used there)
Note that tilde-quoting should have an impact on precedence, for principle
of least surprise, otherwise users have to guess if something is symbolic
or not to determine precedence.  Also security implications
for lookalike graphemes.

* Pegex grammar should work at some level midway between a pure tokenizer
and the resulting AST.  It should form trees where easy to do without
non-trivial lookahead.  In particular, sequences of barewords and etc where
interpretation of what is a pre/infix op and what is a var/expr/etc depends
on the number of barewords and what they're next to, should be output as a
flat list and no attempt to form a tree in the grammar.  Also, the Pegex
grammar perhaps should capture everything, including ws, just in case.

* IDEA: Add a concept kind of like a distinct depot/package catalog but it
is system-generated and not directly editable by the user; in here would
live copies of all of the canonical type definitions, especially scalar
types, from both the system and user namespaces.  The copies in this
package are the ones cited by a ScalarWP value in the low-level type system
as being the scalar type name.  When there is just one normal copy of the
type def, it is more of a redirection.  When multiple user depots declare a
copy of a type and then the mounting commands further specify that they are
to be considered the same data types, then just a single copy exists in the
new sys-gen package for both, rather than one for each.  This has various
benefits like saving a chicken and egg problem concerning say transient
values in the system when various depots that may declare types for it can
come and go.  It means the low level type system type name for an
(immutable) ScalarWP value never has to be changed for the life of the DBMS
process no matter what happens with the depots user types are declared in
or as copies come and go.  For that matter, we can just identify this copy
of the type name with a plain integer, this separately being mapped to
namechains for the various normal copies it is aliased with, so we get
memory efficiency too.  This all probably has wider design implications or
can inspire other changes.

* Consider the provision of a limited search path functionality for
user-defined functions and types so that they could be invoked tersely or
unqualified like builtins, particularly so reasonable infix syntax or
singleton types could be supported.  The fully-qualified invocation of such
a floating name would have another special name token, in the same way that
"par" and "nlx" etc are special.  Maybe "shp" (SearcH Path).  So, in the
system catalog, all unqualified names "foo" become explicit shp-prefixed
names "shp.foo", which includes references to all system-defined routines
as typically used.  On a tangent, we can revise how NameChain are expanded
in the system catalog, due to wanting to make it as close to the user's raw
syntax as possible, so eg not expanding unqualified to sys.*; if necessary
for ease of scanning the catalog, we might store 2 versions of a namechain
together, where one is expanded and the other isn't.  A "shp.foo" would be
interpreted as follows: 1. first all system-defined possibilities would be
explored and exhausted; 2. then user-defined possibilities that are
siblings of the invoker or siblings of ancestors from closest to farthest,
and then failure.  To be specific, in a given "shp.foo.bar.baz", just the
"foo" is looked for as siblings of ancestors, and then the first one that
is found would then be dug into as usual, to succeed or fail, same as if
one had said "par" instead.
A "shp" can be included along with other special chain
elements so to further customize the search path so that say #2 can be
followed relative to a specific place in the namespace rather than from
where it is actually invoked; for example "fed.lib.db.foo.bar.shp.baz" but
that this example wouldn't actually work, so details still to figure out;
on the other hand, we could say that this last feature would be unnecessary
because the existing APMaterialNCSelExprNode feature would make stuff work.
Or possibly APMaterialNCSelExprNode/etc could be employed with an expanded
role to translate shp.foo to abs-paths which then could be used as usual.
And so, the parser doesn't have to resolve paths so much from unqualified
to qualified, this functionality being pushed back, but instead the parser
still has to resolve what a reference is to, whether a function or a type
or a data-entity etc so users will still have to provide enough clarity in
their syntax to disambiguate this.

* Note that Postgres and MySQL both support an UPDATE statement extension
where you can mention multiple tables besides those being updated, where
semantics are that all of the tables are joined with the one being updated
and so you can use data from other tables in the SET clause.
This is a *very* useful feature and Muldis D should have an analogy,
or this feature would be helpful in implementing Muldis D features
like certain kinds of relational assignment involving self joined with foo.
As of Postgres 9.1 the UPDATE also supports the WITH clause.

* Note, http://facility9.com/2011/12/ten-reasons-postgresql-is-better-than-sql-server/
which gives some interesting details or explanations of Postgres (per 9.1)
features, including use cases for writeable CTEs, and compares with SQL
Server; a response also lists some of Postgres' weaknesses in comparison.
Also, re unlogged tables, and serializable transactions.

* Note, see http://en.wikipedia.org/wiki/Vector_clock and such things.

* Note that in Postgres, db encoding SQL_ASCII just means text has no
encoding and Postgres just treats it as bytes, so any values can be stored.
Other encoding choices are basically constraints to ensure your
data is valid according to that encoding.

* On 2011 Oct 17 I made a comment on Andrew Dunstan's blog that in the
context of foreign keys, making changes to parent and child
tables simultaneously in a single statement is much better than trying to
determine correct order of operations.  In response Andrew said "That might
be possible in 9.1 with writeable CTEs, but I happen to be on 9.0 with this
client, and the constraints are not deferrable."

* Note that Postgres databases default to SQL_ASCII (7-bit) encoding when
an explicit encoding (such as UTF8) is not specified upon their creation.
"The SQL_ASCII setting behaves considerably differently from the other
settings. When the server character set is SQL_ASCII, the server interprets
byte values 0-127 according to the ASCII standard, while byte values
128-255 are taken as uninterpreted characters. No encoding conversion will
be done when the setting is SQL_ASCII. Thus, this setting is not so much a
declaration that a specific encoding is in use, as a declaration of
ignorance about the encoding. In most cases, if you are working with any
non-ASCII data, it is unwise to use the SQL_ASCII setting because
PostgreSQL will be unable to help you by converting or validating non-ASCII
characters."

* ALSO...
Use colons to separate any kind of heading/body pairs, both materials and
values.  Take Relation now "@:[...]:{...}" as example to follow.
Also, routines now "function (...): (...)" or "updater (...): {...}" or
"procedure (...): [...]"; this for routines is inspired by Python.  This
then opens the door for routine body bounding chars to be opt sometimes,
and makes clearer where a heading ends and a body starts when there are
various extra heading clauses such as is-x or implements x.  Also consider
using ":" in other places where pairs are, maybe freeing up => for
something more specific; eg Python uses ":" in dicts rather than =>; or do
the opposite; keep "=>" for named param/attr/etc lists and use the ":" for
things like Bag literals or generic dicts that are binary relations ... use
one for atvl:atvl (bags/dicts), other for atnm:atvl (tuples, arg-lists).
Also consider using "::" for something, maybe type conversion, as Pg does.
Keep "::=" as for explicitly associating names with what they are naming.
DESPITE WHAT MNEUMONICS SAYS, lets use the : for name/name and value/value
pairs within delimiters also (unless <- still better for rename) and so
maybe the only place => is used is as a binary infix op to construct Pair
tuples, same as .. is an infix op to construct intervals maybe I guess.

* Have ";" as separator (opt lead or term) for both statements/vars/exprs
etc as well as whole materials.  This also comes together nicely for the
simpler routines that don't need to have bounders because they are just
single statements or expressions, for example:
    cube ::= function (Int <-- topic : Int) :
        topic ^ 3;
... and that's it.

* Remove the "var" and "attr" keywords or make them optional noisewords.
Simply having "foo : bar" in a procedure statement position should be
enough to know it is a variable declaration.
Likewise, "foo : bar" in a sca/tup/rel typedef can be known an attr def.
Then, other things can gain optional noisewords, such as "result" before
the type in function sigs, or "param" before a param in routine sigs,
or "expr" or "stmt" optionally before those things in a routine, etc.

* Update system catalog, if necessary, to support specifying where a named
expression, or a variable declaration, lives visually in a statement list.

* Consider dropping the special support for dot-accessors as their own
expression node kinds in the system catalog, meaning AccExprNodeSet, and
code can just use an ordinary function invocation on Tuple.attr() instead;
or that node kind can be downgraded to just be doing aliasing, like when
you write "a ::= b ::= c" (target is then just a Name, not a NameChain).
This change would improve internal language consistency.
As part of this change, the formal syntax "t.{x}" goes away so just "t.x"
remains.  Then "." becomes an ordinary dyadic infix operator in the context
of referring to a data entity, same as "."(t,x) where "." synonyms "attr".
But when referring to a routine/type the whole dotted name is a NameChain.
But if "." is just an infix operator, then "t.x" means
"attr(<value-of-t>,<value-of-x>)" so things aren't actually that simple.
Regardless, if there may be separate "." for relations or scalars then
"t.x" may actually point to a virtual op, only func forms disambiguate.

* NEXT PRIORITY...
Add support for material and parameter synonyms.  And change what params
any positional arguments implicitly go with from topic|other to 0|1|...
But don't actually change any routines/params until later, except adding
0|1 to all topic|other.

* Update the array-specific postcircumfix concrete syntaxes to make them
more generic such that the array index/es (what's inside the "[]") may be
any arbitrary value expression rather than having to be an integer or
interval literal.  But if nothing else changes, this means the slice will
have to be spelled like "ary[{x..y}]" rather than "ary[x..y]", but
individual element access like "ary.[x]" will still work.  But now you can
actually have the x,y variables rather than those having to be constants.

* Consider taking a more Perl 6 like approach by turning ".." and its 3
friends into infix dyadic functions that take endpoint values and result in
interval values.  Then the surrounding curly braces are no longer needed,
and you can once again say "ary[x..y]".  Note that if ppl still want/need
delimiters for an interval, they can always use parens, like "(x..y)".
If we also redefine an MPInterval to be a set_of.SPInterval, then any
{x..y} would unambiguously mean either a set or MPInterval, but we may then
lose the shorthand "x" meaning "x..x", but this could be ok tradeoff.

* Consider also making the likes of "," and "=>" into dyadic functions
along the lines of Perl 6, though this would have further consequences.

* Add boolean monadic function "so"/"?" which returns its argument;
it serves as a useful noiseword in code, helping parity with "not"/"!".

* Have a Boolish mixin-union type which Boolean or Bit etc compose.
By having "so" and "not" in this union type, users can compose Boolish
into ostensibly less-Boolean-like types like numbers or strings and so they
can define implementations for "so" and "not" that are like the Perl
operators when one treats numbers or strings like booleans.
But the system-defined numeric and string types *won't* compose Boolish/etc
as we prefer stronger typing or more explicitness by default.
The details of this will require more thought, maybe more mixin types.

* Account for that we can't generally have a virtual N-adic operator that
accepts the empty set as input, where that is implemented by type-specific
N-adic operators that accept the empty set as input, because it wouldn't
know which type-specific N-adic to dispatch to for the empty set.
Instead, make the type-specific N-adic virtual with 2 implementers, one
taking just the empty set and one taking nonempty sets; the type-specific
0..N virtual will not implement the type-generic 0..N; rather the
type-specific 1..N will implement both 0..N virtuals directly; the
type-specific 0 will only implement the type-specific 0..N; there would
also be, as applicable, a type-generic 0 implementing just the generic
0..N.  Eg, so we have just Num-0..N, Num-0, Int-0..N, Int-0, Int-1..N at
least for ops that have identity values, and no -0 where there aren't.

* Consider creating an analogy to virtual routines that is to existing
virtual routines what domain-union types are to mixin-union types.  That
is, define a kind of virtual that declares what other routines implement
it, and so is not user-extensible.  This is essentially an alternative way
for users to write wrappers for related routines that dispatch on argument
types.  An example is we can have separate "ungroup" and "unwrap" functions
that take only nonempty relations vs those that take empty ones too; the
nonempty-only ones don't need the extra parameter to say what attributes to
extend with when the relation is empty.  Or maybe those should be normal
functions and better examples for the new kind of virtual will come around.

* Don't worry about declaring identity values somehow attached to dyadic
function definitions in the catalog; instead, use the virtuals mechanism
we have to just declare the triple {0..N,0,1..N}, um, or something.

* Make all system-defined functions generally return special values on
failure rather than throw exceptions.  For example, make division return
the NaN.DivByZero singleton and so on.  But functions whose whole role is
to assert, such as treat(), would still throw actual exceptions, and some
other kinds of problems may be better suited to thrown exceptions.  This
gives users the choice to either explicitly accept such situations or not.
If users don't handle such situations, then often-times they will
immediately get a type constraint violation exception (which is an actual
exception, not a special value), and the description of the exception
message is still informative enough, eg
"Nan.DivByZero isn't a value of type Integer".  This works because typical
system-defined type-specific functions will not accept special values as
input, even when they might return them, so in nested expressions we still
get exceptions in about the same places for the same reasons.
But now the Muldis D analogy of "@foo = map { $_.x / $.y } @ints" in Perl
won't itself throw an exception, if @foo is a Universal-array, but it will
throw an exception if @foo is an just-Integer-array.
One could say that a significant portion of the exception throw/catch
system has been made redundant by the type system.  For example, a routine
signature formally declares exception-like conditions as its result types.
I suppose one might say this could lead to action-at-a-distance problems
such as what exceptions are meant to help prevent in the first place,
but when code is written with fairly narrow declared types, then errors
would not tend to get very far before detection, I would think.
Now subject-to-update parameters of procedures can be tricker, because what
if the param decl type is Text|IOErr but the variable argument is a
just-Text var?  Is that something we would expect to fail at compile time
or just optionally at runtime only if a IOErr is to be returned?
Make sure *don't* call the special values "exceptions"; use something else?
See also the list of IEEE float special values, and Mathematica/etc such as
http://mathworld.wolfram.com/Indeterminate.html / etc.

* Support at least a base level of controlled override-overloading with
virtual operators, meaning where multiple impls overlap in their domains.
Normally, if several implementation signatures match the arguments to the
virtual, it is undefined which one is called, and in general it is onerous
to determine which one is more "specific" than another to pick that.
The base proposal is that the virtual operator itself can name, or be its
own, default implementor, where this default is invoked if all of the other
implementors don't match the arguments, rather than there being a type
constraint violation.  Actually, best for a virtual to not be its own
default, so users are able to always invoke the default implementation
directly without worry of it being overridden by something.
A natural extension to this is that in any cases where an implementer of a
virtual is itself a virtual, it can do likewise, providing a default.
In this way, on an operator-by-operator basis, we can support a hierarchy
of sorts like in a multiple-inheritence OO system.
This idea still needs thought to flesh out details of course.

* Demote the numeric operators that are more statistics-oriented from the
language core into a new Statistics extension or some such.  Specifically
this means these 5 in [Numeric|Rational|Integer]: range, frac_mean, median,
frac_mean_of_median, mode; and these 2 in Integer: whole_mean,
whole_mean_of_median.  Also, this "mean" is "arithmetic mean" (division of
sum); there is also "geometric mean" (root of product), etc.  After the
demotion, this set of ops can be changed or expanded to be something more
appropriate for statistical applications; some yet-missing SQL-standard
functions like pop-etc can then come in also.  Now these core-removed
functions are just shorthands for not-too-complicated expressions that
users can define for themselves with core ops, so they're not really
missing anything important if they only get the core.
For example, the current (arithmetic) mean is just:
    arith_mean ::= function (Rat <-- topic : bag_of.Rat)
        ([+]topic / #+topic)
... and geometric mean is something like:
    geom_mean ::= function (PRat <-- topic : bag_of.PRat)
        ([*]topic ** (0 - #+topic))
... but any vers in the dedic Statistics could be impl more efficiently.

* Note that general case of "median" is "quantile" (median is 2-quantile).

* Drop special entity name embedded support for inline type declarations
like "foobag : bag_of.Foo"/etc; instead, this syntax is demoted to a
dialect-specific thing that is just sugar for something like "foobag :
relation-type Bar { attr value : Foo, attr count : PInt, primary-key {
value } }".  That way, we can always point to a specific material that
actually exists when asked what is the declared type of "foobag", and also
we are psychologically more free to just declare things as relation types
anyway, and the added flexibility that comes with that, such as in the
definition of the system catalog itself, and also then the concept of an
entity name chain is no longer overloaded.

* Generalize the Set/Array/Bag/Maybe-specific operators so that: 1. the
names of the value/index/count attributes can be specified with arguments
(that are optional, and default to the current ones if not given); 2. they
work with relations of arbitrary degree.  For example, merge the Counted
extension into Bag and call it Counted, and generalize Array into Ranked
("Ordered" is already taken and best left as is) which also absorbs the
ranking and quota functions from Relation.pod, and generalize Set into
Relation.  The Counted|Bag is then any 1+ degree relation with a
positive-integer typed attribute C that has a key (or superkey) on all of
the attributes except for C; it is treated as special by the functions,
which are analogies to general relational functions that work as normal on
all attributes but C and merge C.  The Array|Ranked is then any 1+ degree
relation with a nonnegative-integer typed attribute I that has a key on I
and is further constrained that "max(r{I})+1 = #r"; I is treated as special
by the functions.  The Maybe is then any relation with a nullary key.  With
these generalizations, some concrete syntax like .[N] will just compile
into special cases such as assuming certain special attribute names, and
you can use the foo() syntax when that isn't the case.  After these
generalizations, some Counted|Array|Maybe|etc functions can be core and
others can be pushed into extensions, as is appropriate.  After these
generalizations, we may or may not still have named Array|Bag|etc types,
which will probably keep their definitions, as special cases of the
generalized where the attribute names match the canonical ones.  Also
rename "index" to "rank" in Array perhaps.  After the
generalizations, the distinct usefulness of Set would decrease somewhat.
Note: For a generalization of Maybe, consider the Zoo name, inspired by
Database Explorations that discusses MD's canonical missing info solution,
or alternately call it C01 in the spirit of D0C0/D0C1/D0.
Still in question is what if anything to change about [S|M]PInterval/etc.

* Consider removing MPInterval as a s-d type and rename SP to "Interval";
then, either one can just use "Interval" as a "Set" element to get the same
effect, or a relation over "Interval" can at least be demoted from core.

* Add official support for functions/expressions to be able to do some
things that they otherwise couldn't, such as have side-effects or be
quasi-non-deterministic.  To be specific, add support for side-effects that
occur external to the current in-DBMS process, such as output via some
side-channel like STDERR or a message queue, which can be used for
debugging a function.  But any such functionality can't directly affect the
current process, and in particular it can't affect the
function/expression's result value.  On the other hand, it is acceptable
for something to cause the function/expression to abort with a thrown
exception, since this isn't changing the result value.  There should be
metadata for any function which does or might do something like this, to
declare the fact.  In addition, we could support a limited form of
non-determinism, such as allowing a rand() or now() function that does
affect the calling function/expression's result, but that this is
constrained to be mutually deterministic within the whole of a single
Muldis D multi-update-statement.  That is, given the same arguments (or
none), now() would always return the same value within a
multi-update-statement, and might only change between different
multi-update-statements, and rand() likewise.  This might also give some
support for partial-sort functions, as long as they are consistent within
multiple calls in the same multi-update-statement.  Once again, such things
would need to be tagged with metadata.  Normal deterministic functions
always have the same result no matter how far apart.

* Make autonomous transactions / in-DBMS processes not so much startable
directly by a process but rather that the kernal/etc process always does it
directly and any other process asks to have such done by sending a message
to the kernal.  Similarly, DBMS-clients just become message passers, and
they start a process the same way as internally, by sending a message to
the kernal/etc to please call this procedure for me, and the result to the
client is also a message.  This also generalizes the stateful/stateless
thing and streaming/cursor or not thing.  Now also tied into this is
stimulus-response-rules, in that all stimuli are messages.  The kernal can
also initiate messages, such as this depot did mount, or whatever.

* Note that Oracle's autonomous transaction support looks like this:
    create procedure foo as pragma autonomous_transaction; begin ... end;

* Quoth http://ledgersmbdev.blogspot.ca/2012/09/or-modelling-interlude-postgresql-vs.html :
"A simple description of the difference [between MySQL and Postgres] is:
MySQL is what you get when application developers build an RDBMS.
PostgreSQL is what you get when database developers build an application
development platform."

* For real work projects in the short term where one would conceivably use
a Postgres enum, I should just use a "text check value in ('foo',...)"
instead, as it is much easier to manage/change those types.  For example,
if we want to temporarily add extra enum values outside normal range for
testing so the system ignores those values, eg status_code='WASQ'.

* Consider relaxing the restriction of how much of a depot must be defined
just in terms of itself.  So, for example, only a depot's data types (and
dbvar) must be defined wholly internally to the depot.  But any routines in
a depot may invoke routines outside of the depot if the former aren't used
in the definition of a data type or dbvar.

* Consider adding some way of generating a type specification from a value
of that type and consider having something like a system catalog which
describes the actual database value rather than a prescribed database type,
such as to help introspection of a database whose declared type is just
'Database'.  The MST thing of TTM may tie into this.
See also how the "Pick" DBMS works, or something.

* Add a scm_foo to the system catalog next to any place that declares a
DBMS entity name, particularly an expr/var/material, to indicate whether
the declared name is considered explicitly user-specified or parser-gen.
There may be more than 2 possible values (making this an enum rather than a
Bool) that relate, say, to distinguishing explicitly named but inlined
items versus explicitly named and not inlined items.  The sys-cat might
restrict based on this such that it doesn't allow certain references to
entities whose names are marked parser-generated, because any generated
source code would have to make the references visible.  A related
implication is that any entity names marked as generated are not sacred and
are free to be automatically renamed by different catalog-updating actions
such as source code optimizers.  Maybe also have something to distinguish
things declared in positional format so "0"=> etc don't appear, maybe.

* Numeric updates ...
See http://archive.adaic.com/standards/83lrm/html/lrm-02-04.html .
Excise the M;N format for bases 17..36 leaving just 2..16, absolutely.
Use "#" as separator rather than ";".
Write M as a base-10 integer rather than a single character.
These are more like Ada "based" literals then, read better, frees up ";".
So 16#FF is an integer, 16#'FF' is a blob.
Maybe also add Perl-6 inspired commalists, like this:
60#[43,5,12] (integer); no good reason for a blob analogy.

* Define some generic framework for units and measures, so to make it
easier to perform the large fraction of math involving such things, and it
can be more strongly typed and bug free.
The core of the system would have the domain-union type "Measurement" (or
more general), whose composing types should be named after the kind of
thing being measured (eg, "Mass" or "Duration") or the names of the
relevant units (eg, "Kilograms" or "Seconds"), and there would likely be a
variety of subtypes that are union types themselves for further
categorization.  These likely all scalar types.  Subcategories include:
1. Single-unit (eg, just seconds) or multi-unit (eg, any/all of YMDHIS).
2. Approximate (carries amount plus margin/sigfigs) vs exact (no margin).
3. Semantics, such as where versus howmuch, or continuous v discrete field.
4. What can be added or differenced or multiplied or divided etc, either
two of the same measure or a measure and a bare num, what result type is;
we may possibly have a separate domain-union type for each of those.
- When units are always directly convertable, they can be possreps of the
same scalar type, such as {seconds, minutes, hours}?.
- When not directly convertable, should be separate types, such as either
seconds vs months or ...
- For simplicity, we could consider all units flat, such that rather than
adding multiple dimensions orthogonal to everything else, we would just
have MetresPerSecondSquared etc types, this especially because many
plain-sounding units are actually defined as such, eg "Watts"="Amps*Volts".
- With this very basic structure, nearly all the complexity would be in the
operators, and generally one would explicitly define separate functions
for every kind of unit-measures they wish to pair up; eg, dividing distance
by time to get speed would be "(MetresPSecondSq <-- 0:Metres, 1:Seconds)"
but of course that func could overload the generic "/" virtual, +just work.

* To simplify the units and measures framework to not have to deal with
margin/sigfigs directly, we could have a framework for general inexact math
that tracks margin/sigfigs, but this is probably no more complicated than
supporting rationals when we have integers.

* IN PROGRESS ...
Rewrite/update anything talking about matters affected by process isolation
to both declare that Muldis D is generally orthogonal or agnostic to such
matters and makes no guarantees in general that any routine, even a recipe
or updater/function invoked by one, will see a consistent view of the
database during its execution, and generally remove "atomic" terminology,
and rename "nested transaction" to some other terminology.  Rather, any
guarantees of serializability of a recipe/etc will need further work by
users such as to explicitly configure their isolation or locks or whatever
as appropriate, and of course everything's affected by what DBMS you use
and what concurrency models it supports, such as locking or MVCC.  Likewise
the model being used affects when conflict errors may manifest, eg at
commit or earlier, or when/if user tasks will block, or how complicated it
is to resolve or avoid a conflict.  Matters of the concurrency model or
isolation are best not legislated by Muldis D but be left up to the
implementations and users.  Muldis D just has to require that the database
is always in a consistent state on statement boundaries et al.

* Define how one can split a Plain_Text depot into multiple text files since
you would conceptually put an entire potentially large program in one.

* Tweak the STD dialects to account for defining system modules with them.

* Update STDIO.pod and Cast.pod concerning the Text types split.

* PACKAGE:
- Support variant of "<[ a..z A..Z _ ]><[ a..z A..Z 0..9 _ - ]>*" nonquoted
name strs that's more liberal "<[ a..z A..Z 0..9 _ - ]>+" for just atnms,
possrep names, param and arg names, so any of "-foo", "3", "-4" can be bw.
- Update system catalog and grammars to add lightweight aliasing support
for whole materials, as a new "synonym" (name?) material.  These have no
mutual order but the actual non-synonym target is the "primary" name.
Grammar can be "synonym foo of nlx.lib.bar" et al in general form, or
"function foo|bar|baz (...) {...}" where original is the first one "foo"
and the other synonyms all live in the same subdepot, and in particular the
others are "not" inner materials of "foo".
- Also add [Integer, Rational, Boolean], make [Int,Rat,Bool] into synonyms.
- Likewise (and necessarily), subdepots themselves can have synonyms.
- Also update tuple (and by extension) database types to add attribute
synonyms which semantically are lightweight virtual attribute maps that
simply make 2 attributes always-identical so only one ever needs storing
and no map function is required.  Not the same as material/sdp synonyms.
- Update system catalog and grammars to add support for routine parameter
aliases, built-in to the definitions of the routines; all names for a param
are defined in an array, that ordering being source-code-metadata, and the
first item in the list being the "primary" name.
Grammar can be "function foo (Int <-- topic|0 : Int, other|1 : Int) {...}".
This is not supported for param names in generic expr context except for
the shorthand "=>foo", so "=>1 is allowed".
- Change grammar so any number positionals supported for both s-d and u-d,
always map to "0","1".."N" and *not* "topic","other".
Also, any ".foo" now is short for "0.foo" rather than "topic.foo".
- Consider changing param names of special routines like value-filter etc,
or at least change any "topic" to "0" (other "1") so it works with ".foo".
- Update the documented signatures of all system-defined routines to use
the updated grammars reflecting the above additions.  Add param aliases of
"0" and "1" for every "topic" and "other" respectively, keeping said old
names too, and add other aliases as appropriate.  Add routine synonyms for
every distinct way of spelling a routine that rtn-invo-alt-syn provided, so
one can then always use that spelling in "foo(...)" plain-rtn-inv syntax;
update all routine docs so that the "also known as" comments no longer
mention any declared synonyms, no longer mention anything as "C<foo>" but
rather just anything as "I<foo>".
- Just stick to that, basically, leave anything else such as Unicode or
rtn-invo-alt-syn alone/not-removed for this release.

* Consider adding a midweight version of virtual-attribute-maps which is
like the fullweight version but that it expressly maps 1 attr to 1 attr; it
still uses a map function but that is no longer Tuple<--Tuple.

* Demote the "[array|set|etc]_of" types from a special concept knowable by
the backend (and explained in Basics.pod), where you can essentially use
some data types without them being declared as system catalog materials, so
that instead actual s-c materials *are* required; this syntax will remain
only as a dialect feature which is a shorthand for inline type definitions;
eg, these are now all equivalent:
    - param : set_of.Foo
    - param : relation-type { over tuple-type T { value : Foo } }
    - param : set-type over Foo
... or we might consider more material kinds specif to [set|array|etc]-type
so to help preserve the user's syntax and be more compact, maybe, but those
6 or so could probably be repr by single m-k which has an enum type attr.
Also thanks to the change about replacing N-adic with dyadic s-d routines,
and its precedent, there is less need for "foo_of" shorthands anyway.

* Externalize all the details of character string repertoires or encodings
from the Muldis D core, such that say all the details of Unicode become
part of a Muldis D extension instead, and maybe ASCII likewise.
More plans pending.

* Considering the following items where non-ASCII chars are much more
pervasive (though strictly optional), replace the "op_char_repertoire"
pragma with a pragma that affects all non-quoted code in general, including
all nonquoted (but not quoted) entity names.  The options would be, at
least, the 3: ASCII, Unicode_6.2.0_canon, Unicode_6.2.0_compat.  There
would separately be options for each kind of quoted character string:
quoted entity names, texts, comments; see later TODO item about this; as
per that, all of these could be part of a single pragma.  A simpler
implementation could support only ASCII across the board as literal
characters, while non-ASCII data could be supported as escape sequences.

* Look at this for a long list of Unicode gotchas / false assumptions, etc:
http://stackoverflow.com/questions/6162484/why-does-modern-perl-avoid-utf-8-by-default

* Enhance the cat-type/syntax for defining tuple types as attr lists (and
by extension, relations and scalar possreps) to let users provide an
optional hint for the order that tuple/sca-pr/etc attributes should be
consulted when doing an equality test between 2 tuples/etc so to direct the
DBMS to do the least expensive comparisons first, eg integer attributes,
prior to more expensive ones, eg blob attributes; since the test
short-circuits, and assuming the vast majority of compares would return
false, this should aid performance in a clean way without users resorting
to overload operators or something for performance reasons.  This is a
separate hint from that garnered by marking relation attrs as key attrs,
and could work within that eg to suggest order within multi-attr keys.
Other areas in the language could probably be assisted by hints also.

* Consider support for functions that aren't fully deterministic but for
which it is reaonable to cache their results.  For example, a function ...
THIS THOUGHT REMAINS UNFINISHED.

* Consider making the generic equality test operator virtual, so
user-defined scalar types can explicitly define it for themselves (or it is
generated if they don't explicitly say otherwise), but it is still
system-defined for tuples and relations.  The semantics of this operator
then are treated not so much as "is same" but rather as "is substitutable",
which is how the system would treat it.  This still needs a lot of thought.
To be more specific, "=(Int,Int)" and "=(Int,List)" are system-defined and
can't be overridden, but "=(List,List)" isn't at that level.
Perhaps the answer is for "=" to be defined at the lower type level only
and "=(Any,Any)" be undefined and a type-mismatch error as TTM suggests?
And rather certain other operators are more universal and over which "="
can be defined?  Maybe have multple =-like ops for different purposes.

* Consider adding native concepts of "value instance identifiers" (or
substitute "occurrence" or "sample" etc for "identifier") where these are
analogous to Perl "references", and provide system "functions" for
obtaining the VII of values, like Perl has "ref".  Note that given the same
instance X, multiple vii(X) must give the same result.
Especially in a system where the generic equality test operator is virtual,
VII can provide an implementation-influenced baseline something or other.
THIS THOUGHT REMAINS UNFINISHED.

* Taking further the idea of how various Text subtypes are defined, eg that
normalization is required, so that the Text generic equality operator would
"just work":  Consider defining that formally a Tuple or Relation etc value
has a canonical form in the low-level type system, meaning the member
attributes and tuples are always sorted in a specific system-defined way,
and so any operators that are sensitive to the low-level type system would
be fully deterministic, and the regular "=" operator would just work
without that having to be virtual or have special cases.  The only wrinkle
with that then relates to scalars and their multiple possreps.  Of course,
this is just the canon, but most normal operators wouldn't be sensitive to
this and so tuples/relations are still effectively unsorted, and more
important, an implementation doesn't actually have to store them sorted
normally, same as the "NFD" character strings don't actually have to be
stored that way by the system.  The canonical order is as follows: 1. Two
Int always sort as is normal for integers; 2. Two List always sort as is
normal for comparing strings, by comparing their elements pairwise; 3. An
Int always sorts before a List.  Further to this canonicalization, we no
longer offer multiple forms for Tuple and Relation; now, the payload of
each has 2 elements, heading and body, where the heading is sorted by
attribute name String and each tuple-body corresponds as usual, and a
relation body is a list of 1 element per tuple-body, and the elements of
that list are sorted by tuple-body List value.  So now the main thing to
figure out how to work this is in regard to multiple scalar possreps, so
that "=" has the correct semantics at the user level.  Ostensibly the
solution is quite simple, which is to mandate that a specific possrep is
the scalar type's canonical low-level form, and that moreover the name of
this possrep must be the empty string, meaning that if you actually want
that possrep to have a different name you must have a second possrep whose
form is identical to it.  A consequence of this is that, in the low-level
type system, every scalar type only has 1 possrep, and hence the possrep
name doesn't actually have to be stored, so then a scalar value then just
becomes a 2-element payload where the first is the scalar type name and the
second is a tuple value payload.  A benefit of all this canonicalization is
also that certain operations like a function to extract "a" tuple from a
relation can be fully deterministic, or similarly that we have a way of
finding a default "total sort order" for the entire type system that is
fully deterministic, even if not generally useful.  The function for this
low-level sorting, that works on Universal, is *disjoint* from the ordering
routines that users normally deal with, and is just intended for use in
canonicalization of List etc for "="; it is a plain real function like "="
and not a virtual like "<=>" etc.  Regardless, this whole paragraph
requires more thought.

* Update the Plain_Text grammar to split up the "Name_payload" or its parts
further so that, rather than just the 2 "[|non]quoted_name_str", there is
at least the additional "nonquoted_rtn_invo_name_str" which is only allowed
to be used in a routine invocation context like <op><unspace>(...), with
trailing parenthesis, and not in a context lacking trailing parenthesis.  A
"nonquoted_rtn_invo_name_str" is a nonquoted string containing no
whitespace and, in addition to all the chars nonquoted_name_str allows,
also many other symbolic chars such that wouldn't confuse the parser, so
bracketing chars would likely be disallowed, at least as leading or
trailing characters in the string, and trailing colon could be disallowed,
and leading comma or leading => etc.  The idea here is that people can then
write "+(foo,bar)" for addition or "++(foo)" for increment, or "=(foo,bar)"
for comparison, "@(t)" or "%(r)", or ":=(target,value)" for assign.
In this case, if infix ops are allowed, they'd have to have mandatory
surrounding whitespace.
We also generally have to revisit Unicode for what is allowed in bareword
variable/etc names such as non-Latin or accented letters in general.  The
parser would have to use Unicode character classes in its definitions,
then.  Look at what Perl 6 does for some guidance.
As per another change, also assume that the idea of the internal catalog
no longer using Unicode for sys-def entity names is no longer true.
So Muldis D would then much more be Polish notation (with parens) by
default, and it should be much easier to just use the whole language that
way when it is more terse like this.  Supporting polish without parens
would be up to rtn-inv-alt-syn replacemnts while above is in plain-rtn-inv.
See also the 2nd(+?) next TODO item on splitting rtn-inv-alt-syn.
Also add yet another nonquoted...name_str that is just for use with
attribute/param/arg names and is only slightly less restrictive than the
old nonquoted_name_str in that it also allows strings of just or leading
digit chars; this is mainly so one can write positional params wo quotes.
Maybe just this last one can be added ASAP, and the other wait longer.

* Consider creating a branch of the Muldis D spec (and of the Muldis D
Manual) which retains all of the current spec features, and subsequently
strip out the whole rtn_inv_alt_syn catalog abstraction level in trunk so
that we can more radically evolve the language design at the more
fundamental level which plain_rtn_inv has access to, without worrying about
clashes or the complexity of a dozen-plus-precedence-level grammar.
Ideally the more fundamental level can evolve to the point that a
lot of what rtn_inv_alt_syn offers is no longer necessary in practice
with regards to making the code more terse.  The branch would merge in the
more fundamental changes with the old retained rtn_inv_alt_syn to see how
they might look together, or show how the new is absorbing the old; ideally
their differences would reduce over time without th branch losing features.
In the interest of marketing, the reduced trunk would retain all or much of
the example code using the then-removed features, as well as gain ones
using not yet specced features.  Each examples section would potentially be
split in 2, with the normal "Examples" just using the reduced spec features
and a new "Potential Future Examples" having anything not yet specced.
Also, the 3 Dialect files wouldn't actually lose the rtn_inv_alt_syn
precedence level but rather it would be made impotent as the grammar would
just define it as a non-proper superset of plain_rtn_inv for now; mainly
the change is that the 2 main pod sections "FUNCTION INVOCATION ALTERNATE
SYNTAX EXPRESSIONS" and "IMPERATIVE INVOCATION ALTERNATE SYNTAX STATEMENTS"
would be removed, or alternately stripped down to collection of "Potential
Future Examples" sections with a bit of commentary to explain if needed.

* The new version may be a lot easier to learn, considering that SQL + many
other C-like languages actually don't have too many non "f()" format ops.
Perhaps the main use of rtn_inv_alt_syn later is for people that want their
code to look like math/logic/etc exprs rather than named function calls.
IDEA:  Split rtn_inv_alt_syn into 2 abstraction levels where the lower one
has just 1-2 dozen or so plain prefix/infix ops such as
[:=, =,≠,!=, <,>,≤,<=,≥,>=,--,++, not,!,and,or,xor, +,-,|-|,*,/, ~, @,%,#]
and few are allowed having modifiers or that aren't in most languages.
Likely disallowed in lower level are [<=>,abs,div,mod,exp,^,**,log], the
other math ops, all other or Unicode variants of logic ops, all hyper-ops
including hypers of := or !, practically all relational/set/array/etc ops
including membership or sub/super tests.  As a middle-ground, for which we
could probably have a middle-third level from the split, are all the
postcircumfix ops that do restricted-to-constants shorthands of the likes
of array element access, projection, rename, un/group, un/wrap etc.
Things like the full set of infix logic ops are reserved for highest level,
and likewise for majority of Unicode ops and their ASCII-symbolic versions.
Now assuming we get generic <sym-op>(...) in plain-rtn-inv, and so
"+(foo,bar)" etc is an option, then we should reprioritize the above 3
post-split levels so that a level adding just postcircumfix syntax for
project/group/ary-acc/rename/etc should be the lowest additional level, so
one can be able to say "foo{...}" without also needing support for foo+bar.
Maybe call that new lowest "rtn_inv_pcfx_alt_syn".  Making postcircumfix
the lowest alt syn is also fitting because just it is like some of the
lower levels such as code-as-data where using some syntaxes make certain
inputs hard-coded, such as the attr names or interval-endpoint-flags,
versus those taking variables in the the more verbose generic syntaxes.
Presumably all levels higher than rtn_inv_pcfx_alt_syn are plain infix
or paren-less prefix with fully-variable arguments like generic functions.

* Consider making ASCII lookalikes for as many Unicode symbolic operators
as possible; for example:
 - join: ⋈ -> |X|
 - semijoin: ⋉ -> |X
 - semidiff: ⊿ -> |>
 - diff: ∖ -> \
Well it's a thought anyway, though may not be worth the trouble in general.
If the backslash is allowed to be used this way, then we'll have to put
limits on its other uses for starting escape sequences, otherwise work out.

* Maybe this isn't feasible, but ...
Consider formally making every function map 1:1 from a tuple input to a
tuple output; it declares exactly 1 parameter that is a tuple type and its
result declared type is a tuple type.  Consider making every updater
formally do something analogous, such as having exactly 2 tuple-typed
parameters where only 1 is subject-to-update.  A recipe is like that but
has 4 tuple-typed parameters, 2 like updater and 2 global alias analogies.
A virtual attribute map kind of resembles this already.
Doing this would require making tuple attribute accessors special, their
own expression/etc node kind and not just a function ... though they kind
of are already as an alternative; also, variable assignment would have to
be a special node kind and not just an updater; in both cases, to save
their definitions from being mutually recursive.

Note that the first relational database system was the IBM IS/1, in 1970-2,
and the second one was IBM Peterlee Relational Test Vehicle (PRTV); the
latter's command language was Information Systems Base Language (ISBL).

----------

* In all 3 STD.pod, add code examples for each of these 4 material kinds:
scalar-type, domain-type, subset-type, mixin-type.

* In all 3 STD.pod, complete the description text, defining interpretation
in Plain_Text and structure in the 2 Perl-STD, for each of these 7 material
kinds: scalar-type, tuple-type, relation-type, domain-type, subset-type,
mixin-type, subset-constraint.

* In all 3 STD.pod, populate the entire pod sub-section for each of these 2
material kinds, to provide concrete grammar, description text, and code
examples: distrib-key-constraint, distrib-subset-constraint.

----------

* Eliminate the simple monadic postfix special syntax category.  Convert ++
and -- into simple prefix ops, because an expression with
that in it is no longer end-weighted, and it would be less likely to
confuse people into thinking the op is variable increment rather than just
returning a result.  Removing the category also simplifies the parser as
there are no longer pre vs post precedence conflicts, and helps open the
door to the parser being more generic.  Simply eliminate postfix "!"
factorial or change it to prefix "fact".

* Update Basics.pod or other places to distinguish between the 2 main ways
that a type can be infinite, such as with "outwardly infinite" and
"inwardly infinite"; the later is when any 2 values have an infinite number
of others between them, so eg a time-of-day type could be infinite in the
inward sense but not in th outward sense; th result type of sin() likewise.
Also, the singleton types -Inf, Inf only refer to outwardly infinite types.

* Change the basic exception throwing mechanism from a function/procedure
to its own expression/statement node kind.  Call the new node kind "fail"
or "failure" or "throw" or "raise" something.  The "fail" node has a child
expression node or references a variable node which defines an Exception
value.  Simply evaluating a "fail" expression node will throw the exception
so a "fail" expr node is expected to only be the child of a short-circuit
expression like ??!!.
- Add a "fail" term, which throws a generic/default Exception value,
and/or a tight-binding "fail" prefix-keyword which takes an Exception arg;
that term/prefix is the concrete syntax for the new fail node.
- The "assertion" function can then go away; instead of writing
[$foo asserting $foo != 0], say [$foo = 0 ?? fail !! $foo].
- Add a few simple functions that each result in a kind of generic
Exception value.  At least have a niladic one for the most gen exception.
Then one could write [<cond-expr> ?? gen_exception() !! <expr-when-ok>].
- The treated() function then is just a wrapper over ??!! + isa.
- The fail() procedure will go away, replaced with a term/keyword also,
which maps to the "fail" statement node.
- Maybe use 'fail' for niladic term and 'raise' for prefix term?
- New keyword speelings:
    - failure
    - raised <expr>
    - fail
    - raise <var>
- Maybe alternatively, make an assertion into a lexical entity that is like
an expr node but doesn't have its own node name, and so is always used
either inline or offside, the main point being that users don't have to
come up with another node name when the node represents the same value as
another node and should naturally just have the same name.
Example:
    foo ::= ...
    asserts bar( foo )
    baz( foo )
... here, the assertion only happens when baz() is going to be evaluated;
the spelling is "asserts" since it should be an adjective.
- There also needs to be a version that can assert multiple exprs.
- Or actually, the ??!! version may still be better?
- Naming the "duplicate" isn't actually that hard; just use a leading
underscore, eg:
    _foo ::= foo asserting bar
    _foo ::= bar ?? foo !! failure
... so maybe that's best?
- A BIG THING TO CONSIDER HERE IS, HOW DO FUNCTIONAL LANGUAGES MAKE
ASSERTIONS ON COMBINATIONS OF ARGUMENTS ... OR IS THE ANSWER THAT ALL
FUNCTIONS HAVE EXACTLY ONE ARGUMENT?  SEE WHAT HASKELL/ETC DOES.

* Change generic assertion mechanism from a function/procedure to its own

* Add support for materials to have aliases.  But this kind of alias would
be simple, just an alternate unqualified name that exists in the same
namespace and is for the same material.  Aliases would be declared with an
"aliases" attribute, typed set-of-Name, held directly in the same catalog
types that have "name" attributes; for example, add it to the "FunctionSet"
type.  So, R.count becomes a simple alias for R.cardinality, and we can add
a whole bunch more aliases, so to make it friendlier for people who prefer
to call routines with foo(x,y) syntax rather than alternate symbols.  A
common use could be to provide both "prefix" and "infix" reading names,
such as both "product" and "multiply", and especially to give shorthands.
Example: "function product|multiply|mul (Int <-- x : Int, y : Int) {...}".
The first one in the list is the primary name, remainder are the aliases.
Or actually, it would probably be better for FunctionSet et al to *not*
internalize aliases, but rather have each alias exist as a separate
material which cites what it aliases.  And then that version could exist in
any public namespace (usually nlx), and not just the same subdepot as what
is being aliased.
The SYNONYM schema object of Oracle and other dbs corresponds to this, and
maybe "synonym" is what I should call mine too, being what the specific
material kind is called, leaving "alias" as a more generic term.
Even if we have separate synonym materials for routines/etc, one can still
declare them bundled into their originals like in the above foo|bar example
as that would just be a dialect shorthand but produce separate materials.
Also useful in support of users having their own home subdepots which have
aliases to the things they use, without them having to know where they are.
Add alias for every 'op' node 2nd element for a routine, meaning eg add
"+" and "⋈" as aliases, and so then a Muldis D parser can then produce
calls to those, as if one said `"+"(4,5)` or `"⋈"(foo,bar)`, and so we can
better remember the individual syntactic choices that the users made.
But then, how do we deal with the idea of making logical-not into a meta-op
so that there is no actual is_not_same|"≠" function etc; how do we
preserve user's individual syntactic choices then?  So think about that.
While SQL synonyms can also be used for relvars, mine would probably only
be used for materials - types, routines, stim-resp-rules, themselves, etc;
perhaps leave relvar aliases to be handled by virtual attributes.

* With the improvements from having aliases or supporting "+"(x,y) etc, and
other language improvements, it becomes a lot more feasible for users to
settle for users to be satisfied with "plain_rtn_inv", that being
sufficiently terse, and so there is less need for "rtn_inv_alt_syn" to be
implemented or available.

* Maybe also treat material names like `function "infix<+>" (...) {...}` as
special such that if a parser encounters a random "foo + bar" then it would
parse it as if it were `"infix<+>"(foo,bar)` maybe I guess.  But if this is
going to work in a general sense, including for user-defined things, then
general format rules have to be set out for the parser so that if it sees
anything like X, without knowing what ops are declared, then it treats it
as an operator rather than some other construct.  On the other hand, we're
sure to run into trouble in trying to support non foo(x,y) syntax for
user-defined operators (besides those overloading system-defined virtuals),
and so better off just not doing this period; "infix<+>" is not special.

* Add support for routine parameters to have aliases, that is, for a named
parameter to be able to bind with a named argument where the argument may
have several possible names.  One use for this would be to support
parameters where it is desired to refer to them within their routine using
one name, but to use a different name in the argument, such as because the
latter is shorter or reads better (the Perl 6 spec should have some
examples of this).  Another use for this is to provide better support for
mixtures of arbitrary numbers each of positional and named routine
arguments; any parameters that would be reasonable to have a positional
argument would have 2 names, where one is an integer and one is text.  All
Muldis D grammars would be updated to no longer consider 'topic' and
'other' as special, which is a contrived notion, and instead consider
'0','1',... special.  And so, for all system-defined or user-defined
routines, any `op(foo,&bar,baz)` would be parsed into the same thing as
`op("0"=>foo,&"1"=>bar,"2"=>baz)`, and `.name` would be `"0".name`.  Now it
will so happen that "topic","other" will be commonly used in parameter
names, typically paired with "0","1" but we can now be a lot freer to name
parameters something more descriptive, such as "addends", and not
artificially make them topic/other simply so they support positional
syntax.  An idea for declaration syntax when aliases exist is to use the
"|" char; eg `function foo (Int <-- topic|"0" : Int, other|"1" : Int)`.
Of course, this complexity is only in param lists; arg lists are unchanged
and still are plain tuples with a single name per attr/arg.
For simplicity, a single param name will be more important than the others,
and only that would be its "expression node name" or "variable name" within
its routine, by which it must be referenced; therefore, the current
system catalog for declaring parameters can remain unchanged, and new
rtn-decl-type rtn-heading-attrs can be added to declare aliases.
Largely for flexibility, and correctness where they don't make sense,
parameters will never automatically have a number alias, but rather only
when the routine definer explicitly gives it one.
Of course, these aliases only apply to regular params, not global params.
One result of this change is that the Muldis D grammars will no longer
consider positional ro and rw args in separate spaces such that they can
appear in either order; now all positional args must be in the correct
mixed relative order, as there is only one "0", not one per ro and rw.

* Add special syntax for more ops:
    - ?#foo - "has 1+ elements" - is_not_empty(foo)
    - !#foo - "has zero elements" - is_empty(foo)
    - foo :=!# - assign_empty(&foo)
... and maybe rename underlying routines in the process.

* Update the mixins feature to add support for mixins that define
attributes that types can compose, whereby we support some approximation of
"specialization by extension" while still actually being just
"specialization by constraint".
Maybe also it could be said ...
A primary purpose of mixins is to help with managing software reuse, mainly
when multiple types have a number of attributes in common, a mixin can
define these and then the multiple types can compose that mixin.  A mixin
or type that composes a mixin can both add additional attributes of its own
to what the mixin defines, and the composer can add extra constraints over
the composed attributes like forcing a subtype.
Maybe also do ...
Support delegation / 'handles'; for example:
    - Name explic delegate to Text attr
    - maybe Blob, Text explic delegate to String attr
    - a ColoredCircle would delegate to both Color and Circle attrs?
This will all take some work to get right; not /all/ Rat/etc can be subst.
Probably *only* those operators that Rational/etc explicitly declares can
be delegated to Rat/etc by TAIInstant/etc.

* Replace many N-adic routines with dyadic ones, specifically
those whose definition is a repetition of a dyadic operation (so, 'sum' or
'join' etc yes but 'mean' no), which users then can invoke by way of a
reduction function if they want N-adic syntax.  Also let system catalog
store more information such as whether or not functions are commutative or
associative or idempotent or symmetric etc; likewise, the function def can
store what the operation's identity value is, if it has one, as meta-data,
useable when comm/assoc; the reduction func can read this using a
meta-programming function or something.  Reduction will fail if used on a
base func that doesn't define an identity if given an empty list.
The point of this change is to make the common dyadic case of N-adic
operators simpler, and also set a foundation for user-defined operators
that provide more information such that a compiler can be more effective
in optimizing them, or something.
The explicit/normal way, then, to indicate in code whether you want the
parser to produce a reduce op wrapper call rather than nested direct
invocations in the system catalog, is to just invoke the reduction
operator directly and explicitly pass an operand list; but the reduce op
would have special syntax, taking normal collection exprs, such as:
    [+] {5,23,5}
    [~] ['hello', 'world']
    [join] {order,inventory}
    [*] {1..5}
... or something.  Not using that would parse into nested dyadic calls
instead though the compiler can still rearrange.
Once we do that, its also simple to add hyper-operators, though arguably
these are redundant with 'map' or 'extension' etc.
Or this would be better for simplicity, given it won't be used as often,
and any dyadic infix function at all may be used, spelled the same way:
    reducing + {4,23,5}
    reducing ~ [...]
    reducing join {...}
    reducing * {...}
    reducing <nlx.lib.myfunc>(a=>3) {...}
... and so the regular operators can be parsed as usual.
Or maybe:
    reduced {4,23,5} using +
    reduced {...} using <nlx.lib.myfunc(a=>3)
... but that might have an end-weight problem?
Or, still go symbolic like the first one, but use prefix notation so that
it works well with both symbolic and wordy or inline-defined operators:
    []+ {5,23,5}
    []~ ['hello', 'world']
    []join {order,inventory}
    []* {1..5}
    []<nlx.lib.myfunc>(a=>3) {...}
Another consideration is that, when combined with routine synonyms that are
symbolic, the plain_rtn_inv alone would let you do this:
    reduce( <"+">(), {5,23,5} )
    reduce( <"~">(), ['hello', 'world'] )
    reduce( <join>(), {order,inventory} )
    reduce( <"*">(), {1..5} )
    reduce( <nlx.lib.myfunc>(a=>3), {...} )

* Furthering the above, add somewhat generalized support for what Perl 6
calls "meta" operators, at least in that we define and exploit several.
The general reducer above would be one of these.  Another is the negated
relational, whose syntax is putting ! or not- in front of any Bool-resu op.
Another is the assignment, putting := in front of any function.
For !, we can then eliminate all the "not" variants of any Bool-resulting
functions, so eg "x != y" parses into "not(is_same(x,y))", same as if
they had said "!(x = y)".  As for the old intended purpose of all the not-
variants, which is to preserve the user's intent of how code should look,
we could simply have an alias for the not() function which is what is
parsed into when != is used, and the old not() is just parsed into when the
separate prefix op is used.  On the other hand, while lots of not- variants
would go away, we'll keep the alias-but-param-order-reversed dualities such
as less-than/greater-than and sub-superset; unlike these, what we're
eliminating would not result in losing track of which args are lhs/rhs.
A related change is infix ops like ≠ or ⊈ would parse into not(foo()) even
though they don't have the !; these would be aliases for the combos, same
as Perl 6 has != as an alias for !==.
For :=, we can eliminate all the updaters that are just shorthands for
doing an op and assigning the result to one of the args.  And so a
"foo :=union bar" would parse to "assign(&foo,union(foo,bar))".  Once
again, an alias for assign() can exist which such combos are parsed into,
where the regular assign() is used when users write "foo := foo union bar".
Of course, despite Muldis D requiring operator combos where singles used to
work, we assume that implementations will be smart enough to, say, use a
single "!=" or "insert into foo ..." etc when it sees the combination, so
there is no performance loss.
Probably, any meta'd operator would have the same precedence as the base
operator that it is modifying.
Adding the hyper-meta may not be useful since we already have map()/etc;
or alternately it might be useful in avoiding some uses of map or extend
or substitute etc where users are just adding/defining one attr.
Or maybe hyper-meta would only be useful with Set/Array/Bag because the
general map/extend/etc would require naming the attribute explicitly.
As for ASCII vs Unicode etc, that preference is never encoded in the system
catalog, so when code would be generated from the system catalog, it would
be up to the generator's configuration for which versions are used.

* Add hyper-meta in a more general fashion, as per the Ranked general type
of which Array is a more specific kind.  The hyper-meta is fundamentally
associated with the join operator, because it typically involves taking 2
relations, joining them on one set of same-named attrs (exactly 1 usually),
and then taking another set of *same-named* attrs and applying the hypered
op pairwise and deriving a single replacement set of those attrs with the
results.  The argument attrs would be renamed distinct first.  For example,
given 2 relations A{key,value,x} and B{key,value,y}, where we assume that
"key" is a unary key of each relation, the expression
"A >>+<< B" is roughly like this code:
  with (
    a ::= A{%others_a<-!key,value}{value_a<-value}
    b ::= B{%others_b<-!key,value}{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr{<-%others_a}{<-%others_b}
And the result is a relation with heading {key,value,x,y} but of course
with the more typical case the inputs and output are just {key,value}, in
which case that simplifies to:
  with (
    a ::= A{value_a<-value}
    b ::= B{value_b<-value}
    ab ::= a join b
    f ::= function (Tuple <-- t : Tuple) {
      %{ value => t.value_a + t.value_b }
    }
    fr ::= extension( ab, <nlx.lib.f> ){!value_a,value_b}
  )
  fr
A variant taking a relation and a tuple would be like the >>+>> /etc form.
We might have variants for join vs union etc or generalize this further so
that bag/counted variants of relational ops can be defined using this
generalized hyper in combination with the regular relational ops, maybe.

* About extra metadata in the system catalog for functions/etc, see
http://www.postgresql.org/docs/9/static/extend.html for some ideas, such
as 35.13.x on Pg's use of COMMUTATOR and NEGATOR where function pairs
declare their complement operator.  The first pairs up "<" and ">" say (and
"+" pairs with itself) while the second pairs up "<", ">=" (dbl-chk that).

* Note that Pg exts are like Muldis D system modules in what they do, such
as that they add types and routines etc to the language.

* Change multi-update to be a sequence of statements rather than a set, and
explicitly allow the same target to be used more than once ... this could
be the case anyway thanks to virtual relvars etc.

* Move or adapt more Text functions into Stringy.
- Fundamentally all Stringy funcs work on Text in terms of the
"maximal_chars" possrep; this will just work correctly for when all
func args are of the same Text subtype, such as Canon etc.
- The Stringy/Text ops are analogous to Rational ops such that it is like
doing fraction math.  catenation() is like sum(), replication() is like
multiply, a substring test is related to difference/subtract (maybe "?~"
and "!~" might work as infix ops for something?).
- Move cat_with_sep to Stringy; semantics are clear cut and generalizable.
- has_substr ought to work with Stringy no problem from the Text and Array
perspectives, but Blob presents an issue purely concerning bit alignment,
such as whether we're searching on bits or on octet/etc alignments.

----------

* Update the virtual attributes maps so there is a way to manually specify
a reverse function, as meanwhile all the virtuals don't have to be either
read-only or updatable due to an automatically generated reverse function,
which might vary by implementation, which may be considered broken.  Note
that the reverse functions might have to be defined as per-tuple
operations, separately for insert/substitute/delete.

* Add new "material" kinds that define state constraints (address as simple
nlx.*.data.*), like type constraints but ref in reverse.

* Update the "material" kinds that def stimulus-response rules / triggered
routines so that they work for more kinds of stimuli, and maybe change the
keywords.  The material kind has 2 main attributes, where the "stimulus"
defines what to look out for and "response" defines what to do when the
former is sighted.  Some possible keywords for the first are "stimulus",
"cause", "when"; for the latter, "response", "effect", "invoke".

* Add new "material" kinds that define descriptions of resource locks that
one wants to get, starting with basic whole dbvar, relvar locks (address as
simple fed.data.foo.*, as well as simple relvar tuple locks (addr as prior
plus lists of values to match like with a semijoin); leave out generic
predicate locks at first but note they will be added later.
Update the system catalog concerning managing shared|exclusive locks or
looking for consistent reads between statements, etc.

* Large updates to docs concerning transactions and resource locking.
Note:  Supposedly PostgreSQL and MySQL use read-committed isolation by
default while SQLite provides serializable.

* Rewrite the "Exception" catalog type so it can carry metadata on what
kind of exception occurred, not just that an exception occurred.

* Also study SQL concept of conditions and handlers, looks sort of like
something between exception handling, signals; or it is their exceptions.

* Also adapt something like Postgres' LISTEN/NOTIFY/UNLISTEN feature, which
is an effective way for DB clients to be sent signals, such as when a
database relvar has changed.

* Use a conceptual framework for database transactions that is strongly
inspired by how distributed source-code version control systems (VCSs)
work, in particular drawing on GIT specifically.  The fundamental feature
of the framework is that the DBMS is managing a depot consisting of 1..N
versions of the same database, where every one of these versions is both
consistent and durable.  Each version is completely defined in isolation,
conceptually, and so any versions in a depot may be deleted without
compromising each of the other versions' ability to define a version of the
entire database.  It is implementation-dependent as to how the versions are
actually stored, such as each having all of the data versus most of them
just having deltas from some other version; what matters is that each
version *appears* to be self-contained.  Every version is created as a
single atomic action, and it is never modified afterwards, though it may be
later deleted (also an atomic action).  Every in-DBMS user process,
henceforth called "user", has its own concept of the current state of the
database, which is one of the depot's versions that is designated a "head".
A user's current head is never replaced during the course of the in-DBMS
process unless the user explicitly replaces it, such as by either
performing an update or requesting to see the latest version (the latter
done such as with an explicit "synchronize" control statement).  Therefore,
each user is highly isolated from all the others, and is guaranteed
consistent repeatable reads and no phantoms; they will get repeatable reads
until they request otherwise.  The framework has no native concept of
"nesting transactions" or "savepoints" or explicit "commit" or "rollback"
commands.  Rather, every single DBMS-performed parent-most multi-update
statement (which is the smallest scope where TTM requires the database to
be consistent both immediately before and immediately after its execution),
is a durable atomic transaction all by itself.  The effect of a successful
multi-update statement is to both produce a new (durable) version in the
depot and to update the executing user's "head" to be that new version (the
prior version may then be deleted automatically depending on
circumstances); a failed multi-update statement is a no-op for the depot,
and the user gets a thrown exception.  A depot's versions are arranged in a
directed acyclic graph where each version save the oldest cites 1..N other
versions as its parents, and conversely each version may have 0..N
children.  A child version has exactly 1 parent when it was created as the
result of executing a multi-update statement in the context of the parent
version; the parent version is the pre-update state of the database and the
child is the post-update state of the database.  A child version has
multiple parents when it is the result of merging or serializing the
changes of multiple users' statements that ran in parallel.  One main
purpose of tracking parents like this is for reliable merging of parallel
changes, so that the intended semantics of each change can be interpreted
correctly, and potential conflicts can be easily detected, and effectively
resolved.  More on how this works follows below.  Note that versions simply
have unique identifiers to be referenced with and there is no implied
ordering between them if they are generated as serial numbers or using date
stamps, though versions with earlier date stamps are given priority in the
case of a merge conflict.  So a multi-update statement is the only native
"transaction" concept, and it is ACID all by itself.  Now, the
multi-statement "transactions" or concepts of nested transactions or
savepoints would all be syntactic sugar over the native concept, and
basically involve keeping track of versions prior to the head and
optionally making an older one the head.  This framework uses the VCS
concept of "branching" (which is something that GIT strongly encourages the
use of, as GIT makes later "merging" relatively painless) as the native way
to manage concurrent autonomous database updates by multiple users.  By
default, when no users have made any changes to the database, a depot just
has a "trunk", and its childmost or only version is called "master"; every
database user process' "head" starts off as the "master" version when that
process starts.  Each (autonomous) user process that wants to update the
database will start by creating a new branch off of the trunk, and
subsequent versions of theirs will go into that, rather than into the trunk
or some other branch.  The trunk is shared by all users while each user's
branch is just for that user, as their private working space.  Note that,
unlike a VCS in general where branches can become long-lived and interact
with each other independently of the trunk, the framework instead follows
the typical needs of an RDBMS, which espouses a single world view as being
dominant over any others, and expects that any branches will be very
short-lived, not existing for longer than a conceptual "database
transaction" would; only the trunk is expected to be long-lived.  (This
isn't to say that a DBMS can't maintain them long term, but one that acts
like a typical RDBMS of today wouldn't.)  Note that the final action on a
branch that involves merging into the trunk, this would be perceived by all
other DBMS users as all of the changes wrought by the branch being a single
atomic update, though the user performing it may see several steps.

* Flesh out matters related to starting or communicating between multiple
autonomous in-DBMS processes, in general, besides the special case about
sequence generators.

----------

* Add to Routines_Catalog.pod and other files
definitions of any remaining routines, eg String routines, that would be
needed so that for all system-defined types all the necessary
system-defined routines would exist that are necessary for defining said
types, especially their constraint or mapping etc definitions.  So in
String.pod we need [catenation, repeat, length, has_substr] etc.
Also add "is_coprime" or GCD or LCM or etc which are used either in the
constraint definition of Rat or in a normalization function for Rat; see
also "the Euclidean algorithm" as an efficient way to do the calculations.

* Consider adding type introspection routines like: is_incomplete() or
is_dh() or is_primitive|structure|reference|enumerated etc.  Or don't
since one could look that up in the system catalog.  But more tests on
individual values might be useful, or maybe we have enough already.

* Add ext/TAP.pod, which is a partial port of Perl 5's Test::More / Perl
6's Test.pm / David Wheeler's pgTAP to Muldis D; assist users in testing
their Muldis D code using TAP protocol.  The TAP messages have type Text.

----------

* Add concept of shallowly homogeneous / sh- relation types to complement
the deeply version, and named maximal types like SHRelation, SHSet,
SHArray, etc to complement the DH/etc, and sh_set_of/etc to complement
dh_set_of/etc; but not sh-scalar or sh-tuple as the concept doesn't make
sense there.  Then update functions like Relation.union/etc to take
sh_set_of.Relation rather than set_of.Relation, which more formally defines
some of their input constraints.

* Consider adding an imperative for-each looping statement; the main
question here is whether it should work on any (unordered) relation or just
on an Array (in which case it iterates through the tuples in sequence by
index); the question is what tasks the for-each would be used for; perhaps
both versions are useful; presumably the main reason to have for-each at
all is when I/O is involved and some derivative needs to be output either
where order matters or where order does not matter; but perhaps only a
routine is needed here such as a catenate function plus normal I/O output.
The question also is what tasks would an imperative for-each be needed for
that functional constructs like the list-processing relational functions
can't better be used for those tasks instead.

----------

* Add a round-rule param to rat division, I suppose, since in general we'll
need it if we want to maintain a rational radix through every op (+,-,*
will already do so when all their args are in the desired radix).

* Add explicit support for +/- underflow, +/- overflow, NaNs, etc.
I'm inclined to think +/- zero is unnecessary when we have underflow and
can be confusing anyway (just a single normal number zero is better).
I'm not sure if +/- overflows are useful or if infinities cover them for
our purposes.  How this would work is that we define a set
of scalar singleton types, one for each of the special values.  Then we
define extended versions of the Int, Rat, etc types where the extended
types are defined in terms of being union types that union the regular
numeric types with the special singleton type values.  This approach also
means just one each of +Underflow, -Overflow, etc is needed and is a member
of extended Int or Rat etc.  Consider using the existing names "Int"/"Rat"
with the versions that include these special values, and make new names for
the current simpler versions that don't, such as "IntNS" (int no specials),
"RatNS", etc.  Either way, it is useful to support the full range of values
that a Perl 6 numeric can support, or that an IEEE float can support,
without users necessarily having to define it themselves.
IDEA:  Maybe make all normal math/etc ops work with the extended versions
(those with NaNs, infinities, etc) and in situations where users don't want
those special values they just use a declared type excluding them, and then
the normal type constraints will take care of throwing exceptions when one
divides by zero for example.

* Flesh out Interval.pod to add a complement of functions for comparing
multiple intervals in different ways, such as is-subset, is-overlap,
is-consecutive, etc, as well as for deriving intervals from a
union/intersect/etc of others, as well as for treating intervals as normal
relations in some contexts, such as for joining or filtering etc, as well
as a function or 3 to do normalization of Interval values.
Maybe the type name 'Range' can be used for something.
Maybe the type name 'Span' or 'SpanSet' can be used for something;
there are Perl modules with those names concerning date ranges.
Input is welcome as to what interval-savvy functions Muldis D should have.

* Consider renaming Interval to Range, even if that is less specific,
for brevity, and also so we have a word that looks less like Integer
or that an Int abbreviation would be less ambiguous.
- Update, upon discussion, I decided to stay with "interval", which both
Hugh, Philip, Derek agree is the best term, with none arguing differently.

* Flesh out some window/partition funcs, which are kind of like a
generalization of aggregation/reduction functions.  A window()/partition()
wrapper func is like the summary() wrapper func but it has the same number
of output tuples as input ones; when wrapping an agg/reduc func, all output
tuples have the same value per tuple in the same group; when wrapping a
window/partition-oriented func, such as rank(), each tuple in the group
gets or can get a different value.
See these:
- http://www.postgresql.org/docs/9.0/interactive/tutorial-window.html
- http://www.postgresql.org/docs/9.0/interactive/functions-window.html
- http://www.postgresql.org/docs/9.0/interactive/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS

* IN PROGRESS ...
Add Bool-resulting relational operators EXISTS and FORALL, that provide
"existential quantification" and "universal quantification" respectively,
these being useful in constraint definitions.  See TTM book p168, pp394-5
for some info on those.  Also add analogies to Perl 5's List::MoreUtils
operators any(), all(), notall(), none(), true(), false(); some of those
may be the same as EXISTS/FORALL.  Also add an EXACTLY operator like the
Tutorial D language has, and a one() op that is between any() and none().
Maybe some pure boolean ops can be added analogous to the above also; eg
any() an alias for or() and all() an alias for and().
is_(any|all|one|none|notall|etc)_of_(restr|semijoin|semidiff|etc)
source is any|etc matching|where|etc filter|etc
ADD RELATIONAL OPERATORS THAT COMBINE BOOL OPS ADDED IN 0.80.0 WITH
RELATIONAL MAP/RESTRICTION/ETC AND ... The new functions are modelled after
some in Perl 5's List::MoreUtils module.
That is, add prefix ops exactly|all|any|one|none|etc
which take a relation and result in True or False depending on what that
relation's cardinality is.  In some cases, an extra arg is needed:
    - exactly((s⋉t),n) = (#(s⋉t) = n)
    - none((s⋉t)) = exactly((s⋉t),0) = !#(s⋉t)
    - any((s⋉t)) = !exactly((s⋉t),0) = ?#(s⋉t)
    - all((s⋉t),#s) = exactly((s⋉t),#s) = (#(s⋉t) = #s)
    - notall((s⋉t),#s) = !exactly((s⋉t),#s) = (#(s⋉t) != #s)
    - one((s⋉t)) = exactly((s⋉t),1) = (#(s⋉t) = 1)
OR MAYBE THESE AREN'T ANY MORE USEFUL THAN THEIR EQUIVALENT EXPRS.

* Consider adding sequence generator updaters|procedures in Integer.pod.

* Consider adding random value generators for data types other than integer
and rational numerics, such as for character strings or binary strings.

* Consider analogy to SQL's "[UNION|EXCEPT|INTERSECT] CORRESPONDING BY
(attr1,attr2,...)", which is a shorthand for combining projection and
union, that takes a list of attributes and unions the projections of those
attributes from every input relation; so this means, as with join(), that
the input relations don't need to have the same headings.

----------

* In Plain_Text, consider further changes to how character escape sequences
in strings/etc are done.  For example, whether the simple escape sequence
for each string delimiter char may be used in all kinds of strings (as they
are now) or just in strings having the same delim char as is being escaped.

* IN PROGRESS ...
Update the STD dialects to support inline definition of basic
routines (and types?) right in the expressions/etc where they are used,
such as filter functions in restriction() invocations, so many common cases
look much more like their SQL or Perl counterparts, or for that matter, a
functional language's anonymous higher order functions.  This syntax would
be sugar over an explicit material definition plus a FooRef val selection,
which means the inner def effectively is an expression node, and users can
choose to name or not name the FooRef selecting node as normal with value
expressions.  It is expected that the materials could be decl anonymously
and names for them (the inn.foo, not the FooRef's lex.foo) would be
generated as per inline expression nodes etc.

* Further to the previous item, add some special syntax, similar to how one
references a parameter to get its argument's value, which can see into the
caller's lexical scope.  This would be sugar over declaring parameters with
the same name and having the caller explicitly pass arguments to it,
without having to explicitly write that.  Generally this syntax would only
be used with inline-declared routines.  But similarly, add some special
syntax allowing one to essentially just write the body of a routine without
having to explicitly write its heading / parameter list, which is useful
for routines invoked directly from a host language, where said parameters
are attached to host bind variables.  Now one still has to say what the
expected data type is for these bind variables, but then the explicit
syntax for such Muldis D routines is more like that of a SQL statement you
plug into DBI or whatever, without the explicit framing.  May not work
anywhere, but should help where it does.  Maybe use $$foo rather than $foo
to indicate that the 'foo' wasn't explicitly declared in the current
lexical scope and we are referring to the caller or a bind variable.  Or
rather than $$foo, have something like "(param foo : Bar)" for an
expression-inline parameter definition and use, where the part after the
"param" has all the same syntax as an actual param list; this is the one
for host language bind parameters.  Actually that might be useful by
itself.  Similarly "(caller foo)" would be the look to parent Muldis D
lexical scope, or $$foo would just do that maybe, unless this should have
an explicit type declaration still.  Note, if same inline-declared host
param used more than once, you just need "(param foo : Bar)" form once and
other uses can just say foo as per usual; in fact, it must be this way.

* Consider in all STD adding a new pragma that concerns whether data in
delimited character string literals is ASCII or Unicode etc.
Example Plain_Text grammar additions:
            <ws>? ',' <ws>? str_char_repertoire <ws>? '=>' <ws>? <str_cr>
    <str_cr> ::=
        '{' <ws>?
            [<str_cr_describes> <ws>? '=>' <ws>? <str_char_reper>]
                ** [<ws>? ',' <ws>?]
        <ws>? '}'
    <str_cr_describes> ::=
        all | text | name | cmnt
    <str_char_reper> ::=
          ASCII
        | Unicode_6.2.0_canon
        | Unicode_6.2.0_compat
Example Plain_Text code additions:
    str_char_repertoire => { text => Unicode_6.2.0_canon,
        name => Unicode_6.2.0_compat, cmnt => Unicode_6.2.0_compat },
    str_char_repertoire => { all => ASCII },
Of particular interest is the Unicode canonical vs compatibility, that is
NFC|D vs NFKC|D; it is generally recommended such as by the Unicode
consortium to use canonical for general data but to use compatibility for
things like identifiers or to avoid some kinds of security problems; see
http://www.unicode.org/faq/normalization.html.  Note that compatibility is
a smaller repertoire than canonical, so converting from the latter to the
former will lose information.  The text|name affect how delimited char
strs that are Text|Name are interpreted, and the effects are
orthogonal to whether characters are specified literally or in escaped
(eg "\c<...>" form); canonical will preserve exactly what is stated (but
for normalization to NFD) and compatibility will take what is stated and
fold it so semantically same characters become the same codepoints (like as
normalizing to NFKD).  The suggested usage is compatibility for Name to
help avoid security or other problems, and canonical for Text; as for
comments, I currently don't know which is better.  If ASCII is chosen, the
semantics are different; with both Unicode any input is accepted but folded
if needed; for ASCII, it is more likely an exception would be raised if
there are any codepoints outside the 0..127 range in character strings.
The 'all' is a shorthand for giving the same value to all 3 text|name|cmnt
and is more likely to occur with ASCII but it might happen otherwise.
An additional reason to raise this feature is to setup support for other
char sets in future, such as Mojikyo, TRON, GB18030, etc which go beyond
Unicode eg no Han-unification (see http://www.jbrowse.com/text/unij.html +
http://www.ruby-forum.com/topic/165927) but type system also needs update.

* Update HDMD_Perl6_STD.pod considering that a 2010.03.03 P6Syn update
eliminated the special 1/2 literal syntax for rats and so now one writes
<1/2> instead (no whitespace allowed by the '/'); now 1/2 could still work
but now it does so using regular constant folding and so having a higher
precedence op nearby affects its interpretation.

* Update HDMD_Perl6_STD.pod considering names of Perl collection types,
such that "Enum" is the immutable "Pair" and "EnumMap" was renamed from
"Mapping", and "FatRat" is now the "Rat" of unlimited size, etc.

* Consider using postcircumfix syntax for extracting single relation
attrs into Set or Bag etc, meaning wrap_attr; eg "r.@S{a}", "r.@B{a}".
Now that might not work for Array extraction, unless done like
"(r.@A{a} ordered ...)" or some such, which isn't pure postcircumfix,
but that may be for the best anyway.

* Consider adding concrete syntax that is shorthand for multiple
single-attribute extractions where each goes to a separate named expression
node (or variable) but the source is a single collection-typed expr/var.
Or the source could be a multiplicity as well, or mix and match.
The idea here is to replicate some common idioms in Perl such as
"(x, y) = @xy[0,1]" or "(x, y) = %xy{'x','y'}", this being more useful
when the source is an anonymous arbitrary expression.
Proposed syntax is that, on each side of the "::=" or ":=", the source and
target lists are bounded in square brackets, indicating named items assign
in order, and syntax for collections supplying/taking multiple items are
ident to single-attr accessors (having a ".") but that a list is in the
braces/brackets; for example: "[x, y] ::= [3, 4]",
"[a, b] ::= t.{a,b}", "[c, d] ::= ary.[3,5]".  This syntax would
resolve into multiple single-attr accessors when app in system catalog.
The assignment variants of the above would naturally fall out the ability
to have arbitrary expressions on both sides of the ":=", so what you do is
have an array-valued expression on both sides, eg "[x,y] := [y,x]" works
because "[...]" is an array literal now.
We can overload ".[]" for tuples in general so they extract like projection
but return an array rather than a tuple, so we can then say
"[a,b] ::= t.[a,b]" or even "t1.[x,y] := t2.[a,b]" to multi-substitute,
that being a shorthand for "t1.x := t2.a, t1.y := t2.b".  We can't do that
for general relations though since the array subtype of rel is using it.
This mechanism also provides a general way for a function to have multiple
ord retv; eg, "[x,y,z] := foo(...)"; like Perl's "($x,$y,$z) = foo(...)".
A variable (or subject-to-update parameter), "bar", may be aliased using
"foo ::= bar" such that "foo" is an expr node, but like all named exprs in
procedures, "foo" is conceptually reevaluated per mu-statement.
Ordered tuples can be used instead of arrays, and in fact might be a better
solution for multiple reasons.  To do this, just say "%:{x,y,z}" rather
than "[x,y,z]"; the former is shorthand for '%:{"0"=>x,"1"=>y,"2"=>z}'.

* In Plain_Text, consider loosening the grammar regarding some of the normal
prefix or postfix or infix operators so that rather than mandating
whitespace be present between the operators and their arguments, the
whitespace is optional where it wouldn't cause a problem.

----------

* Restore the concept of public-vs-private entities directly in sub|depots.

* Restore the concept of "topic namespaces" (analogous to SQL DBMS concept
of "current database|schema" etc) in some form if not redundant.

* Update the system catalog to deal with database users and privileges etc.

----------

* IN PROGRESS ...
A Muldis D host|peer language can probably hold lexical-to-them variables
whose Muldis D value object is External typed, and so they could
effectively pass around an anonymous closure of
their own language.  Such a value object would be a black box to the host
and can't be dumped to Muldis D source code.

* IN PROGRESS ...
Fully support direct interaction with other languages, mainly either peer
Parrot-hosted languages or each host language of the Muldis D
implementation.  Expand the definition of the "reference" main type
category (or if we need to, create a 5th similarly themed main category) so
that it is home to all foreign-managed values, which to Muldis D are simply
black boxes that Muldis D can pass around routines, store in transient
variables, and use as attributes of tuples or relations.  These
of course can not be stored in a Muldis D depot/database, but they can be
kept in transiant values of Muldis D collection types which are held in
lexical variables by the peer or host language; that language is then
really just using Muldis D as a library of relational functions to organize
and transform its own data.  We also need to add a top level namespace by
which we can reference or invoke the opaque-to-us data types and routines
of the peer or host language.  This can not go under sys.imp or
sys.anything because these are supposed to represent user-defined types and
routines, which in a dynamic peer language can appear or disappear or
change at a moment's notice, same as in Muldis D; on the other hand, types
or routines built-in to the peer/host language that we can assume are as
static as sys.std, could go under sys.imp or something.  This also doesn't
go under fed etc since fed is reserved for data under Muldis D control and
only ever contains pure s/t/r types.  Presumably this namespace will be
subdivided by language analogously to sys.imp or whatever syntax Perl 6
provides for calling out into foreign languages co-hosted on Parrot.  Since
all foreign values are treated as black boxes by Muldis D, it is assumed
that the Muldis D implementation's bindings to the peer/host language will
be providing something akin to a simple pointer value, and that it would
provide the means to know what foreign values are mutually distinct or
implement is_same for them.  One thing for certain is that every
foreign value is disjoint from every Muldis D value, and by default every
foreign value is mutually distinct from every other foreign too, unless
identity is overloaded by the foreign, like how Perl 6's .WHICH works.
The foreign-access namespace may have a simple catalog variable
representing what types and routines it is exposing, but to Muldis D this
would be we_may_update=false.

* IN PROGRESS ...
About External type ... update Perl5_STD and
Perl6_STD to add a new selector node kind 'External' which takes any Perl
value or object as its payload; this is treated completely as a black box
in general within the Muldis D implementation.  For matters of identity
within the Muldis D envirnment, it works as follows:  Under Perl 6, the
Perl value's .WHICH result determines its identity.  Under Perl 5, if the
value is a Perl ref ('ref obj' returns true) then its memory address is
used, and this applies to all objects also (since all refs are mutable,
this seems to be the safest bet); otherwise ('ref obj' is false) then the
value's result in a string context, "obj", is used as the identity; the
mem addr and stringification would both be prefixed with some constant to
distinguish the 2 that might stringify the same.  By default, an
External supports no operators but is/not_same.

----------

* Add new "FTS" or "FullTextSearch" extension which provides weighted
indexed searching of large Text values in terms of their component tokens,
such as what would be considered "words" in human terms.  This is what
would map to the full text search capabilities that underlying SQL DBMSs
may provide, if they are sufficiently similar to each other, or there might
be distinct FTS extensions for significantly different ones?

* Add new "Perl5Regex" extension which provides full use of the Perl 5
regular expression engine for pattern matching and transliteration of Text
values.  Maybe the PCRE library can implement this on other foundations
than Perl 5 itself if they are sufficiently alike; otherwise we can also
have a separate "PCRE" extension.  Or the same extension can provide both?

* Add new "Perl6Rules" extension which provides full use of the Perl 6
rules engine for pattern matching and transliteration of Text values.

* Add new "PGE" or "ParrotGrammarEngine" extension, or whatever an
appropriate replacement is, for pattern matching and transliteration of
Text values.  This and "Perl6Rules" may or may not be sufficiently similar
to combine into one extension.

* Add functions for splitting strings on separators or catenating them with
such to above extensions or to Text.pod as appropriate.  Text has one now.

* Update or supplement the order-determination function for Text so that it
compares whole graphemes (1 grapheme = sequence starting with a base
codepoint plus N combining codepoints, or something) as single string
elements, rather than say comparing a base char against a combining char.

* Add new "Complex" extension which provides the numeric "complex" data
types (each expressed as a pair of real numbers with at least 2 possreps
like cartesian vs polar) and operators.  Note that the SQL standard does
not have such data types but both many general languages as well some
hardware CPUs natively support them.  Probably make "Complex" a mixin type
and have the likes of "RatComplex" and "IntComplex" composing it.  Note
that a complex number over just integers is also called a Gaussian integer.
A question to ask is whether a distinct "imaginary" type is useful; some
may say it is and Digital Mars' "D" has it, but I don't know if others do.
In any event, complex numerics should most likely not be part of the core,
even though their candidacy could be considered borderline; for one thing,
I would expect that most actual uses of them would work with inexact math.

* Add other mathematical extensions, such as ones that add trigonometric
functions et al, or ones that deal with hyperreal/hypercomplex/etc types,
or ones with variants of the core numeric types that propagate NaNs etc.

* Consider adding a sleep() system-service routine, if it would be useful.

* Add multiplication and division operators to the Duration types; these
would both be dyadic ops where the second op is a Numeric.

* Consider adding a Temporal.pod type specific to representing a period in
time, maybe simply as an alias for 'interval_of.*Instant' or some such.
See also the PGTemporal project and its 'Period' type.

* Flesh out "Spatial" extension; provide operators for the spatial data
types, maybe overhaul the types.

* Consider another dialect that is JSON ... like HDMD in form, but stringy.

* Fundamentally, a Muldis D DBMS API or client-server protocol has a
command pattern where the request is Muldis D code and so is the response.
This is like the relationship between Javascript and JSON, where the data
is expressed in the same syntax as code.  Plain_Text by default.
Actually, this brings up an interesting thought in that a DBMS shell could
be analagous to writing HTTP requests manually like telnet to port 80.
But a different Muldis D dialect would be optimized for machine-to-machine
like client-server stuff.
If we ignore parameter binding, a programmer API could also be a function
or procedure call that takes Muldis D code as a single argument and has
Muldis D code as the result; the argument is code defining a tuple value.

* Mention in the DBMS or learn from "Cego" (http://www.lemke-it.com/).

* Apparently Postgres has no built in scheduler feature, and
PgAgent is designed to cover that need.

----------

* Add one or more files to the distro that are straight Plain_Text code like
for defining a whole depot (as per the above) but instead these files
define all the system entities.  Or more specifically they define just the
interfaces/heads of all the system-defined routines, and they have the
complete definitions of all system-defined types, and they declare all the
system catalog dbvars/dbcons.  In other words these files contain
everything that is in the sys.cat dbcon; anything that users can introspect
from sys.cat can also be read from these files in the form of Plain_Text
code, more or less.  The function of these files is analogous to the Perl 6
Setting files described in the Perl 6 Synopsis 32, except that the Muldis D
analogy explicitly does not define the bodies of any built-in routines.  An
idea is that Muldis D implementations could take these files as is and
parse them to populate their sys.cat that users see; of course, the
implementations can actually implement the routines/types as they want.
Note that although this Muldis D code would be bundled with the spec, it is
most likely that the Plain_Text-written standard impl test suite will not.
Note that these files will not go in lib/ but in some other new dir.  Note
that it is likely any implementation will bundle a clone of these files
(suitably integrated as desired) rather than having an actual external
dependency on the Muldis::D distro.  Note that some explicit comment might
be necessary to say there are no licensing restrictions on copying this
builtins-interfaces-defining code into Muldis D implementations, or maybe
no comment is necessary.  Probably a good precedent is to look at what
legalities concern existing tutorial/etc books that have sample code.

* Create another distribution, maybe called Muldis::D::Validator, which
consists essentially of just a t/ directory holding a large number of files
that are straight Plain_Text code, and that emit the TAP protocol when
executed.  The structure and purpose of this collection is essentially
identical to the official Perl 6 test suite.  A valid Muldis D
implementation could conceivably be defined as any interpreter which runs
this test suite correctly.  This new distro would be a "testing requires"
external dependency of both Muldis::D::RefEng and any Parrot-hosted language
or other implementation, though conceivably either could bundle a clone of
Muldis::D::Validator rather than having an actual external dependency.
This test suite would be LGPL licensed.  This new distribution would have a
version number that is of X.Y.Z format like Muldis::D itself, where the X.Y
part always matches that of the Muldis D spec that it is testing compliance
with, while the .Z always starts at zero and increments independently of
the Muldis D spec, as often there may be multiple updates to ::Validator
for awhile between releases of the language spec, and also since .Z updates
in the language spec only indicate bug fixes and shouldn't constitute a
change to the spec from the point of view of ::Validator.
