                                 Muldis::D
---------------------------------------------------------------------------

Following is a summary of things that still need doing.  It is specific to
the Muldis D specification distribution only, and doesn't talk about things
that would go in other distributions, including implementations.  (But,
look at lib/Muldis/D/SeeAlso.pod for a list of actual or possible
implementations.)

Alternately, this list deals with possible ideas to explore, which may or
may not be good ideas to pursue.

The following list is loosely ordered by priority, and is organized into
groups by approximate subject area, but list items may
actually be addressed in a different order.  There is no specific time
table for these items; they are simply to be done "as soon as possible".

* Generally speaking, make a new release to CPAN once every week, assuming
the progress is non-trivial, so there are regular public snapshots with
nicely rendered documentation.

----------

* Update the development status of the Muldis D language spec to "alpha"
from "pre-alpha" and truncate the "Changes" file (but don't set the version
to 1.0.0 yet) only when the spec is complete enough that the Muldis Rosetta
Example Engine reference implementation has fully implemented the language
core, or a significant and computationally complete working subset thereof,
and so the language spec is then considered sufficiently complete with
corner cases exposed; Muldis Rosetta would also be updated to "alpha"
status et al simultaneously.  There are no other preconditions to consider
either project "alpha" status.  Curr est circa Mar..Jun of 2010 for this.

* Preconditions for considering the Muldis D language spec to be either
"beta" or "released" status or "1.0" incl:  A significant, computationally
complete working core or subset thereof as a Parrot hosted language, a
TAP speaking test suite with significant feature coverage, a serious level
of post-alpha-status design input solicited of other interested parties,
implementations over multiple SQL DBMSs.

----------

* IN PROGRESS, FIRST PRIORITY ...
Consider adding a direct analogy to Perl 6 "roles", whose primary purpose
is to help with managing software reuse, mainly when multiple types have a
number of attributes in common, a role can define these and then the
multiple types can compose that role.  This concept should be workable with
both InnerScalarType and InnerNonscalarType.  A role can compose other
roles.  A role or type that composes a role can both add additional
attributes of its own to what the role defines, and the composer can add
extra constraints over the composed attributes like forcing a subtype.  Now
on one hand we could make a role a distinct concept, which by itself can't
be instantiated as a type, or we could reuse the above 2 (IST/INT) type
definition facilities and say an existing such type can be composed into
another.  Now that latter case would in a manner of speaking give Muldis D
the advantages of other OO languages or SQL that work using "specialization
by extension" except that composing an existing type in this way will
actually, for a scalar base, create a disjoint type not a subtype, so we
get the code reuse but Muldis D is still specialization by constraint.  On
the other hand, if we just declare roles as roles that can't be
instantiated themselves, then declaring a role could also serve to declare
a non-dh union type name that can be referred to in type constraint
definitions, that union type implicitly ranging over all actual types that
compose it, and so in this case the composers actually are subtypes; the
problem with this is, what to do about a default value for said union type
whose role isn't composed into anything ... maybe each one needs an
implicit DVPT subtype?  But maybe by TTM terms, what a role actually is is
a dummy type of sorts, where you declare the union name before what it
unions, and the regular types it unions declare themselves part of the
union rather than the other way around.  In any event, a composed role
would never appear in a value's MST or root type list, same as any type
declared as a union.  I should look at TTM and see if they intended for
dummy types to fulfil a role such as this.  Except all directly declared
subtypes of a role union are always disjoint, for scalar ones anyway; the
role is defin shorthand.

* Create the following mixins/roles and group types and ops under them:
    - Ordered :
        - tps: Ordinal, Rational, Stringy, NameChain, etc
        - ops: order/<=>
    - Ordinal :
        - tps: Boolean, Integral, Order
        - ops: succ/++, pred/--, last, first
    - Boolean :
        - tps: Bool
        - ops: not/!, and, or, etc
    - Numeric :
        - tps: Integral, Rational, ... in the future, Complex etc
        - ops: abs/||, sum/+, diff/-, product/*, quotient/'/',
            abs_diff/|-|, range, median, mode
    - Integral :
        - tps: Int
        - ops: remainder/mod/%, power/I^, factorial/I!, fetch_random
    - Rational :
        - tps: Rat, Instant, Duration
        - ops: mean, power/N^, log, natural_[power|log], fetch_random
    - Stringy :
        - tps: Blobby, Textual, String
        - ops: catenation/~, replication/x
    - Blobby :
        - tps: Blob
        - ops: other Blob ops
    - Textual :
        - tps: Text, Name, Comment
        - ops: other Text ops
Maybe the best term for the above is "dummy type", which TTM defines as a
scalar root type with zero possreps.  Or "dummy type" is not exactly the
same but that can be used in describing the above thing;
perhaps "Scalar" is (implicitly) a "dummy type" for example.
A dummy type example is "plane figure" and say "rectangle" is declared with
"isa plane figure" same as "square" might be declared with "isa rectangle".
But unlike the "dummy type" example of TTM which suggests it is a base of
a single inheritence tree, the thing we want one should be able to inherit
from multiple dummy types at once, which probably still works for the term.
TTM also says that a dummy type is by necessity a union type.

* Support delegation / 'handles'; for example:
    - Instant, Duration explicitly delegate to Rat attr,
        and so all operators declared for 'Rat' simply work with
        Instant, Duration also as if all their sigs were s/Rat/Instant/ etc
    - Name explic delegate to Text attr
    - maybe Blob, Text explic delegate to String attr
    - a ColoredCircle would delegate to both Color and Circle attrs?
This will all take some work to get right; not /all/ Rat/etc can be subst.
Probably *only* those operators that Rational/etc explicitly declares can
be delegated to Rat/etc by Instant/etc.

----------

* Update the virtual attributes maps so there is a way to manually specify
a reverse function, as meanwhile all the virtuals don't have to be either
read-only or updatable due to an automatically generated reverse function,
which might vary by implementation, which may be considered broken.  Note
that the reverse functions might have to be defined as per-tuple
operations, separately for insert/substitute/delete.

* Add new "material" kinds that define state constraints (address as simple
nlx.*.data.*), like type constraints but ref in reverse.

* Add new "material" kinds that define
transition constraints.  The initial version would simply take a
'before' and an 'after' state argument.  But it may be good to provide an
alternative where arguments show the delta since often that is what
constraint writers want to know anyway.

* Add new "material" kinds that def stimulus-response rules
/ triggered routines.

* Add sequence generator updaters|procedures in Integer.pod.

* Add new "material" kinds that define descriptions of resource locks that
one wants to get, starting with basic whole dbvar, relvar locks (address as
simple fed.data.foo.*, as well as simple relvar tuple locks (addr as prior
plus lists of values to match like with a semijoin); leave out generic
predicate locks at first but note they will be added later.
Update the system catalog concerning managing shared|exclusive locks or
looking for consistent reads between statements, etc.

* Large updates to docs concerning transactions and resource locking.
Note:  Supposedly PostgreSQL and MySQL use read-committed isolation by
default while SQLite provides serializable.
Add catalog types that represent thrown exceptions.  Prob non-dh tuple.
Also study SQL concept of conditions and handlers, looks sort of like
something between exception handling, signals; or it is their exceptions.

* Use a conceptual framework for database transactions that is strongly
inspired by how distributed source-code version control systems (VCSs)
work, in particular drawing on GIT specifically.  The fundamental feature
of the framework is that the DBMS is managing a depot consisting of 1..N
versions of the same database, where every one of these versions is both
consistent and durable.  Each version is completely defined in isolation,
conceptually, and so any versions in a depot may be deleted without
compromising each of the other versions' ability to define a version of the
entire database.  It is implementation-dependent as to how the versions are
actually stored, such as each having all of the data versus most of them
just having deltas from some other version; what matters is that each
version *appears* to be self-contained.  Every version is created as a
single atomic action, and it is never modified afterwards, though it may be
later deleted (also an atomic action).  Every in-DBMS user process,
henceforth called "user", has its own concept of the current state of the
database, which is one of the depot's versions that is designated a "head".
A user's current head is never replaced during the course of the in-DBMS
process unless the user explicitly replaces it, such as by either
performing an update or requesting to see the latest version (the latter
done such as with an explicit "synchronize" control statement).  Therefore,
each user is highly isolated from all the others, and is guaranteed
consistent repeatable reads and no phantoms; they will get repeatable reads
until they request otherwise.  The framework has no native concept of
"nesting transactions" or "savepoints" or explicit "commit" or "rollback"
commands.  Rather, every single DBMS-performed parent-most multi-update
statement (which is the smallest scope where TTM requires the database to
be consistent both immediately before and immediately after its execution),
is a durable atomic transaction all by itself.  The effect of a successful
multi-update statement is to both produce a new (durable) version in the
depot and to update the executing user's "head" to be that new version (the
prior version may then be deleted automatically depending on
circumstances); a failed multi-update statement is a no-op for the depot,
and the user gets a thrown exception.  A depot's versions are arranged in a
directed acyclic graph where each version save the oldest cites 1..N other
versions as its parents, and conversely each version may have 0..N
children.  A child version has exactly 1 parent when it was created as the
result of executing a multi-update statement in the context of the parent
version; the parent version is the pre-update state of the database and the
child is the post-update state of the database.  A child version has
multiple parents when it is the result of merging or serializing the
changes of multiple users' statements that ran in parallel.  One main
purpose of tracking parents like this is for reliable merging of parallel
changes, so that the intended semantics of each change can be interpreted
correctly, and potential conflicts can be easily detected, and effectively
resolved.  More on how this works follows below.  Note that versions simply
have unique identifiers to be referenced with and there is no implied
ordering between them if they are generated as serial numbers or using date
stamps, though versions with earlier date stamps are given priority in the
case of a merge conflict.  So a multi-update statement is the only native
"transaction" concept, and it is ACID all by itself.  Now, the
multi-statement "transactions" or concepts of nested transactions or
savepoints would all be syntactic sugar over the native concept, and
basically involve keeping track of versions prior to the head and
optionally making an older one the head.  This framework uses the VCS
concept of "branching" (which is something that GIT strongly encourages the
use of, as GIT makes later "merging" relatively painless) as the native way
to manage concurrent autonomous database updates by multiple users.  By
default, when no users have made any changes to the database, a depot just
has a "trunk", and its childmost or only version is called "master"; every
database user process' "head" starts off as the "master" version when that
process starts.  Each (autonomous) user process that wants to update the
database will start by creating a new branch off of the trunk, and
subsequent versions of theirs will go into that, rather than into the trunk
or some other branch.  The trunk is shared by all users while each user's
branch is just for that user, as their private working space.  Note that,
unlike a VCS in general where branches can become long-lived and interact
with each other independently of the trunk, the framework instead follows
the typical needs of an RDBMS, which espouses a single world view as being
dominant over any others, and expects that any branches will be very
short-lived, not existing for longer than a conceptual "database
transaction" would; only the trunk is expected to be long-lived.  (This
isn't to say that a DBMS can't maintain them long term, but one that acts
like a typical RDBMS of today wouldn't.)  Note that the final action on a
branch that involves merging into the trunk, this would be perceived by all
other DBMS users as all of the changes wrought by the branch being a single
atomic update, though the user performing it may see several steps.

* Flesh out matters related to starting or communicating between multiple
autonomous in-DBMS processes, in general, besides the special case about
sequence generators.

----------

* Add to Routines_Catalog.pod and other files
definitions of any remaining routines, eg String routines, that would be
needed so that for all system-defined types all the necessary
system-defined routines would exist that are necessary for defining said
types, especially their constraint or mapping etc definitions.  So in
String.pod we need [catenation, repeat, length, has_substr] etc.
Also add "is_coprime" or GCD or LCM or etc which are used either in the
constraint definition of Rat or in a normalization function for Rat; see
also "the Euclidean algorithm" as an efficient way to do the calculations.

* Consider adding type introspection routines like: is_incomplete() or
is_dh() or is_primitive|structure|reference|enumerated etc.  Or don't
since one could look that up in the system catalog.  But more tests on
individual values might be useful, or maybe we have enough already.

* Add ext/TAP.pod, which is a partial port of Perl 5's Test::More / Perl
6's Test.pm / David Wheeler's pgTAP to Muldis D; assist users in testing
their Muldis D code using TAP protocol.  The TAP messages have type Text.

----------

* Add concept of shallowly homogeneous / sh- relation types to complement
the deeply version, and named maximal types like SHRelation, SHSet,
SHArray, etc to complement the DH/etc, and sh_set_of/etc to complement
dh_set_of/etc; but not sh-scalar or sh-tuple as the concept doesn't make
sense there.  Then update functions like Relation.union/etc to take
sh_set_of.Relation rather than set_of.Relation, which more formally defines
some of their input constraints.

* Consider supplementing the current generic infinite-loop-until-leave
statement with alternatives that use boolean control variables, such as
'while' or 'repeat-while'.

* Consider adding an imperative for-each looping statement; the main
question here is whether it should work on any (unordered) relation or just
on an Array (in which case it iterates through the tuples in sequence by
index); the question is what tasks the for-each would be used for; perhaps
both versions are useful; presumably the main reason to have for-each at
all is when I/O is involved and some derivative needs to be output either
where order matters or where order does not matter; but perhaps only a
routine is needed here such as a catenate function plus normal I/O output.

----------

* Add explicit support for +/- underflow, +/- overflow, NaNs, etc.
I'm inclined to think +/- zero is unnecessary when we have underflow and
can be confusing anyway (just a single normal number zero is better).
I'm not sure if +/- infinities are useful or if overflows cover them for
our purposes.  How this would work is that we define a set
of scalar singleton types, one for each of the special values.  Then we
define extended versions of the Int, Rat, etc types where the extended
types are defined in terms of being union types that union the regular
numeric types with the special singleton type values.  This approach also
means just one each of +Underflow, -Overflow, etc is needed and is a member
of extended Int or Rat etc.  Consider using the existing names "Int"/"Rat"
with the versions that include these special values, and make new names for
the current simpler versions that don't, such as "IntNS" (int no specials),
"RatNS", etc.  Either way, it is useful to support the full range of values
that a Perl 6 numeric can support, or that an IEEE float can support,
without users necessarily having to define it themselves.
IDEA:  Make all the normal math/etc ops work with the extended versions
(those with NaNs, infinities, etc) and in situations where users don't want
those special values they just use a declared type excluding them, and then
the normal type constraints will take care of throwing exceptions when one
divides by zero for example.
And so we then can also rid ourselves of having to use Maybe (as it stands
now based on Set) in as many situations as we do now, where functions come
in pairs where one chooses between a Maybe or a possible exception.

* Use terminology like "ordered" versus "ordinal" to distinguish types like
Rat,Text from Int; "ordinal" naturally supports prec,succ (prev,next)
operators while "ordered" does not.  All "ordinal" are "ordered" but not
the reverse.  Making a finite subtype of an infinite ordered type might
also have the effect of making it ordinal, in some cases.  Add concept of
"step size" or "quantum" or such to talk about ordinal, which is the basis
for figuring out all the other ordinal stuff.

* Flesh out Interval.pod to add a complement of functions for comparing
multiple intervals in different ways, such as is-subset, is-overlap,
is-consecutive, etc, as well as for deriving intervals from a
union/intersect/etc of others, as well as for treating intervals as normal
relations in some contexts, such as for joining or filtering etc, as well
as a function or 3 to do normalization of Interval values.
Maybe the type name 'Range' can be used for something.
Maybe the type name 'Span' or 'SpanSet' can be used for something;
there are Perl modules with those names concerning date ranges.
Input is welcome as to what interval-savvy functions Muldis D should have.

* IN PROGRESS ...
Add Bool-resulting relational operators EXISTS and FORALL, that provide
"existential quantification" and "universal quantification" respectively,
these being useful in constraint definitions.  See TTM book p168, pp394-5
for some info on those.  Also add analogies to Perl 5's List::MoreUtils
operators any(), all(), notall(), none(), true(), false(); some of those
may be the same as EXISTS/FORALL.  Also add an EXACTLY operator like the
Tutorial D language has, and a one() op that is between any() and none().
Maybe some pure boolean ops can be added analagous to the above also; eg
any() an alias for or() and all() an alias for and().
is_(any|all|one|none|notall|etc)_of_(restr|semijoin|semidiff|etc)
$source is any|etc matching|where|etc $filter|etc
ADD RELATIONAL OPERATORS THAT COMBINE BOOL OPS ADDED IN 0.80.0 WITH
RELATIONAL MAP/RESTRICTION/ETC AND ... The new functions are modelled after
some in Perl 5's List::MoreUtils module.

* Consider adding random value generators for data types other than integer
and rational numerics, such as for character strings or binary strings.

* Consider analogy to SQL's "[UNION|EXCEPT|INTERSECT] CORRESPONDING BY
(attr1,attr2,...)", which is a shorthand for combining projection and
union, that takes a list of attributes and unions the projections of those
attributes from every input relation; so this means, as with join(), that
the input relations don't need to have the same headings.

----------

* IN PROGRESS ...
Update the STD dialects with special syntax for declaring whole depots
in a single expression, catalog+data.  This can be useful for example with
a simple Muldis D implementation that wants to represent a persisted depot
on disk as a single file that is Muldis D source code.

* IN PROGRESS ...
Update the STD dialects with special syntax for some nonscalar catalog
types, particularly definitions of routines and types, so that both doing
this can be made even more like a normal language, and also the missing
pieces to support more appropriate syntax coloring and routine locating in
text editors looking at Muldis D source, eg a 'function' keyword at the
start of each function.  Here are some keywords that can then be added:
function, updater, procedure, type, param, ro_param, upd_param, expr, stmt,
subtype, key, possrep, etc; they apply to both 'outer' and 'inner'
types/routines; also keywords for declaring whole packages or something.

* Update the STD dialects to support inline definition of basic inner
routines (and types?) right in the expressions/etc where they are used,
such as filter functions in restriction() invocations, so many common cases
look much more like their SQL or Perl counterparts, or for that matter, a
functional language's anonymous higher order functions.  This syntax would
be sugar over an explicit inner definition plus a FooRef value selection,
which means the inner def effectively is an expression node, and users can
choose to name or not name the FooRef selecting node as normal with value
expressions.  It is expected that the inners could be declared anonymously
and names for them (the inn.foo, not the FooRef's lex.foo) would be
generated as per inline expression nodes etc.  Presumably also add some new
keyword for a routine to invoke itself without using a name like normal;
this would be sugar for normal functional call syntax, using gen inn name.

* Further to the previous item, add some special syntax, similar to how one
references a parameter to get its argument's value, which can see into the
caller's lexical scope.  This would be sugar over declaring parameters with
the same name and having the caller explicitly pass arguments to it,
without having to explicitly write that.  Generally this syntax would only
be used with inline-declared routines.  But similarly, add some special
syntax allowing one to essentially just write the body of a routine without
having to explicitly write its heading / parameter list, which is useful
for routines invoked directly from a host language, where said parameters
are attached to host bind variables.  Now one still has to say what the
expected data type is for these bind variables, but then the explicit
syntax for such Muldis D routines is more like that of a SQL statement you
plug into DBI or whatever, without the explicit framing.  May not work
anywhere, but should help where it does.  Maybe use $$foo rather than $foo
to indicate that the 'foo' wasn't explicitly declared in the current
lexical scope and we are referring to the caller or a bind variable.  Or
rather than $$foo, have something like "(param $foo : Bar)" for an
expression-inline parameter definition and use, where the part after the
"param" has all the same syntax as an actual param list; this is the one
for host language bind parameters.  Actually that might be useful by
itself.  Similarly "(caller $foo)" would be the look to parent Muldis D
lexical scope, or $$foo would just do that maybe, unless this should have
an explicit type declaration still.  Note, if same inline-declared host
param used more than once, you just need "(param $foo : Bar)" form once and
other uses can just say $foo as per usual; in fact, it must be this way.

* Consider in PTMD_STD using a "<ws>" or other-named token similar to the
Perl 6 concept of something that is treated as insignificant whitespace as
far as interpreting the code is concerned, but that may contain #-delimited
comments.  The idea of this is to make it easier to sprinkle comments
almost anywhere without making the grammar a lot more verbose for it.  It
is expected that any use of such comments beside "<value>" nodes would only
be allowed under a "<depot>", and under plain "<value>" would be rejected.
Mind that such a "<ws>" would not be used everywhere in place of \s+/\s*,
but only in the specific places representing a node that would turn into
a system catalog tuple with a "comment".
Essentially it comes in 2 flavors whose definitions are
"<_ms_oc> ::= \s+ [<non_value_comment> \s+]?" and
"<_os_oc> ::= \s* | \s+ <non_value_comment> \s+" or such.

* Consider using postcircumfix syntax for extracting single relation
attrs into Set or Bag etc, meaning wrap_attr; eg "$r.@S{a}", "$r.@B{a}".
Now that might not work for Array extraction, unless done like
"($r.@A{a} ordered ...)" or some such, which isn't pure postcircumfix,
but that may be for the best anyway.

* Consider adding concrete syntax that is shorthand for multiple
single-attribute extractions where each goes to a separate named expression
node (or variable) but the source is a single collection-typed expr/var.
Or the source could be a multiplicity as well, or mix and match.
The idea here is to replicate some common idioms in Perl such as
"($x, $y) = @xy[0,1]" or "($x, $y) = %xy{'x','y'}", this being more useful
when the source is an anonymous arbitrary expression.
Proposed syntax is that, on each side of the "::=" or ":=", the source and
target lists are bounded in square brackets, indicating named items assign
in order, and syntax for collections supplying/taking multiple items are
ident to single-attr accessors (having a ".") but that a list is in the
braces/brackets; for example: "[$x, $y] ::= [3, 4]",
"[$a, $b] ::= $t.%{a,b}", "[$c, $d] ::= $ary.[3,5]".  This syntax would
resolve into multiple single-attr accessors when app in system catalog.

----------

* Restore the concept of public-vs-private entities directly in sub|depots.

* Restore the concept of "topic namespaces" (analagous to SQL DBMS concept
of "current database|schema" etc) in some form if not redundant.

* Update the system catalog to deal with database users and privileges etc.

----------

* IN PROGRESS ...
A Muldis D host|peer language can probably hold lexical-to-them variables
whose Muldis D value object is Reference typed, and so they could
effectively pass around an anonymous closure of
their own language.  Such a value object would be a black box to the host
and can't be dumped to Muldis D source code.

* IN PROGRESS ...
Fully support direct interaction with other languages, mainly either peer
Parrot-hosted languages or each host language of the Muldis D
implementation.  Expand the definition of the "reference" main type
category (or if we need to, create a 5th similarly themed main category) so
that it is home to all foreign-managed values, which to Muldis D are simply
black boxes that Muldis D can pass around routines, store in transient
variables, and use as attributes of tuples or relations.  These
of course can not be stored in a Muldis D depot/database, but they can be
kept in transiant values of Muldis D collection types which are held in
lexical variables by the peer or host language; that language is then
really just using Muldis D as a library of relational functions to organize
and transform its own data.  We also need to add a top level namespace by
which we can reference or invoke the opaque-to-us data types and routines
of the peer or host language.  This can not go under sys.imp or
sys.anything because these are supposed to represent user-defined types and
routines, which in a dynamic peer language can appear or disappear or
change at a moment's notice, same as in Muldis D; on the other hand, types
or routines built-in to the peer/host language that we can assume are as
static as sys.std, could go under sys.imp or something.  This also doesn't
go under fed etc since fed is reserved for data under Muldis D control and
only ever contains pure s/t/r types.  Presumably this namespace will be
subdivided by language analagously to sys.imp or whatever syntax Perl 6
provides for calling out into foreign languages co-hosted on Parrot.  Since
all foreign values are treated as black boxes by Muldis D, it is assumed
that the Muldis D implementation's bindings to the peer/host language will
be providing something akin to a simple pointer value, and that it would
provide the means to know what foreign values are mutually distinct or
implement is_identical for them.  One thing for certain is that every
foreign value is disjoint from every Muldis D value, and by default every
foreign value is mutually distinct from every other foreign too, unless
identity is overloaded by the foreign, like how Perl 6's .WHICH works.
The foreign-access namespace may have a simple catalog variable
representing what types and routines it is exposing, but to Muldis D this
would be we_may_update=false.
TODO:  Figure out how to declare a Muldis D non-dh type whose attributes
are declared of some foreign type ... or maybe we can't and all such
attributes would simply be declared Universal all the time, might be enuf.

* IN PROGRESS ...
First round to above:  Add system-defined subtype of Reference called
sys.std.Core.Type.Foreign, the
new one being a root type, and then we simply say that all values passed in
from a host or peer language are of that root type, period; this new one is
declared in Types.pod probably maybe I guess.  Then update Perl5_STD and
Perl6_STD to add a new selector node kind 'Foreign' which takes any Perl
value or object as its payload; this is treated completely as a black box
in general within the Muldis D implementation.  For matters of identity
within the Muldis D envirnment, it works as follows:  Under Perl 6, the
Perl value's .WHICH result determines its identity.  Under Perl 5, if the
value is a Perl ref ('ref $obj' returns true) then its memory address is
used, and this applies to all objects also (since all refs are mutable,
this seems to be the safest bet); otherwise ('ref $obj' is false) then the
value's result in a string context, "$obj", is used as the identity; the
mem addr and stringification would both be prefixed with some constant to
distinguish the 2 that might stringify the same.  By default, a
ForeignValue supports no operators but is/not_identical.

----------

* Add new "FTS" or "FullTextSearch" extension which provides weighted
indexed searching of large Text values in terms of their component tokens,
such as what would be considered "words" in human terms.  This is what
would map to the full text search capabilities that underlying SQL DBMSs
may provide, if they are sufficiently similar to each other, or there might
be distinct FTS extensions for significantly different ones?

* Add new "Perl5Regex" extension which provides full use of the Perl 5
regular expression engine for pattern matching and transliteration of Text
values.  Maybe the PCRE library can implement this on other foundations
than Perl 5 itself if they are sufficiently alike; otherwise we can also
have a separate "PCRE" extension.  Or the same extension can provide both?

* Add new "Perl6Rules" extension which provides full use of the Perl 6
rules engine for pattern matching and transliteration of Text values.

* Add new "PGE" or "ParrotGrammarEngine" extension, or whatever an
appropriate replacement is, for pattern matching and transliteration of
Text values.  This and "Perl6Rules" may or may not be sufficiently similar
to combine into one extension.

* Add functions for splitting strings on delimiters or catenating them with
such to above extensions or to Text.pod as appropriate.

* Make further updates to DateTime.pod to improve correctness or
normalization, particularly with durations, maybe add some UTCDuration
subtypes.  Also expand the selection of routines to allow more control over
semantics.  Seek advice from the Perl DateTime people about this stuff.

* Consider updates to DateTime.pod that let one record time zones etc
attached to instant values; this would be a non-central feature.

* Consider adding a Temporal.pod
and/or DateTime.pod type specific to representing a period in
time, maybe simply as an alias for 'interval_of.Instant' or some such.  See
also the PGTemporal project and its 'Period' type.

* Flesh out "Spatial" extension; provide operators for the spatial data
types, maybe overhaul the types.

----------

* Add one or more files to the distro that are straight PTMD_STD code like
for defining a whole depot (as per the above) but instead these files
define all the system entities.  Or more specifically they define just the
interfaces/heads of all the system-defined routines, and they have the
complete definitions of all system-defined types, and they declare all the
system catalog dbvars/dbcons.  In other words these files contain
everything that is in the sys.cat dbcon; anything that users can introspect
from sys.cat can also be read from these files in the form of PTMD_STD
code, more or less.  The function of these files is analagous to the Perl 6
Setting files described in the Perl 6 Synopsis 32, except that the Muldis D
analogy explicitly does not define the bodies of any built-in routines.  An
idea is that Muldis D implementations could take these files as is and
parse them to populate their sys.cat that users see; of course, the
implementations can actually implement the routines/types as they want.
Note that although this Muldis D code would be bundled with the spec, it is
most likely that the PTMD_STD-written standard impl test suite will not.
Note that these files will not go in lib/ but in some other new dir.  Note
that it is likely any implementation will bundle a clone of these files
(suitably integrated as desired) rather than having an actual external
dependency on the Muldis::D distro.  Note that some explicit comment might
be necessary to say there are no licensing restrictions on copying this
builtins-interfaces-defining code into Muldis D implementations, or maybe
no comment is necessary.  Probably a good precedent is to look at what
legalities concern existing tutorial/etc books that have sample code.

* Create another distribution, maybe called Muldis::D::Validator, which
consists essentially of just a t/ directory holding a large number of files
that are straight PTMD_STD code, and that emit the TAP protocol when
executed.  The structure and purpose of this collection is essentially
identical to the official Perl 6 test suite.  A valid Muldis D
implementation could conceivably be defined as any interpreter which runs
this test suite correctly.  This new distro would be a "testing requires"
external dependency of both Muldis::Rosetta and any Parrot-hosted language
or other implementation, though conceivably either could bundle a clone of
Muldis::D::Validator rather than having an actual external dependency.
This test suite would be LGPL licensed.  This new distribution would have a
version number that is of X.Y.Z format like Muldis::D itself, where the X.Y
part always matches that of the Muldis D spec that it is testing compliance
with, while the .Z always starts at zero and increments independently of
the Muldis D spec, as often there may be multiple updates to ::Validator
for awhile between releases of the language spec, and also since .Z updates
in the language spec only indicate bug fixes and shouldn't constitute a
change to the spec from the point of view of ::Validator.

----------

* Whatever else needs doing, such as, fixing bugs.
